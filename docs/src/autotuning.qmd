---
title: "Autotuning Guide"
format:
  html:
    code-fold: false
    toc: true
    toc-depth: 3
---

JAX-MPPI includes a robust autotuning framework to optimize MPPI hyperparameters (like temperature $\lambda$, noise covariance $\Sigma$, and planning horizon). The framework supports multiple optimization strategies, including CMA-ES, Ray Tune, and Quality Diversity (QD) methods.

## Overview

The autotuning system allows you to automatically find the best hyperparameters for your MPPI controller. It treats the MPPI controller as a black-box function and optimizes its parameters to minimize a user-defined cost function.

Key features:

- **Multiple Backends**: Support for `cma` (classic CMA-ES), `evosax` (JAX-native, GPU-accelerated), and `ray` (distributed tuning).
- **Flexible Parameters**: Tune scalars ($\lambda$, horizon), vectors (noise mean/sigma), or custom parameters.
- **State Management**: Handles resizing of internal buffers when parameters like `horizon` change.

## Theoretical Background

### Hyperparameter Optimization Problem

The goal of autotuning is to find the optimal set of hyperparameters $\theta$ (e.g., temperature $\lambda$, noise covariance $\Sigma$, horizon $H$) that minimizes the expected cost of the control task. We formulate this as an optimization problem:

$$
\theta^* = \arg\min_{\theta \in \Theta} \mathcal{J}(\theta)
$$

where $\Theta$ is the admissible hyperparameter space, and the objective function $\mathcal{J}(\theta)$ is the expected cumulative cost of the closed-loop system under the MPPI controller parameterized by $\theta$:

$$
\mathcal{J}(\theta) = \mathbb{E}_{\tau \sim \pi_{\text{MPPI}}(\theta)} \left[ \sum_{t=0}^{T_{task}} c(\mathbf{x}_t, \mathbf{u}_t) \right]
$$

Here, $\tau = \{(\mathbf{x}_0, \mathbf{u}_0), \dots \}$ represents a trajectory rollout, and $c(\mathbf{x}, \mathbf{u})$ is the task cost function. Since $\mathcal{J}(\theta)$ is typically non-convex and noisy (due to the stochastic nature of MPPI and the environment), we employ derivative-free optimization methods.

### CMA-ES (Covariance Matrix Adaptation Evolution Strategy)

CMA-ES is a state-of-the-art evolutionary algorithm for continuous optimization. It models the population of candidate solutions using a multivariate normal distribution $\mathcal{N}(\mathbf{m}, \sigma^2 \mathbf{C})$.

The algorithm proceeds in generations $g$. At each generation:

1. **Sampling**: We sample $\lambda_{pop}$ candidate parameters $\theta_i$ (offspring):
   $$
   \theta_i \sim \mathbf{m}^{(g)} + \sigma^{(g)} \mathcal{N}(\mathbf{0}, \mathbf{C}^{(g)}) \quad \text{for } i = 1, \dots, \lambda_{pop}
   $$

2. **Evaluation**: Each candidate $\theta_i$ is evaluated by running an MPPI simulation to estimate $\mathcal{J}(\theta_i)$.

3. **Selection and Recombination**: The candidates are sorted by their cost $\mathcal{J}(\theta_i)$. The top $\mu$ candidates (parents) are selected to update the mean.

4. **Covariance Adaptation**: The covariance matrix $\mathbf{C}^{(g)}$ is updated to increase the likelihood of successful steps.

## Installation

Autotuning requires optional dependencies.

To install the basic autotuning tools (CMA-ES and Evosax):

```bash
pip install jax-mppi[autotuning]
```

To install extra tools for global optimization (Ray Tune, HyperOpt, Ribs):

```bash
pip install jax-mppi[autotuning-extra]
```

## Quick Start

Here is a minimal example using the classic `cma` library to tune temperature ($\lambda$) and exploration noise ($\Sigma$).

```python
import jax.numpy as jnp
from jax_mppi import mppi, autotune

# 1. Setup MPPI
# Assume 'dynamics' and 'running_cost' are defined
config, state = mppi.create(
    nx=4, nu=1,
    dynamics=dynamics,
    running_cost=running_cost,
    horizon=20
)
holder = autotune.ConfigStateHolder(config, state)

# 2. Define evaluation function
def evaluate():
    # Run simulation with holder.config and holder.state
    # ... simulation loop ...
    # Calculate performance cost (e.g., final distance to goal)
    mean_cost = 1.0  # Placeholder
    return autotune.EvaluationResult(
        mean_cost=mean_cost,
        rollouts=jnp.zeros((1, 20, 4)), # Optional: return trajectories
        params={},
        iteration=0
    )

# 3. Create Tuner
tuner = autotune.Autotune(
    params_to_tune=[
        autotune.LambdaParameter(holder, min_value=0.1),
        autotune.NoiseSigmaParameter(holder, min_value=0.1),
    ],
    evaluate_fn=evaluate,
    optimizer=autotune.CMAESOpt(population=10),
)

# 4. Optimize
best_result = tuner.optimize_all(iterations=30)
print(f"Best parameters: {best_result.params}")
```

## Usage

### Tunable Parameters

The framework provides several built-in parameter classes that know how to extract and update values in the MPPI configuration/state:

| Parameter Class | Description |
|-----------------|-------------|
| `LambdaParameter` | Tunes the inverse temperature $\lambda$. Lower values mean more aggressive exploitation. |
| `NoiseSigmaParameter` | Tunes the diagonal of the exploration noise covariance $\Sigma$. |
| `MuParameter` | Tunes the mean of the exploration noise $\mu$. |
| `HorizonParameter` | Tunes the planning horizon $H$. Automatically resizes internal buffers. |

You can also define custom parameters by subclassing `autotune.TunableParameter`.

### Optimizers

#### CMA-ES (`autotune.CMAESOpt`)
Uses the standard `cma` Python library. Robust and reliable, but runs on CPU and doesn't benefit from JIT compilation for the optimization step itself.

#### Evosax (`autotune_evosax.CMAESOpt`, `SepCMAESOpt`, `OpenESOpt`)
Uses the `evosax` library, which is written in JAX. This allows for:
- GPU acceleration of the optimization strategy.
- Seamless integration with JAX-based evaluation functions.
- Advanced strategies like Sep-CMA-ES (good for high dimensions) and OpenES (good for large populations).

```python
from jax_mppi import autotune_evosax

optimizer = autotune_evosax.CMAESOpt(population=100, sigma=0.1)
```

#### Ray Tune (`autotune_global.RayOptimizer`)
Use this for global optimization over large search spaces or when using schedulers like HyperBand. Requires `ray[tune]`.

```python
from ray import tune
from jax_mppi import autotune_global as autog

params = [
    autog.GlobalLambdaParameter(holder, search_space=tune.loguniform(0.1, 10.0)),
]
```

### Quality Diversity (QD)

To find a diverse set of high-performing parameters (e.g., finding parameters that work well for different environments), use `autotune_qd`. This uses the `ribs` library (CMA-ME algorithm).

```python
from jax_mppi import autotune_qd

tuner = autotune.Autotune(
    params_to_tune=[...],
    evaluate_fn=evaluate,
    optimizer=autotune_qd.CMAMEOpt(population=20, bins=10),
)
```

## Examples

Complete examples can be found in `examples/autotuning/`:

- **`basic.py`**: Minimal example of tuning a scalar parameter.
- **`pendulum.py`**: Tuning MPPI for the pendulum swing-up task.
- **`evosax_comparison.py`**: Benchmarking `cma` vs `evosax` performance.

## API Reference

### `jax_mppi.autotune`

#### `Autotune`
The main orchestrator class.

```python
class Autotune:
    def __init__(
        self,
        params_to_tune: list[TunableParameter],
        evaluate_fn: Callable[[], EvaluationResult],
        optimizer: Optional[Optimizer] = None,
        reload_state_fn: Optional[Callable] = None,
    ): ...

    def optimize_all(self, iterations: int) -> EvaluationResult: ...
```

#### `ConfigStateHolder`
A container for mutable access to MPPI configuration and state.

```python
class ConfigStateHolder:
    def __init__(self, config: Any, state: Any):
        self.config = config
        self.state = state
```

### `jax_mppi.autotune_evosax`

#### `CMAESOpt`
JAX-native CMA-ES optimizer.

```python
class CMAESOpt(EvoSaxOptimizer):
    def __init__(self, population: int = 10, sigma: float = 0.1, **kwargs): ...
```

## Troubleshooting

### Missing Dependencies
If you see `ImportError: CMA-ES optimizer requires the 'cma' package`, install the optional dependencies:
```bash
pip install jax-mppi[autotuning]
```

### Horizon Resizing Issues
When tuning `HorizonParameter`, the internal trajectory buffers (`U`, `action_sequence`) are resized. Ensure your evaluation function does not hold stale references to the old `mppi_state`. Always access state via `holder.state`.

### JIT Compilation Errors
If using `evosax` with a JIT-compiled evaluation loop, ensure your `evaluate_fn` is pure and JAX-compatible. The standard `Autotune` class currently runs the evaluation loop in Python to support complex environments (like Gym), so strict JIT compatibility of the *entire* loop is not enforced unless you manually wrap it.
