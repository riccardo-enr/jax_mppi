[
  {
    "objectID": "autotuning.html",
    "href": "autotuning.html",
    "title": "Autotuning Guide",
    "section": "",
    "text": "JAX-MPPI includes a robust autotuning framework to optimize MPPI hyperparameters (like temperature \\(\\lambda\\), noise covariance \\(\\Sigma\\), and planning horizon). The framework supports multiple optimization strategies, including CMA-ES, Ray Tune, and Quality Diversity (QD) methods.\n\n\nThe autotuning process involves three main components:\n\nTunable Parameters: Parameters you want to optimize (e.g., LambdaParameter, NoiseSigmaParameter).\nEvaluation Function: A function that runs MPPI with a specific configuration and returns a cost (and optionally other metrics).\nOptimizer: The algorithm used to search for the best parameters (e.g., CMAESOpt).\n\n\n\n\nThe autotune module provides a simple interface for CMA-ES optimization.\nimport jax.numpy as jnp\nfrom jax_mppi import mppi, autotune\n\n# 1. Setup MPPI\nconfig, state = mppi.create(...)\nholder = autotune.ConfigStateHolder(config, state)\n\n# 2. Define evaluation\ndef evaluate():\n    # Run simulation with holder.config and holder.state\n    # Calculate performance cost\n    return autotune.EvaluationResult(mean_cost=cost, ...)\n\n# 3. Create Tuner\ntuner = autotune.Autotune(\n    params_to_tune=[\n        autotune.LambdaParameter(holder, min_value=0.1),\n        autotune.NoiseSigmaParameter(holder, min_value=0.1),\n    ],\n    evaluate_fn=evaluate,\n    optimizer=autotune.CMAESOpt(population=10),\n)\n\n# 4. Optimize\nbest_result = tuner.optimize_all(iterations=30)\nprint(f\"Best parameters: {best_result.params}\")\nSee examples/autotune_basic.py and examples/autotune_pendulum.py for complete running examples.\n\n\n\n\n\nFor more complex search spaces or when you want to use advanced schedulers and search algorithms (like HyperOpt or Bayesian Optimization), use autotune_global.\n\nNote: Requires ray[tune], hyperopt, and bayesian-optimization.\n\nfrom ray import tune\nfrom jax_mppi import autotune_global as autog\n\n# Define search space using Ray Tune's API\nparams = [\n    autog.GlobalLambdaParameter(holder, search_space=tune.loguniform(0.1, 10.0)),\n    autog.GlobalNoiseSigmaParameter(holder, search_space=tune.uniform(0.1, 2.0)),\n]\n\ntuner = autog.AutotuneGlobal(\n    params_to_tune=params,\n    evaluate_fn=evaluate,\n    optimizer=autog.RayOptimizer(),\n)\n\nbest = tuner.optimize_all(iterations=100)\n\n\n\nTo find a diverse set of high-performing parameters (e.g., finding parameters that work well for different environments or behavioral descriptors), use autotune_qd.\nfrom jax_mppi import autotune_qd\n\ntuner = autotune.Autotune(\n    params_to_tune=[...],\n    evaluate_fn=evaluate,\n    optimizer=autotune_qd.CMAMEOpt(population=20, bins=10),\n)\n\n\n\n\nThe framework supports tuning the following parameters out-of-the-box:\n\nLambdaParameter: MPPI temperature (\\(\\lambda\\)).\nNoiseSigmaParameter: Exploration noise covariance diagonal.\nMuParameter: Exploration noise mean.\nHorizonParameter: Planning horizon length (resizes internal buffers automatically).\n\nYou can also define custom parameters by subclassing TunableParameter.\n\n\n\nThis section details the mathematical foundations of the autotuning algorithms available in jax_mppi.\n\n\nThe goal of autotuning is to find the optimal set of hyperparameters \\(\\theta\\) (e.g., temperature \\(\\lambda\\), noise covariance \\(\\Sigma\\), horizon \\(H\\)) that minimizes the expected cost of the control task. We formulate this as an optimization problem:\n[ ^* = _{} () ]\nwhere \\(\\Theta\\) is the admissible hyperparameter space, and the objective function \\(\\mathcal{J}(\\theta)\\) is the expected cumulative cost of the closed-loop system under the MPPI controller parameterized by \\(\\theta\\):\n[ () = {{}()} ]\nHere, \\(\\tau = \\{(\\mathbf{x}_0, \\mathbf{u}_0), \\dots \\}\\) represents a trajectory rollout, and \\(c(\\mathbf{x}, \\mathbf{u})\\) is the task cost function. Since \\(\\mathcal{J}(\\theta)\\) is typically non-convex and noisy (due to the stochastic nature of MPPI and the environment), we employ derivative-free optimization methods.\n\n\n\nCMA-ES is a state-of-the-art evolutionary algorithm for continuous optimization. It models the population of candidate solutions using a multivariate normal distribution \\(\\mathcal{N}(\\mathbf{m}, \\sigma^2 \\mathbf{C})\\).\nThe algorithm proceeds in generations \\(g\\). At each generation:\n\nSampling: We sample \\(\\lambda_{pop}\\) candidate parameters \\(\\theta_i\\) (offspring): [ i ^{(g)} + ^{(g)} (, ^{(g)}) i = 1, , {pop} ]\nEvaluation: Each candidate \\(\\theta_i\\) is evaluated by running an MPPI simulation to estimate \\(\\mathcal{J}(\\theta_i)\\).\nSelection and Recombination: The candidates are sorted by their cost \\(\\mathcal{J}(\\theta_i)\\). The top \\(\\mu\\) candidates (parents) are selected to update the mean: [ ^{(g+1)} = {i=1}^{} w_i {i:_{pop}} ] where \\(w_i\\) are positive weights summing to 1, and \\(\\theta_{i:\\lambda_{pop}}\\) denotes the \\(i\\)-th best candidate.\nCovariance Adaptation: The covariance matrix \\(\\mathbf{C}^{(g)}\\) is updated to increase the likelihood of successful steps. This involves two paths:\n\nRank-1 Update: Uses the evolution path \\(\\mathbf{p}_c\\) to exploit correlations between consecutive steps.\nRank-\\(\\mu\\) Update: Uses the variance of the successful steps. [ ^{(g+1)} = (1 - c_1 - c_) ^{(g)} + c_1 c c^T + c{i=1}^{} w_i ({i:{pop}} - ^{(g)})({i:{pop}} - {(g)})T / ^{(g)2} ]\n\nStep Size Control: The global step size \\(\\sigma^{(g)}\\) is updated using the conjugate evolution path \\(\\mathbf{p}_\\sigma\\) to control the overall scale of the distribution.\n\n\n\n\nQuality Diversity (QD) algorithms optimize for a set of high-performing solutions that are diverse with respect to a user-defined measure. jax_mppi uses CMA-ME (Covariance Matrix Adaptation MAP-Elites), which combines the search power of CMA-ES with the archive maintenance of MAP-Elites.\n\n\nWe seek to find a collection of parameters \\(P = \\{\\theta_1, \\dots, \\theta_N\\}\\) that maximize the quality function \\(f(\\theta) = -\\mathcal{J}(\\theta)\\) while covering the behavior space \\(\\mathcal{B}\\).\nLet \\(\\mathbf{b}(\\theta): \\Theta \\to \\mathcal{B}\\) be a function mapping parameters to a behavior descriptor (e.g., control smoothness, risk sensitivity).\n\n\n\nThe behavior space \\(\\mathcal{B}\\) is discretized into a grid of cells (the archive \\(\\mathcal{A}\\)). Each cell \\(\\mathcal{A}_{\\mathbf{z}}\\) stores the best solution found so far that maps to that cell index \\(\\mathbf{z}\\):\n[ {} = {: (()) = } f() ]\n\n\n\nCMA-ME maintains a set of emitters, which are instances of CMA-ES optimizing for improvement in the archive.\n\nEmission: An emitter samples a candidate \\(\\theta\\) from its distribution \\(\\mathcal{N}(\\mathbf{m}, \\sigma^2 \\mathbf{C})\\).\nEvaluation: Calculate quality \\(f(\\theta)\\) and behavior \\(\\mathbf{b}(\\theta)\\).\nArchive Update:\n\nDetermine the cell index \\(\\mathbf{z} = \\text{index}(\\mathbf{b}(\\theta))\\).\nIf cell \\(\\mathcal{A}_{\\mathbf{z}}\\) is empty or \\(f(\\theta) &gt; f(\\mathcal{A}_{\\mathbf{z}})\\), replace the occupant with \\(\\theta\\).\nCalculate the “improvement” value \\(\\Delta\\) (e.g., \\(f(\\theta) - f(\\mathcal{A}_{\\mathbf{z}}^{old})\\)).\n\nEmitter Update: The CMA-ES emitter updates its mean and covariance based on the improvement \\(\\Delta\\), guiding the search toward regions of the behavior space where quality can be improved or new cells can be discovered.\n\n\n\n\n\nFor global search over large, potentially non-convex spaces with complex constraints, we utilize Ray Tune. The problem is formulated as:\n[ {{global}} () ]\nwhere \\(\\Theta_{global}\\) can be defined by complex distributions (e.g., Log-Uniform, Categorical).\nRay Tune orchestrates the search using algorithms like:\n\nBayesian Optimization: Uses a Gaussian Process surrogate model \\(P(f \\mid \\mathcal{D})\\) to approximate the objective and an acquisition function \\(a(\\theta)\\) (e.g., Expected Improvement) to select the next sample: [ {next} = {} a() ]\nHyperOpt (TPE): Models \\(p(\\theta \\mid y)\\) using Tree-structured Parzen Estimators to sample promising candidates.\n\nThese methods are particularly useful for “warm-starting” the local search (CMA-ES) or finding the best family of parameters (e.g., finding the right order of magnitude for \\(\\lambda\\)).",
    "crumbs": [
      "Home",
      "Algorithms",
      "Autotuning Guide"
    ]
  },
  {
    "objectID": "autotuning.html#overview",
    "href": "autotuning.html#overview",
    "title": "Autotuning Guide",
    "section": "",
    "text": "The autotuning process involves three main components:\n\nTunable Parameters: Parameters you want to optimize (e.g., LambdaParameter, NoiseSigmaParameter).\nEvaluation Function: A function that runs MPPI with a specific configuration and returns a cost (and optionally other metrics).\nOptimizer: The algorithm used to search for the best parameters (e.g., CMAESOpt).",
    "crumbs": [
      "Home",
      "Algorithms",
      "Autotuning Guide"
    ]
  },
  {
    "objectID": "autotuning.html#basic-usage-cma-es",
    "href": "autotuning.html#basic-usage-cma-es",
    "title": "Autotuning Guide",
    "section": "",
    "text": "The autotune module provides a simple interface for CMA-ES optimization.\nimport jax.numpy as jnp\nfrom jax_mppi import mppi, autotune\n\n# 1. Setup MPPI\nconfig, state = mppi.create(...)\nholder = autotune.ConfigStateHolder(config, state)\n\n# 2. Define evaluation\ndef evaluate():\n    # Run simulation with holder.config and holder.state\n    # Calculate performance cost\n    return autotune.EvaluationResult(mean_cost=cost, ...)\n\n# 3. Create Tuner\ntuner = autotune.Autotune(\n    params_to_tune=[\n        autotune.LambdaParameter(holder, min_value=0.1),\n        autotune.NoiseSigmaParameter(holder, min_value=0.1),\n    ],\n    evaluate_fn=evaluate,\n    optimizer=autotune.CMAESOpt(population=10),\n)\n\n# 4. Optimize\nbest_result = tuner.optimize_all(iterations=30)\nprint(f\"Best parameters: {best_result.params}\")\nSee examples/autotune_basic.py and examples/autotune_pendulum.py for complete running examples.",
    "crumbs": [
      "Home",
      "Algorithms",
      "Autotuning Guide"
    ]
  },
  {
    "objectID": "autotuning.html#advanced-usage",
    "href": "autotuning.html#advanced-usage",
    "title": "Autotuning Guide",
    "section": "",
    "text": "For more complex search spaces or when you want to use advanced schedulers and search algorithms (like HyperOpt or Bayesian Optimization), use autotune_global.\n\nNote: Requires ray[tune], hyperopt, and bayesian-optimization.\n\nfrom ray import tune\nfrom jax_mppi import autotune_global as autog\n\n# Define search space using Ray Tune's API\nparams = [\n    autog.GlobalLambdaParameter(holder, search_space=tune.loguniform(0.1, 10.0)),\n    autog.GlobalNoiseSigmaParameter(holder, search_space=tune.uniform(0.1, 2.0)),\n]\n\ntuner = autog.AutotuneGlobal(\n    params_to_tune=params,\n    evaluate_fn=evaluate,\n    optimizer=autog.RayOptimizer(),\n)\n\nbest = tuner.optimize_all(iterations=100)\n\n\n\nTo find a diverse set of high-performing parameters (e.g., finding parameters that work well for different environments or behavioral descriptors), use autotune_qd.\nfrom jax_mppi import autotune_qd\n\ntuner = autotune.Autotune(\n    params_to_tune=[...],\n    evaluate_fn=evaluate,\n    optimizer=autotune_qd.CMAMEOpt(population=20, bins=10),\n)",
    "crumbs": [
      "Home",
      "Algorithms",
      "Autotuning Guide"
    ]
  },
  {
    "objectID": "autotuning.html#tunable-parameters",
    "href": "autotuning.html#tunable-parameters",
    "title": "Autotuning Guide",
    "section": "",
    "text": "The framework supports tuning the following parameters out-of-the-box:\n\nLambdaParameter: MPPI temperature (\\(\\lambda\\)).\nNoiseSigmaParameter: Exploration noise covariance diagonal.\nMuParameter: Exploration noise mean.\nHorizonParameter: Planning horizon length (resizes internal buffers automatically).\n\nYou can also define custom parameters by subclassing TunableParameter.",
    "crumbs": [
      "Home",
      "Algorithms",
      "Autotuning Guide"
    ]
  },
  {
    "objectID": "autotuning.html#mathematical-formulation",
    "href": "autotuning.html#mathematical-formulation",
    "title": "Autotuning Guide",
    "section": "",
    "text": "This section details the mathematical foundations of the autotuning algorithms available in jax_mppi.\n\n\nThe goal of autotuning is to find the optimal set of hyperparameters \\(\\theta\\) (e.g., temperature \\(\\lambda\\), noise covariance \\(\\Sigma\\), horizon \\(H\\)) that minimizes the expected cost of the control task. We formulate this as an optimization problem:\n[ ^* = _{} () ]\nwhere \\(\\Theta\\) is the admissible hyperparameter space, and the objective function \\(\\mathcal{J}(\\theta)\\) is the expected cumulative cost of the closed-loop system under the MPPI controller parameterized by \\(\\theta\\):\n[ () = {{}()} ]\nHere, \\(\\tau = \\{(\\mathbf{x}_0, \\mathbf{u}_0), \\dots \\}\\) represents a trajectory rollout, and \\(c(\\mathbf{x}, \\mathbf{u})\\) is the task cost function. Since \\(\\mathcal{J}(\\theta)\\) is typically non-convex and noisy (due to the stochastic nature of MPPI and the environment), we employ derivative-free optimization methods.\n\n\n\nCMA-ES is a state-of-the-art evolutionary algorithm for continuous optimization. It models the population of candidate solutions using a multivariate normal distribution \\(\\mathcal{N}(\\mathbf{m}, \\sigma^2 \\mathbf{C})\\).\nThe algorithm proceeds in generations \\(g\\). At each generation:\n\nSampling: We sample \\(\\lambda_{pop}\\) candidate parameters \\(\\theta_i\\) (offspring): [ i ^{(g)} + ^{(g)} (, ^{(g)}) i = 1, , {pop} ]\nEvaluation: Each candidate \\(\\theta_i\\) is evaluated by running an MPPI simulation to estimate \\(\\mathcal{J}(\\theta_i)\\).\nSelection and Recombination: The candidates are sorted by their cost \\(\\mathcal{J}(\\theta_i)\\). The top \\(\\mu\\) candidates (parents) are selected to update the mean: [ ^{(g+1)} = {i=1}^{} w_i {i:_{pop}} ] where \\(w_i\\) are positive weights summing to 1, and \\(\\theta_{i:\\lambda_{pop}}\\) denotes the \\(i\\)-th best candidate.\nCovariance Adaptation: The covariance matrix \\(\\mathbf{C}^{(g)}\\) is updated to increase the likelihood of successful steps. This involves two paths:\n\nRank-1 Update: Uses the evolution path \\(\\mathbf{p}_c\\) to exploit correlations between consecutive steps.\nRank-\\(\\mu\\) Update: Uses the variance of the successful steps. [ ^{(g+1)} = (1 - c_1 - c_) ^{(g)} + c_1 c c^T + c{i=1}^{} w_i ({i:{pop}} - ^{(g)})({i:{pop}} - {(g)})T / ^{(g)2} ]\n\nStep Size Control: The global step size \\(\\sigma^{(g)}\\) is updated using the conjugate evolution path \\(\\mathbf{p}_\\sigma\\) to control the overall scale of the distribution.\n\n\n\n\nQuality Diversity (QD) algorithms optimize for a set of high-performing solutions that are diverse with respect to a user-defined measure. jax_mppi uses CMA-ME (Covariance Matrix Adaptation MAP-Elites), which combines the search power of CMA-ES with the archive maintenance of MAP-Elites.\n\n\nWe seek to find a collection of parameters \\(P = \\{\\theta_1, \\dots, \\theta_N\\}\\) that maximize the quality function \\(f(\\theta) = -\\mathcal{J}(\\theta)\\) while covering the behavior space \\(\\mathcal{B}\\).\nLet \\(\\mathbf{b}(\\theta): \\Theta \\to \\mathcal{B}\\) be a function mapping parameters to a behavior descriptor (e.g., control smoothness, risk sensitivity).\n\n\n\nThe behavior space \\(\\mathcal{B}\\) is discretized into a grid of cells (the archive \\(\\mathcal{A}\\)). Each cell \\(\\mathcal{A}_{\\mathbf{z}}\\) stores the best solution found so far that maps to that cell index \\(\\mathbf{z}\\):\n[ {} = {: (()) = } f() ]\n\n\n\nCMA-ME maintains a set of emitters, which are instances of CMA-ES optimizing for improvement in the archive.\n\nEmission: An emitter samples a candidate \\(\\theta\\) from its distribution \\(\\mathcal{N}(\\mathbf{m}, \\sigma^2 \\mathbf{C})\\).\nEvaluation: Calculate quality \\(f(\\theta)\\) and behavior \\(\\mathbf{b}(\\theta)\\).\nArchive Update:\n\nDetermine the cell index \\(\\mathbf{z} = \\text{index}(\\mathbf{b}(\\theta))\\).\nIf cell \\(\\mathcal{A}_{\\mathbf{z}}\\) is empty or \\(f(\\theta) &gt; f(\\mathcal{A}_{\\mathbf{z}})\\), replace the occupant with \\(\\theta\\).\nCalculate the “improvement” value \\(\\Delta\\) (e.g., \\(f(\\theta) - f(\\mathcal{A}_{\\mathbf{z}}^{old})\\)).\n\nEmitter Update: The CMA-ES emitter updates its mean and covariance based on the improvement \\(\\Delta\\), guiding the search toward regions of the behavior space where quality can be improved or new cells can be discovered.\n\n\n\n\n\nFor global search over large, potentially non-convex spaces with complex constraints, we utilize Ray Tune. The problem is formulated as:\n[ {{global}} () ]\nwhere \\(\\Theta_{global}\\) can be defined by complex distributions (e.g., Log-Uniform, Categorical).\nRay Tune orchestrates the search using algorithms like:\n\nBayesian Optimization: Uses a Gaussian Process surrogate model \\(P(f \\mid \\mathcal{D})\\) to approximate the objective and an acquisition function \\(a(\\theta)\\) (e.g., Expected Improvement) to select the next sample: [ {next} = {} a() ]\nHyperOpt (TPE): Models \\(p(\\theta \\mid y)\\) using Tree-structured Parzen Estimators to sample promising candidates.\n\nThese methods are particularly useful for “warm-starting” the local search (CMA-ES) or finding the best family of parameters (e.g., finding the right order of magnitude for \\(\\lambda\\)).",
    "crumbs": [
      "Home",
      "Algorithms",
      "Autotuning Guide"
    ]
  },
  {
    "objectID": "plan/index.html",
    "href": "plan/index.html",
    "title": "Development Plans",
    "section": "",
    "text": "This section contains various development plans and investigations for the jax-mppi project.\n\n\n\nPerformance Analysis\nExample Performance Investigation\nQuadrotor Trajectory Following\n\n\n\n\n\nPorting from PyTorch to JAX\nEvosax Integration\nCUDA MPPI Implementation\nCUDA MPPI Submodule Plan"
  },
  {
    "objectID": "plan/index.html#active-plans",
    "href": "plan/index.html#active-plans",
    "title": "Development Plans",
    "section": "",
    "text": "Performance Analysis\nExample Performance Investigation\nQuadrotor Trajectory Following"
  },
  {
    "objectID": "plan/index.html#completed-plans",
    "href": "plan/index.html#completed-plans",
    "title": "Development Plans",
    "section": "",
    "text": "Porting from PyTorch to JAX\nEvosax Integration\nCUDA MPPI Implementation\nCUDA MPPI Submodule Plan"
  },
  {
    "objectID": "plan/completed/cuda_mppi_submodule_plan.html",
    "href": "plan/completed/cuda_mppi_submodule_plan.html",
    "title": "Plan: Add cuda_mppi as git submodule",
    "section": "",
    "text": "Extract the CUDA MPPI implementation from src/cuda_mppi/ into a separate git repository and integrate it as a submodule at third_party/cuda_mppi.\n\n\n\n\nNew repository: riccardo-enr/cuda_mppi\nSubmodule path: third_party/cuda_mppi\nFeature branch: feat/cuda-mppi-submodule\nTrack branch: main (in submodule)\n\n\n\n\n\n\n\nUse gh repo create to create riccardo-enr/cuda_mppi\nSet appropriate description and visibility\nInitialize with README.md\n\n\n\n\n\nClone the new repository locally\nCopy contents from src/cuda_mppi/ to repository root\nCreate proper repository structure:\n\nCMakeLists.txt (standalone build)\nREADME.md (usage and build instructions)\nLICENSE (MIT, matching main project)\n.gitignore\n\nCommit and push initial code\n\n\n\n\n\nCreate branch: feat/cuda-mppi-submodule\nCheckout the new branch\n\n\n\n\n\nCreate third_party/ directory\nRun: git submodule add https://github.com/riccardo-enr/cuda_mppi.git third_party/cuda_mppi\nRemove old src/cuda_mppi/ directory\nCommit submodule addition\n\n\n\n\n\nUpdate root CMakeLists.txt:\n\nChange add_subdirectory(src/cuda_mppi) to add_subdirectory(third_party/cuda_mppi)\n\nVerify pyproject.toml (should work via scikit-build-core automatically)\nCheck for any hardcoded paths in:\n\nPython source files\nCMake files\nDocumentation\n\n\n\n\n\n\nClean build: rm -rf build/ _skbuild/\nTest clone from scratch with submodules\nBuild and install package: uv pip install -e .\nRun test: python examples/test_cuda_mppi.py\nVerify JIT examples work\n\n\n\n\n\nAdd submodule instructions to README.md:\n# Clone with submodules\ngit clone --recursive https://github.com/riccardo-enr/jax_mppi.git\n\n# Or if already cloned\ngit submodule update --init --recursive\nDocument build requirements in both repositories\nUpdate any relevant documentation links\n\n\n\n\n\nCommit all changes with conventional commits\nPush feature branch\nCreate PR referencing issue #23\nMove this plan to completed/ directory\n\n\n\n\n\nIf issues arise, we can:\n\nRemove submodule: git submodule deinit -f third_party/cuda_mppi\nDelete from git: git rm -f third_party/cuda_mppi\nRemove from .gitmodules\nRestore original src/cuda_mppi from git history"
  },
  {
    "objectID": "plan/completed/cuda_mppi_submodule_plan.html#overview",
    "href": "plan/completed/cuda_mppi_submodule_plan.html#overview",
    "title": "Plan: Add cuda_mppi as git submodule",
    "section": "",
    "text": "Extract the CUDA MPPI implementation from src/cuda_mppi/ into a separate git repository and integrate it as a submodule at third_party/cuda_mppi."
  },
  {
    "objectID": "plan/completed/cuda_mppi_submodule_plan.html#decisions-made",
    "href": "plan/completed/cuda_mppi_submodule_plan.html#decisions-made",
    "title": "Plan: Add cuda_mppi as git submodule",
    "section": "",
    "text": "New repository: riccardo-enr/cuda_mppi\nSubmodule path: third_party/cuda_mppi\nFeature branch: feat/cuda-mppi-submodule\nTrack branch: main (in submodule)"
  },
  {
    "objectID": "plan/completed/cuda_mppi_submodule_plan.html#implementation-steps",
    "href": "plan/completed/cuda_mppi_submodule_plan.html#implementation-steps",
    "title": "Plan: Add cuda_mppi as git submodule",
    "section": "",
    "text": "Use gh repo create to create riccardo-enr/cuda_mppi\nSet appropriate description and visibility\nInitialize with README.md\n\n\n\n\n\nClone the new repository locally\nCopy contents from src/cuda_mppi/ to repository root\nCreate proper repository structure:\n\nCMakeLists.txt (standalone build)\nREADME.md (usage and build instructions)\nLICENSE (MIT, matching main project)\n.gitignore\n\nCommit and push initial code\n\n\n\n\n\nCreate branch: feat/cuda-mppi-submodule\nCheckout the new branch\n\n\n\n\n\nCreate third_party/ directory\nRun: git submodule add https://github.com/riccardo-enr/cuda_mppi.git third_party/cuda_mppi\nRemove old src/cuda_mppi/ directory\nCommit submodule addition\n\n\n\n\n\nUpdate root CMakeLists.txt:\n\nChange add_subdirectory(src/cuda_mppi) to add_subdirectory(third_party/cuda_mppi)\n\nVerify pyproject.toml (should work via scikit-build-core automatically)\nCheck for any hardcoded paths in:\n\nPython source files\nCMake files\nDocumentation\n\n\n\n\n\n\nClean build: rm -rf build/ _skbuild/\nTest clone from scratch with submodules\nBuild and install package: uv pip install -e .\nRun test: python examples/test_cuda_mppi.py\nVerify JIT examples work\n\n\n\n\n\nAdd submodule instructions to README.md:\n# Clone with submodules\ngit clone --recursive https://github.com/riccardo-enr/jax_mppi.git\n\n# Or if already cloned\ngit submodule update --init --recursive\nDocument build requirements in both repositories\nUpdate any relevant documentation links\n\n\n\n\n\nCommit all changes with conventional commits\nPush feature branch\nCreate PR referencing issue #23\nMove this plan to completed/ directory"
  },
  {
    "objectID": "plan/completed/cuda_mppi_submodule_plan.html#rollback-plan",
    "href": "plan/completed/cuda_mppi_submodule_plan.html#rollback-plan",
    "title": "Plan: Add cuda_mppi as git submodule",
    "section": "",
    "text": "If issues arise, we can:\n\nRemove submodule: git submodule deinit -f third_party/cuda_mppi\nDelete from git: git rm -f third_party/cuda_mppi\nRemove from .gitmodules\nRestore original src/cuda_mppi from git history"
  },
  {
    "objectID": "plan/completed/evosax_integration.html",
    "href": "plan/completed/evosax_integration.html",
    "title": "Evosax Integration Plan",
    "section": "",
    "text": "Goal: Add evosax as a JAX-native optimization backend for the autotuning framework in jax-mppi.\nStatus: In Progress\n\n\n\nEvosax is a JAX-native library for evolutionary strategies that provides highly efficient, JIT-compiled optimization algorithms. Integrating it into the autotuning framework will:\n\nProvide JAX-native optimization - Full JIT compilation of the entire tuning loop\nEnable GPU acceleration - Evolutionary strategies running entirely on GPU\nAdd diverse algorithms - CMA-ES, OpenES, SNES, Sep-CMA-ES, and more\nImprove performance - Eliminate Python overhead in the optimization loop\nSimplify dependencies - Pure JAX implementation, no external C++ dependencies\n\n\n\n\nPerformance: Fully JIT-compiled ES algorithms vs. Python-based cma library\nJAX ecosystem fit: Natural integration with JAX-based MPPI code\nGPU support: Can run entire autotuning on GPU without host-device transfers\nAlgorithm variety: 15+ evolutionary strategies in one package\nMaintained: Active development and well-documented\n\n\n\n\n\n\n\n\nThe current autotuning system has three modules:\n\nautotune.py (656 lines) - Core framework + CMA-ES via cma library\n\nAbstract Optimizer base class\nCMAESOpt using Python cma package\nParameter abstractions: LambdaParameter, NoiseSigmaParameter, MuParameter, HorizonParameter\nAutotune orchestrator class\n\nautotune_global.py (375 lines) - Ray Tune for global search\n\nRayOptimizer for distributed hyperparameter search\nGlobal parameter variants with search spaces\nIntegration with HyperOpt and BayesOpt\n\nautotune_qd.py (218 lines) - CMA-ME quality diversity\n\nCMAMEOpt using ribs library\nArchive-based diversity preservation\n\n\n\n\n\nAll optimizers follow the Optimizer ABC:\nclass Optimizer(abc.ABC):\n    @abc.abstractmethod\n    def setup_optimization(\n        self, initial_params: np.ndarray,\n        evaluate_fn: Callable[[np.ndarray], EvaluationResult]\n    ) -&gt; None:\n        ...\n\n    @abc.abstractmethod\n    def optimize_step(self) -&gt; EvaluationResult:\n        ...\n\n    def optimize_all(self, iterations: int) -&gt; EvaluationResult:\n        ...\n\n\n\n\n\n\n\nFile: pyproject.toml\nChanges:\n\nAdd evosax&gt;=0.1.0 to [project.optional-dependencies.autotuning] section\nAdd to [project.optional-dependencies.dev] section as well\n\nReasoning: Keep evosax optional like other autotuning dependencies, allowing users to install only what they need.\n\n\n\n\nNew File: src/jax_mppi/autotune_evosax.py (~387 lines)\nContents (Implemented):\n\n\nImplements Optimizer ABC with evosax backends:\nclass EvoSaxOptimizer(Optimizer):\n    \"\"\"JAX-native evolutionary strategies using evosax.\n\n    Fully JIT-compiled optimization loop with GPU support.\n\n    Attributes:\n        strategy: evosax strategy name (e.g., \"CMA_ES\", \"OpenES\", \"SNES\")\n        population: Population size\n        num_generations: Number of generations per optimize_step\n        maximize: Whether to maximize objective (default: False for cost minimization)\n    \"\"\"\n\n    def __init__(\n        self,\n        strategy: str = \"CMA_ES\",\n        population: int = 10,\n        num_generations: int = 1,\n        sigma_init: float = 0.1,\n        maximize: bool = False,\n        es_params: dict | None = None,\n    ):\n        ...\nKey features:\n\nStrategy selection from evosax’s 15+ algorithms\nJIT-compiled ask-evaluate-tell loop\nSupport for both single-step and batch optimization\nConfigurable ES hyperparameters via es_params\n\n\n\n\ndef _create_jax_evaluate_fn(\n    evaluate_fn: Callable[[np.ndarray], EvaluationResult],\n    maximize: bool = False,\n) -&gt; Callable[[jax.Array], float]:\n    \"\"\"Wrap Python evaluation function for JAX compatibility.\n\n    Handles conversion between JAX arrays and numpy arrays,\n    extracts scalar cost from EvaluationResult.\n    \"\"\"\n    ...\nChallenge: The user-provided evaluate_fn may not be JAX-pure (e.g., it might use numpy, modify state, etc.). Need to handle this gracefully.\nSolutions:\n\nOption A: Use jax.pure_callback to call non-pure evaluation functions\nOption B: Require evaluation function to be JAX-pure for evosax optimizer\nOption C: Provide both pure and impure modes\n\nRecommendation: Start with Option C - detect if evaluation is JAX-pure, use direct JIT if yes, use pure_callback if no.\n\n\n\nEvosax can leverage vmap for parallel fitness evaluation:\ndef _batch_evaluate(\n    solutions: jax.Array,  # (population, param_dim)\n    evaluate_fn: Callable,\n) -&gt; jax.Array:  # (population,)\n    \"\"\"Evaluate population in parallel using vmap.\"\"\"\n    return jax.vmap(evaluate_fn)(solutions)\nBenefits:\n\nGPU parallelization of fitness evaluations\nSignificant speedup when dynamics/cost are JIT-compiled\n\nChallenges:\n\nRequires evaluation to be JAX-pure and vmappable\nMay need sequential fallback for non-pure evaluations\n\n\n\n\nProvide convenience classes for common strategies:\nclass CMAESOpt(EvoSaxOptimizer):\n    \"\"\"CMA-ES optimizer (evosax backend).\"\"\"\n    def __init__(self, population: int = 10, sigma: float = 0.1, **kwargs):\n        super().__init__(strategy=\"CMA_ES\", population=population,\n                         sigma_init=sigma, **kwargs)\n\nclass OpenESOpt(EvoSaxOptimizer):\n    \"\"\"OpenAI's Evolution Strategies.\"\"\"\n    def __init__(self, population: int = 100, sigma: float = 0.1, **kwargs):\n        super().__init__(strategy=\"OpenES\", population=population,\n                         sigma_init=sigma, **kwargs)\n\nclass SepCMAESOpt(EvoSaxOptimizer):\n    \"\"\"Separable CMA-ES (faster for high dimensions).\"\"\"\n    def __init__(self, population: int = 10, sigma: float = 0.1, **kwargs):\n        super().__init__(strategy=\"Sep_CMA_ES\", population=population,\n                         sigma_init=sigma, **kwargs)\n\n\n\n\n\nFile: src/jax_mppi/autotune.py\nChanges:\n\nUpdate module docstring to mention evosax as an option\nUpdate examples to show evosax usage\n\nExample addition:\n&gt;&gt;&gt; # With evosax (JAX-native, GPU-accelerated)\n&gt;&gt;&gt; from jax_mppi import autotune_evosax\n&gt;&gt;&gt; tuner = jmppi.autotune.Autotune(\n...     params_to_tune=[...],\n...     evaluate_fn=evaluate,\n...     optimizer=autotune_evosax.CMAESOpt(population=10),\n... )\n\n\n\n\nFile: src/jax_mppi/__init__.py\nChanges:\n# Add conditional import\ntry:\n    from . import autotune_evosax\nexcept ImportError:\n    autotune_evosax = None  # evosax not installed\nReasoning: Keep it optional - don’t break imports if evosax isn’t installed.\n\n\n\n\nNew File: tests/test_autotune_evosax.py (~408 lines)\nTest coverage:\n\nBasic functionality tests\n\nTest EvoSaxOptimizer initialization with various strategies\nTest setup_optimization() creates valid ES state\nTest optimize_step() returns EvaluationResult\nTest optimize_all() finds better solutions\n\nStrategy-specific tests\n\nTest CMAESOpt on simple quadratic function\nTest OpenESOpt convergence\nTest SepCMAESOpt on high-dimensional problem\n\nIntegration tests\n\nTest with actual MPPI parameter tuning (simple 1D system)\nTest with LambdaParameter and NoiseSigmaParameter\nVerify results comparable to CMAESOpt from cma library\n\nJAX-specific tests\n\nTest JIT compilation of optimization loop\nTest with JAX-pure evaluation function\nTest with non-pure evaluation function (using pure_callback)\nTest batched evaluation with vmap\n\nEdge cases\n\nTest with single parameter\nTest with multi-dimensional parameters\nTest parameter bounds enforcement\nTest with invalid strategy name (should raise clear error)\n\n\nTest structure example:\ndef test_evosax_cmaes_simple():\n    \"\"\"Test CMA-ES on simple quadratic objective.\"\"\"\n    # Minimize ||x - target||^2\n    target = np.array([1.0, 2.0, 3.0])\n\n    def evaluate_fn(x: np.ndarray) -&gt; EvaluationResult:\n        cost = float(np.sum((x - target) ** 2))\n        return EvaluationResult(\n            mean_cost=cost,\n            rollouts=jnp.zeros((1, 1, 3)),\n            params={},\n            iteration=0,\n        )\n\n    optimizer = EvoSaxOptimizer(strategy=\"CMA_ES\", population=10)\n    optimizer.setup_optimization(\n        initial_params=np.zeros(3),\n        evaluate_fn=evaluate_fn,\n    )\n\n    best = optimizer.optimize_all(iterations=50)\n\n    # Should converge close to target\n    assert best.mean_cost &lt; 0.1\n\n\n\n\nNew File: examples/autotune_evosax_comparison.py (~307 lines)\nPurpose: Compare evosax vs. cma library performance\nContents:\n\nSetup simple pendulum MPPI tuning task\nRun with CMAESOpt from cma library (baseline)\nRun with CMAESOpt from evosax\nRun with other evosax strategies (OpenES, Sep-CMA-ES)\nCompare:\n\nConvergence speed (iterations to threshold)\nWall-clock time\nFinal performance\n\nGenerate comparison plots:\n\nConvergence curves for each optimizer\nTime comparison bar chart\nParameter trajectory plots\n\n\nExpected outcome: Evosax should be faster in wall-clock time due to JIT compilation, especially with GPU.\n\n\n\n\n\n\nAdd evosax to the autotuning section:\n### Autotuning\n\nJAX-MPPI supports automatic hyperparameter tuning with multiple backends:\n\n- **CMA-ES** (via `cma` library) - Classic evolution strategy\n- **CMA-ES** (via `evosax`) - JAX-native, GPU-accelerated  ⚡ **NEW**\n- **Ray Tune** - Distributed hyperparameter search\n- **CMA-ME** (via `ribs`) - Quality diversity optimization\n\nInstall with: `pip install jax-mppi[autotuning] evosax`\n\n\n\nUpdate autotune.py module docstring with evosax example.\n\n\n\nDocument for users switching from cma to evosax:\n## Migrating from cma to evosax\n\n**Before:**\n```python\nfrom jax_mppi.autotune import CMAESOpt\nopt = CMAESOpt(population=10, sigma=0.1)\nAfter:\nfrom jax_mppi.autotune_evosax import CMAESOpt\nopt = CMAESOpt(population=10, sigma=0.1)\nBenefits: 5-10x faster with GPU acceleration\n\n\n\n\n\n\n\n\nThe main challenge is that user evaluation functions may not be JAX-pure.\nStrategy:\ndef _wrap_evaluation(evaluate_fn, maximize):\n    \"\"\"Wrap evaluation function for JAX compatibility.\"\"\"\n\n    # Try to detect if function is JAX-pure\n    # by checking if it uses only JAX operations\n\n    def jax_eval(x: jax.Array) -&gt; float:\n        # Convert to numpy for non-pure functions\n        x_np = np.array(x)\n        result = evaluate_fn(x_np)\n        cost = result.mean_cost\n        return -cost if maximize else cost\n\n    # For non-pure functions, use pure_callback\n    if not _is_jax_pure(evaluate_fn):\n        @jax.jit\n        def wrapped(x):\n            return jax.pure_callback(\n                jax_eval,\n                jax.ShapeDtypeStruct((), jnp.float32),\n                x\n            )\n        return wrapped\n    else:\n        return jax.jit(jax_eval)\n\n\n\nProvide both modes:\nclass EvoSaxOptimizer(Optimizer):\n    def __init__(self, ..., batched_evaluation: bool = False):\n        self.batched_evaluation = batched_evaluation\n\n    def optimize_step(self):\n        solutions = self.es_state.ask()\n\n        if self.batched_evaluation:\n            # Parallel evaluation with vmap\n            fitness = jax.vmap(self.evaluate_fn)(solutions)\n        else:\n            # Sequential evaluation (safer for non-pure functions)\n            fitness = jnp.array([\n                self.evaluate_fn(x) for x in solutions\n            ])\n\n        self.es_state = self.es_state.tell(fitness)\n        ...\n\n\n\nEvosax doesn’t natively handle box constraints. Options:\n\nRejection sampling: Reject invalid samples (wasteful)\nClipping: Clip to bounds after sampling (biases distribution)\nRepair: Project invalid samples to feasible region\nPenalty: Add penalty for constraint violation\n\nRecommendation: Use clipping (Option 2) for simplicity, add note in docs that proper constrained optimization should use constrained ES variants if needed.\ndef _apply_bounds(x, lower, upper):\n    \"\"\"Apply box constraints via clipping.\"\"\"\n    if lower is not None or upper is not None:\n        x = jnp.clip(x, lower, upper)\n    return x\n\n\n\n\n\nStrategies to support (from evosax):\n\n\n\nCMA_ES - Classic Covariance Matrix Adaptation\nSep_CMA_ES - Separable CMA-ES (faster for high-dim)\nIPOP_CMA_ES - Increasing population CMA-ES\nBIPOP_CMA_ES - Bi-population CMA-ES\nOpenES - OpenAI’s Natural Evolution Strategies\nSNES - Separable Natural Evolution Strategies\nxNES - Exponential Natural Evolution Strategies\nSimpleGA - Simple Genetic Algorithm\nPersistentES - Persistent Evolution Strategies\nLES - Learned Evolution Strategies (meta-learned)\n\n\n\n\n\nDefault: CMA_ES (well-tested, robust)\nHigh-dimensional: Sep_CMA_ES (scales better)\nLarge population budget: OpenES (naturally parallelizable)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\ncma library\nevosax\n\n\n\n\nLanguage\nPython + C\nPure JAX\n\n\nJIT compilation\n❌\n✅\n\n\nGPU acceleration\n❌\n✅\n\n\nBatched evaluation\n❌\n✅ (via vmap)\n\n\nIntegration with JAX code\n⚠️ (numpy conversion)\n✅ (native)\n\n\nAlgorithm variety\nCMA-ES variants only\n15+ strategies\n\n\nPerformance (CPU)\nGood\nSimilar\n\n\nPerformance (GPU)\nN/A\nExcellent (5-10x)\n\n\n\n\n\n\nUse cma library when:\n\nYou need the original CMA-ES implementation\nYour evaluation function has complex Python dependencies\nYou’re not using GPU\n\nUse evosax when:\n\nYou want GPU acceleration\nYour MPPI code is already JIT-compiled\nYou want to experiment with different ES algorithms\nYou need batched parallel evaluation\n\n\n\n\n\n\n\n\n\nTest each strategy initialization\nTest optimize_step produces valid results\nTest parameter bounds enforcement\nTest with different parameter dimensions\n\n\n\n\n\nCompare convergence to cma library on same problems\nTest with actual MPPI parameter tuning\nVerify GPU execution (if GPU available)\n\n\n\n\n\nCompare wall-clock time vs cma library\nMeasure JIT compilation overhead\nProfile GPU vs CPU performance\n\n\n\n\n\nEnsure results are deterministic with fixed seed\nVerify backward compatibility with existing Optimizer API\n\n\n\n\n\n\n\n\nProblem: First call to evosax optimizer incurs JIT compilation cost.\nSolution:\n\nDocument warmup requirement\nProvide warmup() method that JIT-compiles with dummy data\nConsider pre-compilation for common parameter dimensions\n\n\n\n\nProblem: Most user evaluation functions are not JAX-pure (use numpy, I/O, etc.).\nSolution:\n\nUse jax.pure_callback to wrap impure functions\nProvide clear error messages when incompatible operations are detected\nDocument limitations and workarounds\n\n\n\n\nProblem: GPU memory may be limited for large populations.\nSolution:\n\nAdd memory usage estimates in docs\nProvide chunk-based evaluation for very large populations\nDefault to reasonable population sizes\n\n\n\n\nProblem: JAX PRNG behaves differently than numpy.random.\nSolution:\n\nDocument PRNG handling\nEnsure reproducibility with fixed JAX random keys\nProvide utility to seed evosax optimizer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep\nDescription\nEstimated Lines\nTime\n\n\n\n\n1\nAdd dependency to pyproject.toml\n~5\n5 min\n\n\n2\nImplement autotune_evosax.py\n~350\n4-6 hours\n\n\n3\nUpdate autotune.py docs\n~20\n30 min\n\n\n4\nUpdate init.py\n~5\n5 min\n\n\n5\nCreate test_autotune_evosax.py\n~250\n3-4 hours\n\n\n6\nCreate example comparison script\n~200\n2-3 hours\n\n\n7\nUpdate documentation\n~100\n1-2 hours\n\n\nTotal\n\n~930 lines\n11-16 hours\n\n\n\n\n\n\n\n\n\n\nEvoSaxOptimizer implements Optimizer ABC correctly\nAll evosax strategies can be instantiated\nOptimization converges on test problems\nIntegration with existing Autotune class works\nParameter bounds are respected\n\n\n\n\n\nEvosax is faster than cma on GPU (&gt;2x speedup)\nEvosax is competitive with cma on CPU (within 20%)\nJIT compilation overhead is acceptable (&lt;5s for typical problems)\n\n\n\n\n\nAll tests pass (&gt;95% coverage)\nDocumentation is clear and complete\nExamples run without errors\nCode follows existing style conventions\n\n\n\n\n\nWorks with all parameter types (Lambda, NoiseSigma, Mu, Horizon)\nCompatible with existing Autotune orchestrator\nNo breaking changes to existing API\nOptional dependency (doesn’t break install if evosax not available)\n\n\n\n\n\n\n\n\n\nAdd more evosax strategies (GLD, LM-MA-ES, etc.)\nImplement proper constrained optimization variants\nAdd support for multi-objective optimization\nCreate Jupyter notebook tutorial\n\n\n\n\n\nIntegrate with autotune_qd.py for quality diversity\nAdd learned evolution strategies (LES) with meta-learning\nImplement adaptive strategy selection\nAdd visualization of ES state (e.g., covariance ellipsoids)\n\n\n\n\n\nDevelop JAX-native quality diversity framework\nAdd support for multi-fidelity optimization\nImplement distributed evosax with multi-GPU support\nCreate benchmarking suite comparing all optimizers\n\n\n\n\n\n\n\nevosax documentation\nJAX documentation\nCurrent autotuning implementation: src/jax_mppi/autotune.py\nOriginal pytorch_mppi autotune: ../pytorch_mppi/src/pytorch_mppi/autotune.py\n\n\n\n\n\n\nShould we deprecate the cma library backend?\n\nProbably not - keep both for compatibility\nUsers can choose based on their needs\n\nShould batched evaluation be default?\n\nNo - requires JAX-pure evaluation functions\nMake it opt-in with clear documentation\n\nWhich evosax strategies should have convenience classes?\n\nStart with: CMA_ES, Sep_CMA_ES, OpenES\nAdd more based on user feedback\n\nShould we add evosax to core dependencies or keep it optional?\n\nKeep optional - maintains lightweight core\nDocument installation clearly\n\n\n\n\n\n\n\nEvosax is actively maintained by Robert Lange\nCurrent version: 0.1.x (check latest before implementing)\nJAX-native means entire optimization loop can run on GPU\nConsider adding evosax to CI/CD pipeline for testing"
  },
  {
    "objectID": "plan/completed/evosax_integration.html#overview",
    "href": "plan/completed/evosax_integration.html#overview",
    "title": "Evosax Integration Plan",
    "section": "",
    "text": "Evosax is a JAX-native library for evolutionary strategies that provides highly efficient, JIT-compiled optimization algorithms. Integrating it into the autotuning framework will:\n\nProvide JAX-native optimization - Full JIT compilation of the entire tuning loop\nEnable GPU acceleration - Evolutionary strategies running entirely on GPU\nAdd diverse algorithms - CMA-ES, OpenES, SNES, Sep-CMA-ES, and more\nImprove performance - Eliminate Python overhead in the optimization loop\nSimplify dependencies - Pure JAX implementation, no external C++ dependencies\n\n\n\n\nPerformance: Fully JIT-compiled ES algorithms vs. Python-based cma library\nJAX ecosystem fit: Natural integration with JAX-based MPPI code\nGPU support: Can run entire autotuning on GPU without host-device transfers\nAlgorithm variety: 15+ evolutionary strategies in one package\nMaintained: Active development and well-documented"
  },
  {
    "objectID": "plan/completed/evosax_integration.html#current-state-analysis",
    "href": "plan/completed/evosax_integration.html#current-state-analysis",
    "title": "Evosax Integration Plan",
    "section": "",
    "text": "The current autotuning system has three modules:\n\nautotune.py (656 lines) - Core framework + CMA-ES via cma library\n\nAbstract Optimizer base class\nCMAESOpt using Python cma package\nParameter abstractions: LambdaParameter, NoiseSigmaParameter, MuParameter, HorizonParameter\nAutotune orchestrator class\n\nautotune_global.py (375 lines) - Ray Tune for global search\n\nRayOptimizer for distributed hyperparameter search\nGlobal parameter variants with search spaces\nIntegration with HyperOpt and BayesOpt\n\nautotune_qd.py (218 lines) - CMA-ME quality diversity\n\nCMAMEOpt using ribs library\nArchive-based diversity preservation\n\n\n\n\n\nAll optimizers follow the Optimizer ABC:\nclass Optimizer(abc.ABC):\n    @abc.abstractmethod\n    def setup_optimization(\n        self, initial_params: np.ndarray,\n        evaluate_fn: Callable[[np.ndarray], EvaluationResult]\n    ) -&gt; None:\n        ...\n\n    @abc.abstractmethod\n    def optimize_step(self) -&gt; EvaluationResult:\n        ...\n\n    def optimize_all(self, iterations: int) -&gt; EvaluationResult:\n        ..."
  },
  {
    "objectID": "plan/completed/evosax_integration.html#implementation-plan",
    "href": "plan/completed/evosax_integration.html#implementation-plan",
    "title": "Evosax Integration Plan",
    "section": "",
    "text": "File: pyproject.toml\nChanges:\n\nAdd evosax&gt;=0.1.0 to [project.optional-dependencies.autotuning] section\nAdd to [project.optional-dependencies.dev] section as well\n\nReasoning: Keep evosax optional like other autotuning dependencies, allowing users to install only what they need.\n\n\n\n\nNew File: src/jax_mppi/autotune_evosax.py (~387 lines)\nContents (Implemented):\n\n\nImplements Optimizer ABC with evosax backends:\nclass EvoSaxOptimizer(Optimizer):\n    \"\"\"JAX-native evolutionary strategies using evosax.\n\n    Fully JIT-compiled optimization loop with GPU support.\n\n    Attributes:\n        strategy: evosax strategy name (e.g., \"CMA_ES\", \"OpenES\", \"SNES\")\n        population: Population size\n        num_generations: Number of generations per optimize_step\n        maximize: Whether to maximize objective (default: False for cost minimization)\n    \"\"\"\n\n    def __init__(\n        self,\n        strategy: str = \"CMA_ES\",\n        population: int = 10,\n        num_generations: int = 1,\n        sigma_init: float = 0.1,\n        maximize: bool = False,\n        es_params: dict | None = None,\n    ):\n        ...\nKey features:\n\nStrategy selection from evosax’s 15+ algorithms\nJIT-compiled ask-evaluate-tell loop\nSupport for both single-step and batch optimization\nConfigurable ES hyperparameters via es_params\n\n\n\n\ndef _create_jax_evaluate_fn(\n    evaluate_fn: Callable[[np.ndarray], EvaluationResult],\n    maximize: bool = False,\n) -&gt; Callable[[jax.Array], float]:\n    \"\"\"Wrap Python evaluation function for JAX compatibility.\n\n    Handles conversion between JAX arrays and numpy arrays,\n    extracts scalar cost from EvaluationResult.\n    \"\"\"\n    ...\nChallenge: The user-provided evaluate_fn may not be JAX-pure (e.g., it might use numpy, modify state, etc.). Need to handle this gracefully.\nSolutions:\n\nOption A: Use jax.pure_callback to call non-pure evaluation functions\nOption B: Require evaluation function to be JAX-pure for evosax optimizer\nOption C: Provide both pure and impure modes\n\nRecommendation: Start with Option C - detect if evaluation is JAX-pure, use direct JIT if yes, use pure_callback if no.\n\n\n\nEvosax can leverage vmap for parallel fitness evaluation:\ndef _batch_evaluate(\n    solutions: jax.Array,  # (population, param_dim)\n    evaluate_fn: Callable,\n) -&gt; jax.Array:  # (population,)\n    \"\"\"Evaluate population in parallel using vmap.\"\"\"\n    return jax.vmap(evaluate_fn)(solutions)\nBenefits:\n\nGPU parallelization of fitness evaluations\nSignificant speedup when dynamics/cost are JIT-compiled\n\nChallenges:\n\nRequires evaluation to be JAX-pure and vmappable\nMay need sequential fallback for non-pure evaluations\n\n\n\n\nProvide convenience classes for common strategies:\nclass CMAESOpt(EvoSaxOptimizer):\n    \"\"\"CMA-ES optimizer (evosax backend).\"\"\"\n    def __init__(self, population: int = 10, sigma: float = 0.1, **kwargs):\n        super().__init__(strategy=\"CMA_ES\", population=population,\n                         sigma_init=sigma, **kwargs)\n\nclass OpenESOpt(EvoSaxOptimizer):\n    \"\"\"OpenAI's Evolution Strategies.\"\"\"\n    def __init__(self, population: int = 100, sigma: float = 0.1, **kwargs):\n        super().__init__(strategy=\"OpenES\", population=population,\n                         sigma_init=sigma, **kwargs)\n\nclass SepCMAESOpt(EvoSaxOptimizer):\n    \"\"\"Separable CMA-ES (faster for high dimensions).\"\"\"\n    def __init__(self, population: int = 10, sigma: float = 0.1, **kwargs):\n        super().__init__(strategy=\"Sep_CMA_ES\", population=population,\n                         sigma_init=sigma, **kwargs)\n\n\n\n\n\nFile: src/jax_mppi/autotune.py\nChanges:\n\nUpdate module docstring to mention evosax as an option\nUpdate examples to show evosax usage\n\nExample addition:\n&gt;&gt;&gt; # With evosax (JAX-native, GPU-accelerated)\n&gt;&gt;&gt; from jax_mppi import autotune_evosax\n&gt;&gt;&gt; tuner = jmppi.autotune.Autotune(\n...     params_to_tune=[...],\n...     evaluate_fn=evaluate,\n...     optimizer=autotune_evosax.CMAESOpt(population=10),\n... )\n\n\n\n\nFile: src/jax_mppi/__init__.py\nChanges:\n# Add conditional import\ntry:\n    from . import autotune_evosax\nexcept ImportError:\n    autotune_evosax = None  # evosax not installed\nReasoning: Keep it optional - don’t break imports if evosax isn’t installed.\n\n\n\n\nNew File: tests/test_autotune_evosax.py (~408 lines)\nTest coverage:\n\nBasic functionality tests\n\nTest EvoSaxOptimizer initialization with various strategies\nTest setup_optimization() creates valid ES state\nTest optimize_step() returns EvaluationResult\nTest optimize_all() finds better solutions\n\nStrategy-specific tests\n\nTest CMAESOpt on simple quadratic function\nTest OpenESOpt convergence\nTest SepCMAESOpt on high-dimensional problem\n\nIntegration tests\n\nTest with actual MPPI parameter tuning (simple 1D system)\nTest with LambdaParameter and NoiseSigmaParameter\nVerify results comparable to CMAESOpt from cma library\n\nJAX-specific tests\n\nTest JIT compilation of optimization loop\nTest with JAX-pure evaluation function\nTest with non-pure evaluation function (using pure_callback)\nTest batched evaluation with vmap\n\nEdge cases\n\nTest with single parameter\nTest with multi-dimensional parameters\nTest parameter bounds enforcement\nTest with invalid strategy name (should raise clear error)\n\n\nTest structure example:\ndef test_evosax_cmaes_simple():\n    \"\"\"Test CMA-ES on simple quadratic objective.\"\"\"\n    # Minimize ||x - target||^2\n    target = np.array([1.0, 2.0, 3.0])\n\n    def evaluate_fn(x: np.ndarray) -&gt; EvaluationResult:\n        cost = float(np.sum((x - target) ** 2))\n        return EvaluationResult(\n            mean_cost=cost,\n            rollouts=jnp.zeros((1, 1, 3)),\n            params={},\n            iteration=0,\n        )\n\n    optimizer = EvoSaxOptimizer(strategy=\"CMA_ES\", population=10)\n    optimizer.setup_optimization(\n        initial_params=np.zeros(3),\n        evaluate_fn=evaluate_fn,\n    )\n\n    best = optimizer.optimize_all(iterations=50)\n\n    # Should converge close to target\n    assert best.mean_cost &lt; 0.1\n\n\n\n\nNew File: examples/autotune_evosax_comparison.py (~307 lines)\nPurpose: Compare evosax vs. cma library performance\nContents:\n\nSetup simple pendulum MPPI tuning task\nRun with CMAESOpt from cma library (baseline)\nRun with CMAESOpt from evosax\nRun with other evosax strategies (OpenES, Sep-CMA-ES)\nCompare:\n\nConvergence speed (iterations to threshold)\nWall-clock time\nFinal performance\n\nGenerate comparison plots:\n\nConvergence curves for each optimizer\nTime comparison bar chart\nParameter trajectory plots\n\n\nExpected outcome: Evosax should be faster in wall-clock time due to JIT compilation, especially with GPU.\n\n\n\n\n\n\nAdd evosax to the autotuning section:\n### Autotuning\n\nJAX-MPPI supports automatic hyperparameter tuning with multiple backends:\n\n- **CMA-ES** (via `cma` library) - Classic evolution strategy\n- **CMA-ES** (via `evosax`) - JAX-native, GPU-accelerated  ⚡ **NEW**\n- **Ray Tune** - Distributed hyperparameter search\n- **CMA-ME** (via `ribs`) - Quality diversity optimization\n\nInstall with: `pip install jax-mppi[autotuning] evosax`\n\n\n\nUpdate autotune.py module docstring with evosax example.\n\n\n\nDocument for users switching from cma to evosax:\n## Migrating from cma to evosax\n\n**Before:**\n```python\nfrom jax_mppi.autotune import CMAESOpt\nopt = CMAESOpt(population=10, sigma=0.1)\nAfter:\nfrom jax_mppi.autotune_evosax import CMAESOpt\nopt = CMAESOpt(population=10, sigma=0.1)\nBenefits: 5-10x faster with GPU acceleration"
  },
  {
    "objectID": "plan/completed/evosax_integration.html#implementation-details",
    "href": "plan/completed/evosax_integration.html#implementation-details",
    "title": "Evosax Integration Plan",
    "section": "",
    "text": "The main challenge is that user evaluation functions may not be JAX-pure.\nStrategy:\ndef _wrap_evaluation(evaluate_fn, maximize):\n    \"\"\"Wrap evaluation function for JAX compatibility.\"\"\"\n\n    # Try to detect if function is JAX-pure\n    # by checking if it uses only JAX operations\n\n    def jax_eval(x: jax.Array) -&gt; float:\n        # Convert to numpy for non-pure functions\n        x_np = np.array(x)\n        result = evaluate_fn(x_np)\n        cost = result.mean_cost\n        return -cost if maximize else cost\n\n    # For non-pure functions, use pure_callback\n    if not _is_jax_pure(evaluate_fn):\n        @jax.jit\n        def wrapped(x):\n            return jax.pure_callback(\n                jax_eval,\n                jax.ShapeDtypeStruct((), jnp.float32),\n                x\n            )\n        return wrapped\n    else:\n        return jax.jit(jax_eval)\n\n\n\nProvide both modes:\nclass EvoSaxOptimizer(Optimizer):\n    def __init__(self, ..., batched_evaluation: bool = False):\n        self.batched_evaluation = batched_evaluation\n\n    def optimize_step(self):\n        solutions = self.es_state.ask()\n\n        if self.batched_evaluation:\n            # Parallel evaluation with vmap\n            fitness = jax.vmap(self.evaluate_fn)(solutions)\n        else:\n            # Sequential evaluation (safer for non-pure functions)\n            fitness = jnp.array([\n                self.evaluate_fn(x) for x in solutions\n            ])\n\n        self.es_state = self.es_state.tell(fitness)\n        ...\n\n\n\nEvosax doesn’t natively handle box constraints. Options:\n\nRejection sampling: Reject invalid samples (wasteful)\nClipping: Clip to bounds after sampling (biases distribution)\nRepair: Project invalid samples to feasible region\nPenalty: Add penalty for constraint violation\n\nRecommendation: Use clipping (Option 2) for simplicity, add note in docs that proper constrained optimization should use constrained ES variants if needed.\ndef _apply_bounds(x, lower, upper):\n    \"\"\"Apply box constraints via clipping.\"\"\"\n    if lower is not None or upper is not None:\n        x = jnp.clip(x, lower, upper)\n    return x"
  },
  {
    "objectID": "plan/completed/evosax_integration.html#available-evosax-strategies",
    "href": "plan/completed/evosax_integration.html#available-evosax-strategies",
    "title": "Evosax Integration Plan",
    "section": "",
    "text": "Strategies to support (from evosax):\n\n\n\nCMA_ES - Classic Covariance Matrix Adaptation\nSep_CMA_ES - Separable CMA-ES (faster for high-dim)\nIPOP_CMA_ES - Increasing population CMA-ES\nBIPOP_CMA_ES - Bi-population CMA-ES\nOpenES - OpenAI’s Natural Evolution Strategies\nSNES - Separable Natural Evolution Strategies\nxNES - Exponential Natural Evolution Strategies\nSimpleGA - Simple Genetic Algorithm\nPersistentES - Persistent Evolution Strategies\nLES - Learned Evolution Strategies (meta-learned)\n\n\n\n\n\nDefault: CMA_ES (well-tested, robust)\nHigh-dimensional: Sep_CMA_ES (scales better)\nLarge population budget: OpenES (naturally parallelizable)"
  },
  {
    "objectID": "plan/completed/evosax_integration.html#migration-from-cma-library",
    "href": "plan/completed/evosax_integration.html#migration-from-cma-library",
    "title": "Evosax Integration Plan",
    "section": "",
    "text": "Feature\ncma library\nevosax\n\n\n\n\nLanguage\nPython + C\nPure JAX\n\n\nJIT compilation\n❌\n✅\n\n\nGPU acceleration\n❌\n✅\n\n\nBatched evaluation\n❌\n✅ (via vmap)\n\n\nIntegration with JAX code\n⚠️ (numpy conversion)\n✅ (native)\n\n\nAlgorithm variety\nCMA-ES variants only\n15+ strategies\n\n\nPerformance (CPU)\nGood\nSimilar\n\n\nPerformance (GPU)\nN/A\nExcellent (5-10x)\n\n\n\n\n\n\nUse cma library when:\n\nYou need the original CMA-ES implementation\nYour evaluation function has complex Python dependencies\nYou’re not using GPU\n\nUse evosax when:\n\nYou want GPU acceleration\nYour MPPI code is already JIT-compiled\nYou want to experiment with different ES algorithms\nYou need batched parallel evaluation"
  },
  {
    "objectID": "plan/completed/evosax_integration.html#testing-strategy",
    "href": "plan/completed/evosax_integration.html#testing-strategy",
    "title": "Evosax Integration Plan",
    "section": "",
    "text": "Test each strategy initialization\nTest optimize_step produces valid results\nTest parameter bounds enforcement\nTest with different parameter dimensions\n\n\n\n\n\nCompare convergence to cma library on same problems\nTest with actual MPPI parameter tuning\nVerify GPU execution (if GPU available)\n\n\n\n\n\nCompare wall-clock time vs cma library\nMeasure JIT compilation overhead\nProfile GPU vs CPU performance\n\n\n\n\n\nEnsure results are deterministic with fixed seed\nVerify backward compatibility with existing Optimizer API"
  },
  {
    "objectID": "plan/completed/evosax_integration.html#potential-issues-solutions",
    "href": "plan/completed/evosax_integration.html#potential-issues-solutions",
    "title": "Evosax Integration Plan",
    "section": "",
    "text": "Problem: First call to evosax optimizer incurs JIT compilation cost.\nSolution:\n\nDocument warmup requirement\nProvide warmup() method that JIT-compiles with dummy data\nConsider pre-compilation for common parameter dimensions\n\n\n\n\nProblem: Most user evaluation functions are not JAX-pure (use numpy, I/O, etc.).\nSolution:\n\nUse jax.pure_callback to wrap impure functions\nProvide clear error messages when incompatible operations are detected\nDocument limitations and workarounds\n\n\n\n\nProblem: GPU memory may be limited for large populations.\nSolution:\n\nAdd memory usage estimates in docs\nProvide chunk-based evaluation for very large populations\nDefault to reasonable population sizes\n\n\n\n\nProblem: JAX PRNG behaves differently than numpy.random.\nSolution:\n\nDocument PRNG handling\nEnsure reproducibility with fixed JAX random keys\nProvide utility to seed evosax optimizer"
  },
  {
    "objectID": "plan/completed/evosax_integration.html#timeline-estimate",
    "href": "plan/completed/evosax_integration.html#timeline-estimate",
    "title": "Evosax Integration Plan",
    "section": "",
    "text": "Step\nDescription\nEstimated Lines\nTime\n\n\n\n\n1\nAdd dependency to pyproject.toml\n~5\n5 min\n\n\n2\nImplement autotune_evosax.py\n~350\n4-6 hours\n\n\n3\nUpdate autotune.py docs\n~20\n30 min\n\n\n4\nUpdate init.py\n~5\n5 min\n\n\n5\nCreate test_autotune_evosax.py\n~250\n3-4 hours\n\n\n6\nCreate example comparison script\n~200\n2-3 hours\n\n\n7\nUpdate documentation\n~100\n1-2 hours\n\n\nTotal\n\n~930 lines\n11-16 hours"
  },
  {
    "objectID": "plan/completed/evosax_integration.html#success-criteria",
    "href": "plan/completed/evosax_integration.html#success-criteria",
    "title": "Evosax Integration Plan",
    "section": "",
    "text": "EvoSaxOptimizer implements Optimizer ABC correctly\nAll evosax strategies can be instantiated\nOptimization converges on test problems\nIntegration with existing Autotune class works\nParameter bounds are respected\n\n\n\n\n\nEvosax is faster than cma on GPU (&gt;2x speedup)\nEvosax is competitive with cma on CPU (within 20%)\nJIT compilation overhead is acceptable (&lt;5s for typical problems)\n\n\n\n\n\nAll tests pass (&gt;95% coverage)\nDocumentation is clear and complete\nExamples run without errors\nCode follows existing style conventions\n\n\n\n\n\nWorks with all parameter types (Lambda, NoiseSigma, Mu, Horizon)\nCompatible with existing Autotune orchestrator\nNo breaking changes to existing API\nOptional dependency (doesn’t break install if evosax not available)"
  },
  {
    "objectID": "plan/completed/evosax_integration.html#future-extensions",
    "href": "plan/completed/evosax_integration.html#future-extensions",
    "title": "Evosax Integration Plan",
    "section": "",
    "text": "Add more evosax strategies (GLD, LM-MA-ES, etc.)\nImplement proper constrained optimization variants\nAdd support for multi-objective optimization\nCreate Jupyter notebook tutorial\n\n\n\n\n\nIntegrate with autotune_qd.py for quality diversity\nAdd learned evolution strategies (LES) with meta-learning\nImplement adaptive strategy selection\nAdd visualization of ES state (e.g., covariance ellipsoids)\n\n\n\n\n\nDevelop JAX-native quality diversity framework\nAdd support for multi-fidelity optimization\nImplement distributed evosax with multi-GPU support\nCreate benchmarking suite comparing all optimizers"
  },
  {
    "objectID": "plan/completed/evosax_integration.html#references",
    "href": "plan/completed/evosax_integration.html#references",
    "title": "Evosax Integration Plan",
    "section": "",
    "text": "evosax documentation\nJAX documentation\nCurrent autotuning implementation: src/jax_mppi/autotune.py\nOriginal pytorch_mppi autotune: ../pytorch_mppi/src/pytorch_mppi/autotune.py"
  },
  {
    "objectID": "plan/completed/evosax_integration.html#open-questions",
    "href": "plan/completed/evosax_integration.html#open-questions",
    "title": "Evosax Integration Plan",
    "section": "",
    "text": "Should we deprecate the cma library backend?\n\nProbably not - keep both for compatibility\nUsers can choose based on their needs\n\nShould batched evaluation be default?\n\nNo - requires JAX-pure evaluation functions\nMake it opt-in with clear documentation\n\nWhich evosax strategies should have convenience classes?\n\nStart with: CMA_ES, Sep_CMA_ES, OpenES\nAdd more based on user feedback\n\nShould we add evosax to core dependencies or keep it optional?\n\nKeep optional - maintains lightweight core\nDocument installation clearly"
  },
  {
    "objectID": "plan/completed/evosax_integration.html#notes",
    "href": "plan/completed/evosax_integration.html#notes",
    "title": "Evosax Integration Plan",
    "section": "",
    "text": "Evosax is actively maintained by Robert Lange\nCurrent version: 0.1.x (check latest before implementing)\nJAX-native means entire optimization loop can run on GPU\nConsider adding evosax to CI/CD pipeline for testing"
  },
  {
    "objectID": "plan/example_performance_investigation.html",
    "href": "plan/example_performance_investigation.html",
    "title": "Performance Investigation: Quadrotor Examples",
    "section": "",
    "text": "This document details the findings regarding performance differences between the quadrotor control examples: quadrotor_hover.py, quadrotor_circle.py, and quadrotor_figure8_comparison.py.\n\n\n\nquadrotor_hover.py: Fast (~0.2s/step simulated).\nquadrotor_circle.py: Previously slow (~45x slower). Optimized to use jax.lax.scan for maximum performance.\nquadrotor_figure8_comparison.py: Previously slowest. Optimized to use jax.lax.scan for maximum performance.\n\n\n\n\nThe performance disparity was primarily due to the usage of JAX’s Just-In-Time (JIT) compilation and how cost functions are handled in the control loop.\n\n\nIn this example, the MPPI command function is explicitly JIT-compiled by the user, and the cost function is effectively constant (closed over static parameters).\n\n\n\nOriginally, this example re-created the cost function at every time step to update the reference target. This prevented JIT compilation of the main MPPI loop, forcing it to run in eager execution mode (or incurring massive re-compilation costs).\nOptimization Implemented: The implementation has been refactored to:\n\nUse step_dependent_dynamics=True to allow passing the time step t to the cost function.\nUse jax.lax.scan to wrap the entire simulation loop into a single JIT-compiled kernel. This eliminates Python loop dispatch overhead completely.\nPass the entire reference trajectory to the scan function and use jax.lax.dynamic_slice inside the loop to extract the current horizon’s reference. This allows the cost function to close over dynamic data efficiently without recompilation.\n\nParameter Tuning: To improve tracking performance, the following parameters were tuned:\n\nnum_samples: Increased from 1000 to 2000.\nhorizon: Increased from 30 to 50.\nlambda: Decreased from 1.0 to 0.1 (sharper selection).\nCost weights: Significantly increased position and velocity weights.\n\n\n\n\nThis example shared the same issue as quadrotor_circle.py but for three different controllers (mppi, smppi, kmppi).\nOptimization Implemented: Similar to quadrotor_circle.py, the controllers have been updated to use jax.lax.scan for the simulation loop. This required adapting the update logic for all three variants to be compatible with scan and dynamic reference slicing.\nParameter Tuning: Parameters were similarly tuned to handle the aggressive figure-8 trajectory (samples=2000, horizon=50, lambda=0.1).\n\n\n\n\nWhen implementing tracking controllers with JAX MPPI:\n\nUse jax.lax.scan: For simulation loops, wrapping the entire loop in scan provides the best performance by minimizing Python overhead.\nParametrize the Cost Function: Avoid capturing changing concrete values (like current target) in closures if they prevent JIT.\nUse Data Dependencies: Pass changing targets as arguments (Tracers) to the JIT-compiled function.\nStep-Dependent Dynamics: Use step_dependent_dynamics=True to utilize the relative time index t for looking up references in a passed trajectory slice."
  },
  {
    "objectID": "plan/example_performance_investigation.html#summary",
    "href": "plan/example_performance_investigation.html#summary",
    "title": "Performance Investigation: Quadrotor Examples",
    "section": "",
    "text": "quadrotor_hover.py: Fast (~0.2s/step simulated).\nquadrotor_circle.py: Previously slow (~45x slower). Optimized to use jax.lax.scan for maximum performance.\nquadrotor_figure8_comparison.py: Previously slowest. Optimized to use jax.lax.scan for maximum performance."
  },
  {
    "objectID": "plan/example_performance_investigation.html#root-cause-analysis",
    "href": "plan/example_performance_investigation.html#root-cause-analysis",
    "title": "Performance Investigation: Quadrotor Examples",
    "section": "",
    "text": "The performance disparity was primarily due to the usage of JAX’s Just-In-Time (JIT) compilation and how cost functions are handled in the control loop.\n\n\nIn this example, the MPPI command function is explicitly JIT-compiled by the user, and the cost function is effectively constant (closed over static parameters).\n\n\n\nOriginally, this example re-created the cost function at every time step to update the reference target. This prevented JIT compilation of the main MPPI loop, forcing it to run in eager execution mode (or incurring massive re-compilation costs).\nOptimization Implemented: The implementation has been refactored to:\n\nUse step_dependent_dynamics=True to allow passing the time step t to the cost function.\nUse jax.lax.scan to wrap the entire simulation loop into a single JIT-compiled kernel. This eliminates Python loop dispatch overhead completely.\nPass the entire reference trajectory to the scan function and use jax.lax.dynamic_slice inside the loop to extract the current horizon’s reference. This allows the cost function to close over dynamic data efficiently without recompilation.\n\nParameter Tuning: To improve tracking performance, the following parameters were tuned:\n\nnum_samples: Increased from 1000 to 2000.\nhorizon: Increased from 30 to 50.\nlambda: Decreased from 1.0 to 0.1 (sharper selection).\nCost weights: Significantly increased position and velocity weights.\n\n\n\n\nThis example shared the same issue as quadrotor_circle.py but for three different controllers (mppi, smppi, kmppi).\nOptimization Implemented: Similar to quadrotor_circle.py, the controllers have been updated to use jax.lax.scan for the simulation loop. This required adapting the update logic for all three variants to be compatible with scan and dynamic reference slicing.\nParameter Tuning: Parameters were similarly tuned to handle the aggressive figure-8 trajectory (samples=2000, horizon=50, lambda=0.1)."
  },
  {
    "objectID": "plan/example_performance_investigation.html#recommendation-for-future-reference",
    "href": "plan/example_performance_investigation.html#recommendation-for-future-reference",
    "title": "Performance Investigation: Quadrotor Examples",
    "section": "",
    "text": "When implementing tracking controllers with JAX MPPI:\n\nUse jax.lax.scan: For simulation loops, wrapping the entire loop in scan provides the best performance by minimizing Python overhead.\nParametrize the Cost Function: Avoid capturing changing concrete values (like current target) in closures if they prevent JIT.\nUse Data Dependencies: Pass changing targets as arguments (Tracers) to the JIT-compiled function.\nStep-Dependent Dynamics: Use step_dependent_dynamics=True to utilize the relative time index t for looking up references in a passed trajectory slice."
  },
  {
    "objectID": "plan/quadrotor_trajectory_following.html",
    "href": "plan/quadrotor_trajectory_following.html",
    "title": "Quadrotor Trajectory Following with MPPI",
    "section": "",
    "text": "Status: In Progress Branch: feat/quadrotor-traj-foll-example Created: 2026-02-01\n\n\nImplement a comprehensive set of examples demonstrating quadrotor trajectory following using MPPI control. The goal is to showcase the JAX-MPPI library’s capabilities on a realistic robotic system with nonlinear dynamics and provide a reference implementation for users.\n\n\n\nThe current JAX-MPPI library includes:\n\nThree MPPI variants: standard MPPI, SMPPI (smooth), and KMPPI (kernel-based)\nExamples: inverted pendulum, 2D navigation with obstacles\nWell-structured functional API with JIT compilation support\nAutotuning infrastructure for hyperparameter optimization\n\nA quadrotor trajectory following example will:\n\nDemonstrate MPPI on a high-dimensional nonlinear system (13D state space)\nShowcase reference tracking capabilities\nProvide a realistic robotics benchmark\nEnable comparison between MPPI variants for trajectory smoothness\n\n\n\n\nThe quadrotor is modeled as a rigid body with 6 degrees of freedom. The state space is 13-dimensional, representing position, velocity, orientation (quaternion), and angular velocity.\nFrame Conventions:\n\nNED (North-East-Down): Global/world frame where Z-axis points down (gravity is positive Z)\nFRD (Forward-Right-Down): Body frame where X-axis points forward, Y-axis points right, Z-axis points down\n\n\n\nThe state vector \\(\\mathbf{x} \\in \\mathbb{R}^{13}\\) is defined as:\n[ = [^T, ^T, ^T, ^T]^T ]\nwhere:\n\n\\(\\mathbf{p} = [p_x, p_y, p_z]^T\\) is the position in the NED world frame.\n\\(\\mathbf{v} = [v_x, v_y, v_z]^T\\) is the linear velocity in the NED world frame.\n\\(\\mathbf{q} = [q_w, q_x, q_y, q_z]^T\\) is the unit quaternion representing orientation (body FRD to world NED).\n\\(\\boldsymbol{\\omega} = [\\omega_x, \\omega_y, \\omega_z]^T\\) is the angular velocity in the FRD body frame.\n\nThe control input \\(\\mathbf{u} \\in \\mathbb{R}^{4}\\) consists of the total thrust and body angular rates:\n[ = [T, {x,cmd}, {y,cmd}, _{z,cmd}]^T ]\n\n\n\nThe system dynamics are governed by the following equations:\n\n\n[ = ]\n\n\n\n[ = + R()\n\\[\\begin{bmatrix} 0 \\\\ 0 \\\\ -T \\end{bmatrix}\\]\n]\nwhere \\(\\mathbf{g} = [0, 0, g]^T\\) is the gravity vector in NED frame (positive down), \\(m\\) is the mass, \\(T\\) is the thrust magnitude (positive), and \\(R(\\mathbf{q})\\) is the rotation matrix from FRD body frame to NED world frame. The thrust vector in body frame is \\([0, 0, -T]^T\\) (upward thrust is negative Z in FRD).\n\n\n\nThe time derivative of the quaternion is given by:\n[ =\n\\[\\begin{bmatrix} 0 \\\\ \\boldsymbol{\\omega} \\end{bmatrix}\\]\n]\nwhere \\(\\otimes\\) denotes quaternion multiplication. In matrix form involving the skew-symmetric matrix\n[ = () ]\nNote: The implementation must ensure \\(\\|\\mathbf{q}\\| = 1\\), typically by normalization after integration.\n\n\n\n[ = (_{cmd} - ) ]\nwhere \\(\\tau_\\omega\\) is the time constant for the angular velocity tracking.\n\n\n\n\nThe MPPI controller optimizes a cost function \\(J\\) over a horizon \\(H\\). The instantaneous cost \\(C(\\mathbf{x}_t, \\mathbf{u}_t)\\) is defined as\n[ C(t, t) = |t - {ref,t}|{Q{pos}}^2 + |t - {ref,t}|{Q{vel}}^2 + |t|{R}^2 ]\nwhere \\(\\|\\mathbf{z}\\|_W^2 = \\mathbf{z}^T W \\mathbf{z}\\).\n\n\n\n\n\n\n\n\n\n6-DOF rigid body dynamics\nState representation: position (3D), velocity (3D), orientation (quaternion), angular velocity (3D) = 13D\nControl inputs: body thrust + body rates (roll, pitch, yaw rates)\nPhysical parameters: mass, inertia matrix, arm length\nFull nonlinear dynamics with quaternion-based attitude representation\n\n\n\n\n\nMultiple reference trajectory types:\n\nCircular/helical trajectories\nLemniscate (figure-8) trajectories\nMinimum snap polynomial trajectories\nWaypoint-based trajectories\n\nTime-parameterized trajectories with position, velocity, acceleration references\n\n\n\n\n\nPosition tracking error (weighted L2 norm)\nVelocity tracking error\nAttitude tracking error (quaternion distance: 1 - |q^T q_ref|)\nControl effort penalty (R matrix on actions)\nTrajectory smoothness penalty (action rate limiting)\nTerminal cost for goal convergence\n\n\n\n\n\nExample 1: Basic hover control (stabilization around setpoint)\nExample 2: Circular trajectory following\nExample 3: Figure-8 trajectory with MPPI/SMPPI/KMPPI comparison\nExample 4: Minimum snap trajectory following\nExample 5: Obstacle avoidance during trajectory following (stretch goal)\n\n\n\n\n\n3D trajectory plots (reference vs actual)\nTracking error over time\nControl inputs over time\nEnergy consumption\nOptional: animated 3D quadrotor visualization\n\n\n\n\n\n\nPerformance: JIT-compiled control loops running at &gt;100 Hz on CPU\nCode Quality: Follow existing examples pattern (pendulum.py structure)\nDocumentation: Clear docstrings, inline comments for dynamics equations\nTesting: Unit tests for dynamics, cost functions, and integration tests\n\n\n\n\n\n\n\n\n\nstate = [\n    px, py, pz,        # position (3)\n    vx, vy, vz,        # velocity (3)\n    qw, qx, qy, qz,    # quaternion (4) - unit norm constraint\n    wx, wy, wz         # angular velocity in body frame (3)\n]\n\n\n\n\nQuaternions avoid gimbal lock singularities present in Euler angle representations\nMore numerically stable for aggressive maneuvers\nStandard representation in modern quadrotor control literature\nUnit norm constraint: ||q|| = 1 (enforced after integration)\n\n\n\n\n\n\n\naction = [\n    T,              # total thrust magnitude (N) - [0, max_thrust]\n                    # Acts in -Z direction of FRD body frame (upward)\n    wx_cmd,         # roll rate command (rad/s) - body X-axis (FRD forward)\n    wy_cmd,         # pitch rate command (rad/s) - body Y-axis (FRD right)\n    wz_cmd          # yaw rate command (rad/s) - body Z-axis (FRD down)\n]\n\n\n\n\nDirect control of thrust and angular velocities\nEasier to enforce control bounds than motor-level commands\nMore intuitive for trajectory tracking\nStandard in many quadrotor control frameworks\nFRD body frame convention: thrust acts in -Z direction (upward)\n\n\n\n\n\nImplement a modular dynamics function following the library’s pattern:\ndef quadrotor_dynamics(\n    state: jax.Array,\n    action: jax.Array,\n    dt: float = 0.01,\n    mass: float = 1.0,\n    inertia: jax.Array = jnp.eye(3) * 0.1,\n    gravity: float = 9.81,\n    tau_omega: float = 0.05  # angular velocity time constant\n) -&gt; jax.Array:\n    \"\"\"\n    6-DOF quadrotor dynamics with quaternion representation.\n    Frame conventions: NED (world), FRD (body)\n\n    State: [px, py, pz, vx, vy, vz, qw, qx, qy, qz, wx, wy, wz] (13D)\n    - Position/velocity in NED world frame\n    - Quaternion: body FRD to world NED\n    - Angular velocity in FRD body frame\n\n    Action: [T, wx_cmd, wy_cmd, wz_cmd] (4D)\n    - T: thrust magnitude (positive, acts in -Z body direction)\n    - w_cmd: angular rate commands in FRD body frame\n\n    Returns: next_state after dt using RK4 integration\n    \"\"\"\n    # Extract state components\n    pos = state[0:3]\n    vel = state[3:6]\n    quat = state[6:10]  # [qw, qx, qy, qz]\n    omega = state[10:13]  # angular velocity in FRD body frame\n\n    # Extract control\n    thrust = action[0]  # positive magnitude\n    omega_cmd = action[1:4]\n\n    # Rotation matrix from FRD body to NED world frame\n    R = quaternion_to_rotation_matrix(quat)\n\n    # Translational dynamics (NED world frame)\n    # Gravity: positive Z in NED (downward)\n    f_gravity = jnp.array([0, 0, mass * gravity])\n    # Thrust in body frame: [0, 0, -T] (upward in FRD)\n    # Transform to world frame\n    f_thrust = R @ jnp.array([0, 0, -thrust])\n    accel = (f_gravity + f_thrust) / mass\n\n    # Rotational dynamics (FRD body frame, first-order model)\n    # For more realism, can use: omega_dot = inv(I) @ (torque - omega x (I @ omega))\n    omega_dot = (omega_cmd - omega) / tau_omega\n\n    # Quaternion kinematics: q_dot = 0.5 * Omega(omega) @ q\n    # where Omega(omega) is the skew-symmetric matrix\n    q_dot = 0.5 * jnp.array([\n        -omega[0]*quat[1] - omega[1]*quat[2] - omega[2]*quat[3],  # qw_dot\n         omega[0]*quat[0] + omega[2]*quat[2] - omega[1]*quat[3],  # qx_dot\n         omega[1]*quat[0] - omega[2]*quat[1] + omega[0]*quat[3],  # qy_dot\n         omega[2]*quat[0] + omega[1]*quat[1] - omega[0]*quat[2]   # qz_dot\n    ])\n\n    # State derivative\n    state_dot = jnp.concatenate([vel, accel, q_dot, omega_dot])\n\n    # Integration (can use RK4 for better accuracy)\n    next_state = state + dt * state_dot\n\n    # Normalize quaternion to maintain unit norm\n    next_quat = next_state[6:10]\n    next_quat = next_quat / jnp.linalg.norm(next_quat)\n    next_state = next_state.at[6:10].set(next_quat)\n\n    return next_state\n\n\n\nFrame Conventions: NED world frame, FRD body frame\nGravity: Acts in +Z direction in NED (down is positive)\nThrust: Magnitude T (positive) acts in -Z direction in FRD (upward)\nQuaternion normalization after integration is critical\nRK4 integration recommended for better accuracy\nFirst-order model for angular velocity (can be extended to full Euler dynamics)\n\n\n\n\n\n\n\ndef trajectory_running_cost(\n    state: jax.Array,\n    action: jax.Array,\n    reference: jax.Array,\n    t: int,\n    Q_pos: jax.Array,\n    Q_vel: jax.Array,\n    Q_att: jax.Array,\n    R: jax.Array\n) -&gt; float:\n    \"\"\"\n    Trajectory tracking cost with control penalty.\n\n    reference: [px_ref, py_ref, pz_ref, vx_ref, vy_ref, vz_ref, ...]\n    \"\"\"\n    # Extract reference for current time step\n    ref_t = reference[t]  # or interpolate\n\n    # Position tracking error\n    pos_error = state[0:3] - ref_t[0:3]\n    cost_pos = pos_error.T @ Q_pos @ pos_error\n\n    # Velocity tracking error\n    vel_error = state[3:6] - ref_t[3:6]\n    cost_vel = vel_error.T @ Q_vel @ vel_error\n\n    # Attitude tracking (optional)\n    # att_error = ...\n    # cost_att = att_error.T @ Q_att @ att_error\n\n    # Control effort\n    cost_control = action.T @ R @ action\n\n    return cost_pos + cost_vel + cost_control\n\n\n\ndef trajectory_terminal_cost(\n    state: jax.Array,\n    last_action: jax.Array,\n    goal: jax.Array,\n    Q_terminal: jax.Array\n) -&gt; float:\n    \"\"\"Terminal cost for reaching goal state.\"\"\"\n    error = state - goal\n    return error.T @ Q_terminal @ error\n\n\n\n\nImplement modular trajectory generators:\ndef generate_circle_trajectory(\n    radius: float,\n    height: float,\n    period: float,\n    num_steps: int,\n    dt: float\n) -&gt; jax.Array:\n    \"\"\"\n    Generate circular trajectory in NED frame.\n    \n    Args:\n        radius: Circle radius in xy plane (m)\n        height: Altitude in NED frame (positive down, e.g., -5.0 for 5m above ground)\n        period: Period of one revolution (s)\n        num_steps: Number of trajectory points\n        dt: Time step (s)\n    \"\"\"\n    t = jnp.arange(num_steps) * dt\n    omega = 2 * jnp.pi / period\n\n    x = radius * jnp.cos(omega * t)\n    y = radius * jnp.sin(omega * t)\n    z = jnp.ones_like(t) * height  # NED: positive down\n\n    vx = -radius * omega * jnp.sin(omega * t)\n    vy = radius * omega * jnp.cos(omega * t)\n    vz = jnp.zeros_like(t)\n\n    # Stack into trajectory array\n    trajectory = jnp.stack([x, y, z, vx, vy, vz], axis=1)\n    return trajectory\n\n\ndef generate_lemniscate_trajectory(\n    scale: float,\n    height: float,\n    period: float,\n    num_steps: int,\n    dt: float\n) -&gt; jax.Array:\n    \"\"\"\n    Generate figure-8 (lemniscate) trajectory in NED frame.\n    \n    Args:\n        scale: Size of the figure-8 (m)\n        height: Altitude in NED frame (positive down, e.g., -5.0 for 5m above ground)\n        period: Period of one complete figure-8 (s)\n        num_steps: Number of trajectory points\n        dt: Time step (s)\n    \"\"\"\n    t = jnp.arange(num_steps) * dt\n    omega = 2 * jnp.pi / period\n\n    # Lemniscate of Gerono\n    x = scale * jnp.sin(omega * t)\n    y = scale * jnp.sin(omega * t) * jnp.cos(omega * t)\n    z = jnp.ones_like(t) * height  # NED: positive down\n\n    # Velocities (derivatives)\n    vx = scale * omega * jnp.cos(omega * t)\n    vy = scale * omega * (jnp.cos(omega * t)**2 - jnp.sin(omega * t)**2)\n    vz = jnp.zeros_like(t)\n\n    trajectory = jnp.stack([x, y, z, vx, vy, vz], axis=1)\n    return trajectory\n\n\n\n\n\n\n\nExplore existing codebase\nCreate feature branch feat/quadrotor-traj-foll-example\nDraft implementation plan\nImplement quadrotor dynamics module (src/jax_mppi/dynamics/quadrotor.py)\n\nQuaternion utilities (to rotation matrix, normalization, etc.)\nQuaternion kinematics\n6-DOF dynamics with RK4 integration\nUnit tests for dynamics (quaternion norm preservation, energy conservation)\n\nImplement trajectory cost functions (src/jax_mppi/costs/quadrotor.py)\n\nPosition/velocity tracking cost\nQuaternion-based attitude tracking cost\nTerminal cost\nUnit tests for costs\n\n\n\n\n\n\nCreate trajectory generation utilities (examples/quadrotor/trajectories.py)\n\nCircular trajectory\nFigure-8 (lemniscate) trajectory\nHover setpoint\nHelix trajectory (bonus)\nWaypoint interpolation with cubic Hermite splines\nTrajectory metrics computation\n\nUnit tests for trajectory generators (28 tests, all passing)\n\n\n\n\n\nExample 1: Hover control (examples/quadrotor_hover.py)\n\nStabilization around fixed setpoint\nVisualization of state vs time\nPerformance metrics (settling time, overshoot)\n\nExample 2: Circle following (examples/quadrotor_circle.py)\n\nCircular trajectory tracking\nTracking error visualization\nControl input visualization\n3D trajectory plotting\n\nIntegration tests (11 tests covering both examples)\n\n\n\n\n\nExample 3: Figure-8 comparison (examples/quadrotor_figure8_comparison.py)\n\nMPPI vs SMPPI vs KMPPI comparison\nSmoothness metrics (control rate, jerk)\nEnergy consumption comparison\nSide-by-side trajectory plots (6 subplots)\nComprehensive performance comparison table\n\nExample 4: Custom trajectory (examples/quadrotor_custom_trajectory.py)\n\nWaypoint-based trajectories with cubic Hermite interpolation\nUser-defined reference trajectories\nWaypoint passage verification\nCommand-line waypoint parsing\n\nIntegration tests (13 tests for advanced examples)\n\n\n\n\n\nAdd comprehensive docstrings\nCreate README for quadrotor examples (examples/quadrotor/README.md)\nAdd theory documentation (docs/examples/quadrotor.md)\nIntegration tests\nPerformance benchmarks\nUpdate main README with quadrotor examples\n\n\n\n\n\nObstacle avoidance during trajectory following\nFull Euler dynamics for rotational motion (torque-based control)\nMotor-level control (PWM to thrust mapping)\nWind disturbance modeling\nAutotuning example for quadrotor MPPI hyperparameters\nReal-time visualization with animation\nROS integration example\n\n\n\n\n\njax_mppi/\n├── src/jax_mppi/\n│   ├── dynamics/\n│   │   ├── __init__.py\n│   │   ├── linear.py\n│   │   └── quadrotor.py         # NEW: Quadrotor dynamics\n│   ├── costs/\n│   │   ├── __init__.py\n│   │   ├── basic.py\n│   │   └── quadrotor.py         # NEW: Quadrotor-specific costs\n│   └── ...\n├── examples/\n│   ├── pendulum.py\n│   ├── smooth_comparison.py\n│   ├── quadrotor/               # NEW: Quadrotor examples directory\n│   │   ├── __init__.py\n│   │   ├── trajectories.py      # NEW: Trajectory generators\n│   │   ├── plotting.py          # NEW: Visualization utilities\n│   │   └── README.md            # NEW: Quadrotor examples guide\n│   ├── quadrotor_hover.py       # NEW: Example 1\n│   ├── quadrotor_circle.py      # NEW: Example 2\n│   ├── quadrotor_figure8_comparison.py  # NEW: Example 3\n│   └── quadrotor_custom_trajectory.py   # NEW: Example 4\n├── tests/\n│   ├── test_quadrotor_dynamics.py  # NEW\n│   ├── test_quadrotor_costs.py     # NEW\n│   └── test_quadrotor_examples.py  # NEW\n└── docs/\n    ├── plan/\n    │   └── quadrotor_trajectory_following.md  # This file\n    └── examples/\n        └── quadrotor.md         # NEW: Theory and usage guide\n\n\n\n\nFunctionality: All examples run without errors and demonstrate trajectory following\nPerformance: Control loops run at &gt;100 Hz on CPU (JIT-compiled)\nAccuracy: Tracking error &lt;5% of trajectory scale for well-tuned parameters\nCode Quality: Follows existing code style, comprehensive tests (&gt;80% coverage)\nDocumentation: Clear README, docstrings, and theory documentation\nUsability: New users can run examples out-of-the-box with minimal setup\n\n\n\n\n\n\n\nDynamics model: verify state evolution, energy conservation\nCost functions: verify gradient correctness, cost bounds\nTrajectory generators: verify continuity, derivative correctness\n\n\n\n\n\nEnd-to-end MPPI control loop with quadrotor\nJIT compilation compatibility\nBatch rollout generation for visualization\n\n\n\n\n\nBenchmark control loop frequency\nMemory usage profiling\nComparison with baseline implementations\n\n\n\n\n\n\n\n\n\n\n\n\n\nRisk\nImpact\nMitigation\n\n\n\n\nDynamics too complex for real-time control\nHigh\nProfile performance early, optimize JIT compilation\n\n\nQuaternion norm drift during integration\nMedium\nNormalize after each integration step\n\n\nPoor tracking performance\nMedium\nImplement autotuning example, provide tuning guidelines\n\n\nIntegration complexity\nLow\nFollow existing example patterns closely\n\n\n\n\n\n\n\nJAX (already required)\nmatplotlib (for visualization, already used in examples)\nscipy (optional, for minimum snap trajectories)\nAll dependencies should be compatible with existing pyproject.toml\n\n\n\n\n\n[1] Williams, G., et al. “Information theoretic MPC for model-based reinforcement learning.” ICRA 2017.\n[2] Williams, G., et al. “Model predictive path integral control using covariance variable importance sampling.” arXiv:1509.01149, 2015.\n[3] Beard, R. W., & McLain, T. W. “Small Unmanned Aircraft: Theory and Practice.” Princeton University Press, 2012.\n[4] Mellinger, D., & Kumar, V. “Minimum snap trajectory generation and control for quadrotors.” ICRA 2011.\n[5] pytorch_mppi original implementation\n\n\n\n\n\nThis plan should be updated as implementation progresses\nMove to docs/plan/completed/ when all phases are finished\nLink any related issues or PRs here\n\n\n\nNED-FRD Convention:\n\nNED (North-East-Down): World/global frame\n\nX: North, Y: East, Z: Down (positive downward)\nGravity: g = [0, 0, +9.81] m/s² (positive Z direction)\n\nFRD (Forward-Right-Down): Body frame\n\nX: Forward, Y: Right, Z: Down (positive downward)\nThrust: T acts in -Z direction (upward thrust)\nAngular rates: [ωx, ωy, ωz] about [Forward, Right, Down] axes\n\n\nImportant Implementation Details:\n\nAltitude: Negative values indicate height above ground (e.g., z = -5.0 means 5m altitude)\nThrust: Positive magnitude T, applied as [0, 0, -T] in body frame\nRotation matrix R(q): transforms from FRD body to NED world\n\n\n\n\n\n\n\n\nCompleted:\n\nImplemented src/jax_mppi/dynamics/quadrotor.py with full 6-DOF quadrotor dynamics\n\nQuaternion utilities: rotation matrix conversion, normalization, multiplication\nRK4 integration for accurate numerical integration\nFirst-order angular velocity tracking model\nNED-FRD frame conventions properly implemented\nControl bounds enforcement\n\nImplemented src/jax_mppi/costs/quadrotor.py with comprehensive cost functions\n\nTrajectory tracking cost (position + velocity)\nTime-indexed trajectory cost\nHover control cost (with attitude tracking)\nTerminal cost for goal reaching\nQuaternion distance metric\n\nComprehensive test coverage (40 tests, all passing)\n\ntests/test_quadrotor_dynamics.py (19 tests)\ntests/test_quadrotor_costs.py (21 tests)\nTests verify: quaternion math, dynamics correctness, JIT compatibility, gradients\n\n\nKey Features:\n\nAll functions are JIT-compatible for high performance\nGradients work correctly through all dynamics and cost functions\nQuaternion norm preservation verified during integration\nPhysical behaviors validated (gravity, thrust, angular tracking)\n\nNext Steps:\n\nPhase 2: Trajectory generators (circle, figure-8, hover setpoint)\n\n\n\n\nCompleted:\n\nImplemented examples/quadrotor/trajectories.py with comprehensive trajectory generators\n\ngenerate_hover_setpoint() - Constant position stabilization\ngenerate_circle_trajectory() - Circular paths with configurable center and phase\ngenerate_lemniscate_trajectory() - Figure-8 patterns (horizontal or vertical)\ngenerate_helix_trajectory() - Spiral paths with vertical motion\ngenerate_waypoint_trajectory() - Smooth cubic Hermite interpolation through waypoints\ncompute_trajectory_metrics() - Analyze distance, velocity, acceleration\n\nComprehensive test coverage (28 tests, all passing)\n\nTests verify: trajectory shapes, periodicity, continuity\nValidates velocity/position relationships\nChecks metric computation accuracy\n\n\nKey Features:\n\nAll trajectories follow NED frame convention\nAnalytical derivatives for velocity (no numerical differentiation)\nConfigurable parameters (center, phase, duration, dt)\nSupport for both horizontal and vertical figure-8 patterns\n\nNext Steps:\n\nPhase 3: Basic examples (hover control, circle following)\n\n\n\n\nCompleted:\n\nImplemented examples/quadrotor_hover.py - Hover control stabilization\n\nMPPI-based hover controller with position and attitude tracking\nPerformance metrics (settling time, position/velocity error)\nComprehensive visualization (9 subplots: position, velocity, angular velocity, control inputs, errors, cost)\nCommand-line interface with configurable parameters\n\nImplemented examples/quadrotor_circle.py - Circular trajectory tracking\n\nTime-varying reference tracking using trajectory generators\n3D trajectory visualization with top-view projection\nTracking error analysis and metrics\nConfigurable circle parameters (radius, period)\n\nIntegration tests (11 tests)\n\nExample execution tests\nConvergence validation\nQuaternion norm preservation\nCost decrease verification\nCross-example compatibility checks\n\n\nKey Features:\n\nBoth examples run at 50 Hz control rate (JIT-compiled)\nDetailed visualizations saved to docs/media/\nProper NED frame convention throughout\nPerformance metrics automatically computed and reported\n\nNext Steps:\n\nPhase 4: Advanced examples (figure-8 comparison, custom trajectories)\n\n\n\n\nCompleted:\n\nImplemented examples/quadrotor_figure8_comparison.py - MPPI variant comparison\n\nSide-by-side comparison of MPPI, SMPPI, and KMPPI on aggressive figure-8\nComprehensive metrics: tracking accuracy, control smoothness, energy consumption\nSmoothness metrics: control rate (acceleration) and jerk analysis\n6-subplot visualization comparing all three variants\nPerformance comparison table with 7 key metrics\nDemonstrates trade-offs between tracking accuracy and control smoothness\n\nImplemented examples/quadrotor_custom_trajectory.py - Waypoint following\n\nUser-defined waypoint trajectories with smooth interpolation\nCubic Hermite splines for C1 continuity\nWaypoint passage verification and error reporting\nCommand-line interface for custom waypoint specification\n6-subplot visualization including waypoint markers\nDefault square pattern demonstration\n\nIntegration tests (13 tests)\n\nFigure-8 comparison execution and metrics validation\nCustom trajectory with various waypoint configurations\nQuaternion normalization across all controllers\nFinite value checks for all outputs\n\n\nKey Features:\n\nFigure-8 example shows SMPPI produces smoother control (lower jerk)\nCustom trajectory allows arbitrary waypoint sequences\nAll examples maintain 50 Hz control rate\nPublication-quality comparison visualizations\n\nNext Steps:\n\nPhase 5: Documentation and polish\n\n\nLast Updated: 2026-02-02 Author: riccardo-enr"
  },
  {
    "objectID": "plan/quadrotor_trajectory_following.html#objective",
    "href": "plan/quadrotor_trajectory_following.html#objective",
    "title": "Quadrotor Trajectory Following with MPPI",
    "section": "",
    "text": "Implement a comprehensive set of examples demonstrating quadrotor trajectory following using MPPI control. The goal is to showcase the JAX-MPPI library’s capabilities on a realistic robotic system with nonlinear dynamics and provide a reference implementation for users."
  },
  {
    "objectID": "plan/quadrotor_trajectory_following.html#background",
    "href": "plan/quadrotor_trajectory_following.html#background",
    "title": "Quadrotor Trajectory Following with MPPI",
    "section": "",
    "text": "The current JAX-MPPI library includes:\n\nThree MPPI variants: standard MPPI, SMPPI (smooth), and KMPPI (kernel-based)\nExamples: inverted pendulum, 2D navigation with obstacles\nWell-structured functional API with JIT compilation support\nAutotuning infrastructure for hyperparameter optimization\n\nA quadrotor trajectory following example will:\n\nDemonstrate MPPI on a high-dimensional nonlinear system (13D state space)\nShowcase reference tracking capabilities\nProvide a realistic robotics benchmark\nEnable comparison between MPPI variants for trajectory smoothness"
  },
  {
    "objectID": "plan/quadrotor_trajectory_following.html#theoretical-background",
    "href": "plan/quadrotor_trajectory_following.html#theoretical-background",
    "title": "Quadrotor Trajectory Following with MPPI",
    "section": "",
    "text": "The quadrotor is modeled as a rigid body with 6 degrees of freedom. The state space is 13-dimensional, representing position, velocity, orientation (quaternion), and angular velocity.\nFrame Conventions:\n\nNED (North-East-Down): Global/world frame where Z-axis points down (gravity is positive Z)\nFRD (Forward-Right-Down): Body frame where X-axis points forward, Y-axis points right, Z-axis points down\n\n\n\nThe state vector \\(\\mathbf{x} \\in \\mathbb{R}^{13}\\) is defined as:\n[ = [^T, ^T, ^T, ^T]^T ]\nwhere:\n\n\\(\\mathbf{p} = [p_x, p_y, p_z]^T\\) is the position in the NED world frame.\n\\(\\mathbf{v} = [v_x, v_y, v_z]^T\\) is the linear velocity in the NED world frame.\n\\(\\mathbf{q} = [q_w, q_x, q_y, q_z]^T\\) is the unit quaternion representing orientation (body FRD to world NED).\n\\(\\boldsymbol{\\omega} = [\\omega_x, \\omega_y, \\omega_z]^T\\) is the angular velocity in the FRD body frame.\n\nThe control input \\(\\mathbf{u} \\in \\mathbb{R}^{4}\\) consists of the total thrust and body angular rates:\n[ = [T, {x,cmd}, {y,cmd}, _{z,cmd}]^T ]\n\n\n\nThe system dynamics are governed by the following equations:\n\n\n[ = ]\n\n\n\n[ = + R()\n\\[\\begin{bmatrix} 0 \\\\ 0 \\\\ -T \\end{bmatrix}\\]\n]\nwhere \\(\\mathbf{g} = [0, 0, g]^T\\) is the gravity vector in NED frame (positive down), \\(m\\) is the mass, \\(T\\) is the thrust magnitude (positive), and \\(R(\\mathbf{q})\\) is the rotation matrix from FRD body frame to NED world frame. The thrust vector in body frame is \\([0, 0, -T]^T\\) (upward thrust is negative Z in FRD).\n\n\n\nThe time derivative of the quaternion is given by:\n[ =\n\\[\\begin{bmatrix} 0 \\\\ \\boldsymbol{\\omega} \\end{bmatrix}\\]\n]\nwhere \\(\\otimes\\) denotes quaternion multiplication. In matrix form involving the skew-symmetric matrix\n[ = () ]\nNote: The implementation must ensure \\(\\|\\mathbf{q}\\| = 1\\), typically by normalization after integration.\n\n\n\n[ = (_{cmd} - ) ]\nwhere \\(\\tau_\\omega\\) is the time constant for the angular velocity tracking.\n\n\n\n\nThe MPPI controller optimizes a cost function \\(J\\) over a horizon \\(H\\). The instantaneous cost \\(C(\\mathbf{x}_t, \\mathbf{u}_t)\\) is defined as\n[ C(t, t) = |t - {ref,t}|{Q{pos}}^2 + |t - {ref,t}|{Q{vel}}^2 + |t|{R}^2 ]\nwhere \\(\\|\\mathbf{z}\\|_W^2 = \\mathbf{z}^T W \\mathbf{z}\\)."
  },
  {
    "objectID": "plan/quadrotor_trajectory_following.html#requirements",
    "href": "plan/quadrotor_trajectory_following.html#requirements",
    "title": "Quadrotor Trajectory Following with MPPI",
    "section": "",
    "text": "6-DOF rigid body dynamics\nState representation: position (3D), velocity (3D), orientation (quaternion), angular velocity (3D) = 13D\nControl inputs: body thrust + body rates (roll, pitch, yaw rates)\nPhysical parameters: mass, inertia matrix, arm length\nFull nonlinear dynamics with quaternion-based attitude representation\n\n\n\n\n\nMultiple reference trajectory types:\n\nCircular/helical trajectories\nLemniscate (figure-8) trajectories\nMinimum snap polynomial trajectories\nWaypoint-based trajectories\n\nTime-parameterized trajectories with position, velocity, acceleration references\n\n\n\n\n\nPosition tracking error (weighted L2 norm)\nVelocity tracking error\nAttitude tracking error (quaternion distance: 1 - |q^T q_ref|)\nControl effort penalty (R matrix on actions)\nTrajectory smoothness penalty (action rate limiting)\nTerminal cost for goal convergence\n\n\n\n\n\nExample 1: Basic hover control (stabilization around setpoint)\nExample 2: Circular trajectory following\nExample 3: Figure-8 trajectory with MPPI/SMPPI/KMPPI comparison\nExample 4: Minimum snap trajectory following\nExample 5: Obstacle avoidance during trajectory following (stretch goal)\n\n\n\n\n\n3D trajectory plots (reference vs actual)\nTracking error over time\nControl inputs over time\nEnergy consumption\nOptional: animated 3D quadrotor visualization\n\n\n\n\n\n\nPerformance: JIT-compiled control loops running at &gt;100 Hz on CPU\nCode Quality: Follow existing examples pattern (pendulum.py structure)\nDocumentation: Clear docstrings, inline comments for dynamics equations\nTesting: Unit tests for dynamics, cost functions, and integration tests"
  },
  {
    "objectID": "plan/quadrotor_trajectory_following.html#technical-design",
    "href": "plan/quadrotor_trajectory_following.html#technical-design",
    "title": "Quadrotor Trajectory Following with MPPI",
    "section": "",
    "text": "state = [\n    px, py, pz,        # position (3)\n    vx, vy, vz,        # velocity (3)\n    qw, qx, qy, qz,    # quaternion (4) - unit norm constraint\n    wx, wy, wz         # angular velocity in body frame (3)\n]\n\n\n\n\nQuaternions avoid gimbal lock singularities present in Euler angle representations\nMore numerically stable for aggressive maneuvers\nStandard representation in modern quadrotor control literature\nUnit norm constraint: ||q|| = 1 (enforced after integration)\n\n\n\n\n\n\n\naction = [\n    T,              # total thrust magnitude (N) - [0, max_thrust]\n                    # Acts in -Z direction of FRD body frame (upward)\n    wx_cmd,         # roll rate command (rad/s) - body X-axis (FRD forward)\n    wy_cmd,         # pitch rate command (rad/s) - body Y-axis (FRD right)\n    wz_cmd          # yaw rate command (rad/s) - body Z-axis (FRD down)\n]\n\n\n\n\nDirect control of thrust and angular velocities\nEasier to enforce control bounds than motor-level commands\nMore intuitive for trajectory tracking\nStandard in many quadrotor control frameworks\nFRD body frame convention: thrust acts in -Z direction (upward)\n\n\n\n\n\nImplement a modular dynamics function following the library’s pattern:\ndef quadrotor_dynamics(\n    state: jax.Array,\n    action: jax.Array,\n    dt: float = 0.01,\n    mass: float = 1.0,\n    inertia: jax.Array = jnp.eye(3) * 0.1,\n    gravity: float = 9.81,\n    tau_omega: float = 0.05  # angular velocity time constant\n) -&gt; jax.Array:\n    \"\"\"\n    6-DOF quadrotor dynamics with quaternion representation.\n    Frame conventions: NED (world), FRD (body)\n\n    State: [px, py, pz, vx, vy, vz, qw, qx, qy, qz, wx, wy, wz] (13D)\n    - Position/velocity in NED world frame\n    - Quaternion: body FRD to world NED\n    - Angular velocity in FRD body frame\n\n    Action: [T, wx_cmd, wy_cmd, wz_cmd] (4D)\n    - T: thrust magnitude (positive, acts in -Z body direction)\n    - w_cmd: angular rate commands in FRD body frame\n\n    Returns: next_state after dt using RK4 integration\n    \"\"\"\n    # Extract state components\n    pos = state[0:3]\n    vel = state[3:6]\n    quat = state[6:10]  # [qw, qx, qy, qz]\n    omega = state[10:13]  # angular velocity in FRD body frame\n\n    # Extract control\n    thrust = action[0]  # positive magnitude\n    omega_cmd = action[1:4]\n\n    # Rotation matrix from FRD body to NED world frame\n    R = quaternion_to_rotation_matrix(quat)\n\n    # Translational dynamics (NED world frame)\n    # Gravity: positive Z in NED (downward)\n    f_gravity = jnp.array([0, 0, mass * gravity])\n    # Thrust in body frame: [0, 0, -T] (upward in FRD)\n    # Transform to world frame\n    f_thrust = R @ jnp.array([0, 0, -thrust])\n    accel = (f_gravity + f_thrust) / mass\n\n    # Rotational dynamics (FRD body frame, first-order model)\n    # For more realism, can use: omega_dot = inv(I) @ (torque - omega x (I @ omega))\n    omega_dot = (omega_cmd - omega) / tau_omega\n\n    # Quaternion kinematics: q_dot = 0.5 * Omega(omega) @ q\n    # where Omega(omega) is the skew-symmetric matrix\n    q_dot = 0.5 * jnp.array([\n        -omega[0]*quat[1] - omega[1]*quat[2] - omega[2]*quat[3],  # qw_dot\n         omega[0]*quat[0] + omega[2]*quat[2] - omega[1]*quat[3],  # qx_dot\n         omega[1]*quat[0] - omega[2]*quat[1] + omega[0]*quat[3],  # qy_dot\n         omega[2]*quat[0] + omega[1]*quat[1] - omega[0]*quat[2]   # qz_dot\n    ])\n\n    # State derivative\n    state_dot = jnp.concatenate([vel, accel, q_dot, omega_dot])\n\n    # Integration (can use RK4 for better accuracy)\n    next_state = state + dt * state_dot\n\n    # Normalize quaternion to maintain unit norm\n    next_quat = next_state[6:10]\n    next_quat = next_quat / jnp.linalg.norm(next_quat)\n    next_state = next_state.at[6:10].set(next_quat)\n\n    return next_state\n\n\n\nFrame Conventions: NED world frame, FRD body frame\nGravity: Acts in +Z direction in NED (down is positive)\nThrust: Magnitude T (positive) acts in -Z direction in FRD (upward)\nQuaternion normalization after integration is critical\nRK4 integration recommended for better accuracy\nFirst-order model for angular velocity (can be extended to full Euler dynamics)\n\n\n\n\n\n\n\ndef trajectory_running_cost(\n    state: jax.Array,\n    action: jax.Array,\n    reference: jax.Array,\n    t: int,\n    Q_pos: jax.Array,\n    Q_vel: jax.Array,\n    Q_att: jax.Array,\n    R: jax.Array\n) -&gt; float:\n    \"\"\"\n    Trajectory tracking cost with control penalty.\n\n    reference: [px_ref, py_ref, pz_ref, vx_ref, vy_ref, vz_ref, ...]\n    \"\"\"\n    # Extract reference for current time step\n    ref_t = reference[t]  # or interpolate\n\n    # Position tracking error\n    pos_error = state[0:3] - ref_t[0:3]\n    cost_pos = pos_error.T @ Q_pos @ pos_error\n\n    # Velocity tracking error\n    vel_error = state[3:6] - ref_t[3:6]\n    cost_vel = vel_error.T @ Q_vel @ vel_error\n\n    # Attitude tracking (optional)\n    # att_error = ...\n    # cost_att = att_error.T @ Q_att @ att_error\n\n    # Control effort\n    cost_control = action.T @ R @ action\n\n    return cost_pos + cost_vel + cost_control\n\n\n\ndef trajectory_terminal_cost(\n    state: jax.Array,\n    last_action: jax.Array,\n    goal: jax.Array,\n    Q_terminal: jax.Array\n) -&gt; float:\n    \"\"\"Terminal cost for reaching goal state.\"\"\"\n    error = state - goal\n    return error.T @ Q_terminal @ error\n\n\n\n\nImplement modular trajectory generators:\ndef generate_circle_trajectory(\n    radius: float,\n    height: float,\n    period: float,\n    num_steps: int,\n    dt: float\n) -&gt; jax.Array:\n    \"\"\"\n    Generate circular trajectory in NED frame.\n    \n    Args:\n        radius: Circle radius in xy plane (m)\n        height: Altitude in NED frame (positive down, e.g., -5.0 for 5m above ground)\n        period: Period of one revolution (s)\n        num_steps: Number of trajectory points\n        dt: Time step (s)\n    \"\"\"\n    t = jnp.arange(num_steps) * dt\n    omega = 2 * jnp.pi / period\n\n    x = radius * jnp.cos(omega * t)\n    y = radius * jnp.sin(omega * t)\n    z = jnp.ones_like(t) * height  # NED: positive down\n\n    vx = -radius * omega * jnp.sin(omega * t)\n    vy = radius * omega * jnp.cos(omega * t)\n    vz = jnp.zeros_like(t)\n\n    # Stack into trajectory array\n    trajectory = jnp.stack([x, y, z, vx, vy, vz], axis=1)\n    return trajectory\n\n\ndef generate_lemniscate_trajectory(\n    scale: float,\n    height: float,\n    period: float,\n    num_steps: int,\n    dt: float\n) -&gt; jax.Array:\n    \"\"\"\n    Generate figure-8 (lemniscate) trajectory in NED frame.\n    \n    Args:\n        scale: Size of the figure-8 (m)\n        height: Altitude in NED frame (positive down, e.g., -5.0 for 5m above ground)\n        period: Period of one complete figure-8 (s)\n        num_steps: Number of trajectory points\n        dt: Time step (s)\n    \"\"\"\n    t = jnp.arange(num_steps) * dt\n    omega = 2 * jnp.pi / period\n\n    # Lemniscate of Gerono\n    x = scale * jnp.sin(omega * t)\n    y = scale * jnp.sin(omega * t) * jnp.cos(omega * t)\n    z = jnp.ones_like(t) * height  # NED: positive down\n\n    # Velocities (derivatives)\n    vx = scale * omega * jnp.cos(omega * t)\n    vy = scale * omega * (jnp.cos(omega * t)**2 - jnp.sin(omega * t)**2)\n    vz = jnp.zeros_like(t)\n\n    trajectory = jnp.stack([x, y, z, vx, vy, vz], axis=1)\n    return trajectory"
  },
  {
    "objectID": "plan/quadrotor_trajectory_following.html#implementation-plan",
    "href": "plan/quadrotor_trajectory_following.html#implementation-plan",
    "title": "Quadrotor Trajectory Following with MPPI",
    "section": "",
    "text": "Explore existing codebase\nCreate feature branch feat/quadrotor-traj-foll-example\nDraft implementation plan\nImplement quadrotor dynamics module (src/jax_mppi/dynamics/quadrotor.py)\n\nQuaternion utilities (to rotation matrix, normalization, etc.)\nQuaternion kinematics\n6-DOF dynamics with RK4 integration\nUnit tests for dynamics (quaternion norm preservation, energy conservation)\n\nImplement trajectory cost functions (src/jax_mppi/costs/quadrotor.py)\n\nPosition/velocity tracking cost\nQuaternion-based attitude tracking cost\nTerminal cost\nUnit tests for costs\n\n\n\n\n\n\nCreate trajectory generation utilities (examples/quadrotor/trajectories.py)\n\nCircular trajectory\nFigure-8 (lemniscate) trajectory\nHover setpoint\nHelix trajectory (bonus)\nWaypoint interpolation with cubic Hermite splines\nTrajectory metrics computation\n\nUnit tests for trajectory generators (28 tests, all passing)\n\n\n\n\n\nExample 1: Hover control (examples/quadrotor_hover.py)\n\nStabilization around fixed setpoint\nVisualization of state vs time\nPerformance metrics (settling time, overshoot)\n\nExample 2: Circle following (examples/quadrotor_circle.py)\n\nCircular trajectory tracking\nTracking error visualization\nControl input visualization\n3D trajectory plotting\n\nIntegration tests (11 tests covering both examples)\n\n\n\n\n\nExample 3: Figure-8 comparison (examples/quadrotor_figure8_comparison.py)\n\nMPPI vs SMPPI vs KMPPI comparison\nSmoothness metrics (control rate, jerk)\nEnergy consumption comparison\nSide-by-side trajectory plots (6 subplots)\nComprehensive performance comparison table\n\nExample 4: Custom trajectory (examples/quadrotor_custom_trajectory.py)\n\nWaypoint-based trajectories with cubic Hermite interpolation\nUser-defined reference trajectories\nWaypoint passage verification\nCommand-line waypoint parsing\n\nIntegration tests (13 tests for advanced examples)\n\n\n\n\n\nAdd comprehensive docstrings\nCreate README for quadrotor examples (examples/quadrotor/README.md)\nAdd theory documentation (docs/examples/quadrotor.md)\nIntegration tests\nPerformance benchmarks\nUpdate main README with quadrotor examples\n\n\n\n\n\nObstacle avoidance during trajectory following\nFull Euler dynamics for rotational motion (torque-based control)\nMotor-level control (PWM to thrust mapping)\nWind disturbance modeling\nAutotuning example for quadrotor MPPI hyperparameters\nReal-time visualization with animation\nROS integration example"
  },
  {
    "objectID": "plan/quadrotor_trajectory_following.html#file-structure",
    "href": "plan/quadrotor_trajectory_following.html#file-structure",
    "title": "Quadrotor Trajectory Following with MPPI",
    "section": "",
    "text": "jax_mppi/\n├── src/jax_mppi/\n│   ├── dynamics/\n│   │   ├── __init__.py\n│   │   ├── linear.py\n│   │   └── quadrotor.py         # NEW: Quadrotor dynamics\n│   ├── costs/\n│   │   ├── __init__.py\n│   │   ├── basic.py\n│   │   └── quadrotor.py         # NEW: Quadrotor-specific costs\n│   └── ...\n├── examples/\n│   ├── pendulum.py\n│   ├── smooth_comparison.py\n│   ├── quadrotor/               # NEW: Quadrotor examples directory\n│   │   ├── __init__.py\n│   │   ├── trajectories.py      # NEW: Trajectory generators\n│   │   ├── plotting.py          # NEW: Visualization utilities\n│   │   └── README.md            # NEW: Quadrotor examples guide\n│   ├── quadrotor_hover.py       # NEW: Example 1\n│   ├── quadrotor_circle.py      # NEW: Example 2\n│   ├── quadrotor_figure8_comparison.py  # NEW: Example 3\n│   └── quadrotor_custom_trajectory.py   # NEW: Example 4\n├── tests/\n│   ├── test_quadrotor_dynamics.py  # NEW\n│   ├── test_quadrotor_costs.py     # NEW\n│   └── test_quadrotor_examples.py  # NEW\n└── docs/\n    ├── plan/\n    │   └── quadrotor_trajectory_following.md  # This file\n    └── examples/\n        └── quadrotor.md         # NEW: Theory and usage guide"
  },
  {
    "objectID": "plan/quadrotor_trajectory_following.html#success-criteria",
    "href": "plan/quadrotor_trajectory_following.html#success-criteria",
    "title": "Quadrotor Trajectory Following with MPPI",
    "section": "",
    "text": "Functionality: All examples run without errors and demonstrate trajectory following\nPerformance: Control loops run at &gt;100 Hz on CPU (JIT-compiled)\nAccuracy: Tracking error &lt;5% of trajectory scale for well-tuned parameters\nCode Quality: Follows existing code style, comprehensive tests (&gt;80% coverage)\nDocumentation: Clear README, docstrings, and theory documentation\nUsability: New users can run examples out-of-the-box with minimal setup"
  },
  {
    "objectID": "plan/quadrotor_trajectory_following.html#testing-strategy",
    "href": "plan/quadrotor_trajectory_following.html#testing-strategy",
    "title": "Quadrotor Trajectory Following with MPPI",
    "section": "",
    "text": "Dynamics model: verify state evolution, energy conservation\nCost functions: verify gradient correctness, cost bounds\nTrajectory generators: verify continuity, derivative correctness\n\n\n\n\n\nEnd-to-end MPPI control loop with quadrotor\nJIT compilation compatibility\nBatch rollout generation for visualization\n\n\n\n\n\nBenchmark control loop frequency\nMemory usage profiling\nComparison with baseline implementations"
  },
  {
    "objectID": "plan/quadrotor_trajectory_following.html#risk-mitigation",
    "href": "plan/quadrotor_trajectory_following.html#risk-mitigation",
    "title": "Quadrotor Trajectory Following with MPPI",
    "section": "",
    "text": "Risk\nImpact\nMitigation\n\n\n\n\nDynamics too complex for real-time control\nHigh\nProfile performance early, optimize JIT compilation\n\n\nQuaternion norm drift during integration\nMedium\nNormalize after each integration step\n\n\nPoor tracking performance\nMedium\nImplement autotuning example, provide tuning guidelines\n\n\nIntegration complexity\nLow\nFollow existing example patterns closely"
  },
  {
    "objectID": "plan/quadrotor_trajectory_following.html#dependencies",
    "href": "plan/quadrotor_trajectory_following.html#dependencies",
    "title": "Quadrotor Trajectory Following with MPPI",
    "section": "",
    "text": "JAX (already required)\nmatplotlib (for visualization, already used in examples)\nscipy (optional, for minimum snap trajectories)\nAll dependencies should be compatible with existing pyproject.toml"
  },
  {
    "objectID": "plan/quadrotor_trajectory_following.html#references",
    "href": "plan/quadrotor_trajectory_following.html#references",
    "title": "Quadrotor Trajectory Following with MPPI",
    "section": "",
    "text": "[1] Williams, G., et al. “Information theoretic MPC for model-based reinforcement learning.” ICRA 2017.\n[2] Williams, G., et al. “Model predictive path integral control using covariance variable importance sampling.” arXiv:1509.01149, 2015.\n[3] Beard, R. W., & McLain, T. W. “Small Unmanned Aircraft: Theory and Practice.” Princeton University Press, 2012.\n[4] Mellinger, D., & Kumar, V. “Minimum snap trajectory generation and control for quadrotors.” ICRA 2011.\n[5] pytorch_mppi original implementation"
  },
  {
    "objectID": "plan/quadrotor_trajectory_following.html#notes",
    "href": "plan/quadrotor_trajectory_following.html#notes",
    "title": "Quadrotor Trajectory Following with MPPI",
    "section": "",
    "text": "This plan should be updated as implementation progresses\nMove to docs/plan/completed/ when all phases are finished\nLink any related issues or PRs here\n\n\n\nNED-FRD Convention:\n\nNED (North-East-Down): World/global frame\n\nX: North, Y: East, Z: Down (positive downward)\nGravity: g = [0, 0, +9.81] m/s² (positive Z direction)\n\nFRD (Forward-Right-Down): Body frame\n\nX: Forward, Y: Right, Z: Down (positive downward)\nThrust: T acts in -Z direction (upward thrust)\nAngular rates: [ωx, ωy, ωz] about [Forward, Right, Down] axes\n\n\nImportant Implementation Details:\n\nAltitude: Negative values indicate height above ground (e.g., z = -5.0 means 5m altitude)\nThrust: Positive magnitude T, applied as [0, 0, -T] in body frame\nRotation matrix R(q): transforms from FRD body to NED world"
  },
  {
    "objectID": "plan/quadrotor_trajectory_following.html#progress-log",
    "href": "plan/quadrotor_trajectory_following.html#progress-log",
    "title": "Quadrotor Trajectory Following with MPPI",
    "section": "",
    "text": "Completed:\n\nImplemented src/jax_mppi/dynamics/quadrotor.py with full 6-DOF quadrotor dynamics\n\nQuaternion utilities: rotation matrix conversion, normalization, multiplication\nRK4 integration for accurate numerical integration\nFirst-order angular velocity tracking model\nNED-FRD frame conventions properly implemented\nControl bounds enforcement\n\nImplemented src/jax_mppi/costs/quadrotor.py with comprehensive cost functions\n\nTrajectory tracking cost (position + velocity)\nTime-indexed trajectory cost\nHover control cost (with attitude tracking)\nTerminal cost for goal reaching\nQuaternion distance metric\n\nComprehensive test coverage (40 tests, all passing)\n\ntests/test_quadrotor_dynamics.py (19 tests)\ntests/test_quadrotor_costs.py (21 tests)\nTests verify: quaternion math, dynamics correctness, JIT compatibility, gradients\n\n\nKey Features:\n\nAll functions are JIT-compatible for high performance\nGradients work correctly through all dynamics and cost functions\nQuaternion norm preservation verified during integration\nPhysical behaviors validated (gravity, thrust, angular tracking)\n\nNext Steps:\n\nPhase 2: Trajectory generators (circle, figure-8, hover setpoint)\n\n\n\n\nCompleted:\n\nImplemented examples/quadrotor/trajectories.py with comprehensive trajectory generators\n\ngenerate_hover_setpoint() - Constant position stabilization\ngenerate_circle_trajectory() - Circular paths with configurable center and phase\ngenerate_lemniscate_trajectory() - Figure-8 patterns (horizontal or vertical)\ngenerate_helix_trajectory() - Spiral paths with vertical motion\ngenerate_waypoint_trajectory() - Smooth cubic Hermite interpolation through waypoints\ncompute_trajectory_metrics() - Analyze distance, velocity, acceleration\n\nComprehensive test coverage (28 tests, all passing)\n\nTests verify: trajectory shapes, periodicity, continuity\nValidates velocity/position relationships\nChecks metric computation accuracy\n\n\nKey Features:\n\nAll trajectories follow NED frame convention\nAnalytical derivatives for velocity (no numerical differentiation)\nConfigurable parameters (center, phase, duration, dt)\nSupport for both horizontal and vertical figure-8 patterns\n\nNext Steps:\n\nPhase 3: Basic examples (hover control, circle following)\n\n\n\n\nCompleted:\n\nImplemented examples/quadrotor_hover.py - Hover control stabilization\n\nMPPI-based hover controller with position and attitude tracking\nPerformance metrics (settling time, position/velocity error)\nComprehensive visualization (9 subplots: position, velocity, angular velocity, control inputs, errors, cost)\nCommand-line interface with configurable parameters\n\nImplemented examples/quadrotor_circle.py - Circular trajectory tracking\n\nTime-varying reference tracking using trajectory generators\n3D trajectory visualization with top-view projection\nTracking error analysis and metrics\nConfigurable circle parameters (radius, period)\n\nIntegration tests (11 tests)\n\nExample execution tests\nConvergence validation\nQuaternion norm preservation\nCost decrease verification\nCross-example compatibility checks\n\n\nKey Features:\n\nBoth examples run at 50 Hz control rate (JIT-compiled)\nDetailed visualizations saved to docs/media/\nProper NED frame convention throughout\nPerformance metrics automatically computed and reported\n\nNext Steps:\n\nPhase 4: Advanced examples (figure-8 comparison, custom trajectories)\n\n\n\n\nCompleted:\n\nImplemented examples/quadrotor_figure8_comparison.py - MPPI variant comparison\n\nSide-by-side comparison of MPPI, SMPPI, and KMPPI on aggressive figure-8\nComprehensive metrics: tracking accuracy, control smoothness, energy consumption\nSmoothness metrics: control rate (acceleration) and jerk analysis\n6-subplot visualization comparing all three variants\nPerformance comparison table with 7 key metrics\nDemonstrates trade-offs between tracking accuracy and control smoothness\n\nImplemented examples/quadrotor_custom_trajectory.py - Waypoint following\n\nUser-defined waypoint trajectories with smooth interpolation\nCubic Hermite splines for C1 continuity\nWaypoint passage verification and error reporting\nCommand-line interface for custom waypoint specification\n6-subplot visualization including waypoint markers\nDefault square pattern demonstration\n\nIntegration tests (13 tests)\n\nFigure-8 comparison execution and metrics validation\nCustom trajectory with various waypoint configurations\nQuaternion normalization across all controllers\nFinite value checks for all outputs\n\n\nKey Features:\n\nFigure-8 example shows SMPPI produces smoother control (lower jerk)\nCustom trajectory allows arbitrary waypoint sequences\nAll examples maintain 50 Hz control rate\nPublication-quality comparison visualizations\n\nNext Steps:\n\nPhase 5: Documentation and polish\n\n\nLast Updated: 2026-02-02 Author: riccardo-enr"
  },
  {
    "objectID": "testing.html",
    "href": "testing.html",
    "title": "Testing Guide",
    "section": "",
    "text": "This guide explains the testing stack for jax_mppi and provides instructions on how to run and write tests.\n\n\nThe project uses pytest for running tests. You can run all tests using uv:\nuv run pytest\nTo run a specific test file:\nuv run pytest tests/test_mppi.py\nTo run a specific test case:\nuv run pytest tests/test_mppi.py::TestMPPICommand::test_command_returns_correct_shapes\n\n\n\nThe tests are located in the tests/ directory and mirror the source code structure where appropriate. The test suite is divided into several files, each covering a specific flavor or aspect of the library.\n\n\n\ntests/test_mppi.py: Tests for the base MPPI implementation (jax_mppi.mppi).\n\nGoal: Ensure the correctness of the core algorithm, state management, and configuration options.\nScope:\n\nInitialization: Verifies that create() returns correct shapes and types for config and state.\nCommand Generation: Tests the command() function to ensure it generates valid actions within bounds and correctly updates the state.\nConfiguration Options: Validates various settings like u_per_command (multi-step control), step_dependent_dynamics (time-varying systems), sample_null_action (ensuring baseline inclusion), and u_scale (control authority scaling).\nIntegration: Includes basic convergence tests to verify that the cost decreases over iterations (e.g., TestMPPIIntegration).\n\n\ntests/test_smppi.py: Tests for Smooth MPPI (jax_mppi.smppi).\n\nGoal: Verify that the “smooth” variant correctly operates in the lifted velocity control space and produces continuous action sequences.\nScope:\n\nLifted Space: Checks that the internal state (U) represents control velocity/acceleration, while action_sequence represents the integrated actions.\nSmoothness: Verifies that the smoothness cost penalty (w_action_seq_cost) effectively reduces action variance.\nBounds: Tests that bounds are respected for both the control velocity (u_min/u_max) and the final action (action_min/action_max).\nContinuity: checks that the shift operation maintains continuity in the action space, preventing jumps during receding horizon updates.\n\n\ntests/test_kmppi.py: Tests for Kernel MPPI (jax_mppi.kmppi).\n\nGoal: Ensure that kernel-based interpolation works correctly and that optimization occurs effectively in the reduced control point space.\nScope:\n\nKernels: Tests the properties of time-domain kernels (e.g., RBFKernel), such as shape and distance decay.\nInterpolation: Verifies that control points (theta) are correctly mapped to full trajectories (U) via _kernel_interpolate, preserving values at control points.\nOptimization: Checks that the MPPI update rule is applied to the control points (theta) rather than the full trajectory.\nSmoothness: Confirms that the resulting trajectories are smooth due to the kernel properties (e.g., by checking second derivatives).\n\n\n\n\n\n\n\ntests/test_pendulum.py: End-to-end integration tests using a Pendulum environment.\n\nGoal: Validate that the algorithms can solve a concrete, non-linear control task.\nScope:\n\nStabilization: Tests if MPPI can stabilize the pendulum at the upright position.\nSwing-up: Tests the more difficult task of swinging up from a hanging position.\nPhysics: Sanity checks the pendulum dynamics and cost functions.\n\n\n\n\n\n\n\ntests/test_autotune.py: Unit tests for the autotuning framework (jax_mppi.autotune).\n\nGoal: Verify the components of the hyperparameter optimization system.\n\ntests/test_autotune_integration.py: Integration tests for autotuning.\n\nGoal: Ensure that the autotuner can successfully improve performance on a benchmark task (finding better parameters than the default).\n\n\n\n\n\n\nWhen adding new features or fixing bugs, please add corresponding tests.\n\nLocate the appropriate test file: If you are modifying mppi.py, add tests to tests/test_mppi.py.\nUse Class-Based Structure: Group related tests into classes (e.g., TestMPPIBasics, TestMPPICommand).\nProperty-Based Testing: Where possible, test properties (e.g., “output shape depends on input shape in this way”) rather than just hardcoded values.\nIntegration Tests: For significant algorithmic changes, ensure that tests/test_pendulum.py still passes or add a similar simple control task to verify efficacy.\nJAX Compatibility: Ensure tests check that functions can be JIT-compiled if they are intended to be used within jax.jit.\n\n\n\ndef test_new_feature(self):\n    nx, nu = 2, 1\n    config, state = mppi.create(nx=nx, nu=nu, noise_sigma=jnp.eye(nu))\n\n    # ... perform action ...\n    action, new_state = mppi.command(config, state, ...)\n\n    # ... assert expected behavior ...\n    assert action.shape == (nu,)",
    "crumbs": [
      "Home",
      "Development",
      "Testing Guide"
    ]
  },
  {
    "objectID": "testing.html#running-tests",
    "href": "testing.html#running-tests",
    "title": "Testing Guide",
    "section": "",
    "text": "The project uses pytest for running tests. You can run all tests using uv:\nuv run pytest\nTo run a specific test file:\nuv run pytest tests/test_mppi.py\nTo run a specific test case:\nuv run pytest tests/test_mppi.py::TestMPPICommand::test_command_returns_correct_shapes",
    "crumbs": [
      "Home",
      "Development",
      "Testing Guide"
    ]
  },
  {
    "objectID": "testing.html#test-suite-structure",
    "href": "testing.html#test-suite-structure",
    "title": "Testing Guide",
    "section": "",
    "text": "The tests are located in the tests/ directory and mirror the source code structure where appropriate. The test suite is divided into several files, each covering a specific flavor or aspect of the library.\n\n\n\ntests/test_mppi.py: Tests for the base MPPI implementation (jax_mppi.mppi).\n\nGoal: Ensure the correctness of the core algorithm, state management, and configuration options.\nScope:\n\nInitialization: Verifies that create() returns correct shapes and types for config and state.\nCommand Generation: Tests the command() function to ensure it generates valid actions within bounds and correctly updates the state.\nConfiguration Options: Validates various settings like u_per_command (multi-step control), step_dependent_dynamics (time-varying systems), sample_null_action (ensuring baseline inclusion), and u_scale (control authority scaling).\nIntegration: Includes basic convergence tests to verify that the cost decreases over iterations (e.g., TestMPPIIntegration).\n\n\ntests/test_smppi.py: Tests for Smooth MPPI (jax_mppi.smppi).\n\nGoal: Verify that the “smooth” variant correctly operates in the lifted velocity control space and produces continuous action sequences.\nScope:\n\nLifted Space: Checks that the internal state (U) represents control velocity/acceleration, while action_sequence represents the integrated actions.\nSmoothness: Verifies that the smoothness cost penalty (w_action_seq_cost) effectively reduces action variance.\nBounds: Tests that bounds are respected for both the control velocity (u_min/u_max) and the final action (action_min/action_max).\nContinuity: checks that the shift operation maintains continuity in the action space, preventing jumps during receding horizon updates.\n\n\ntests/test_kmppi.py: Tests for Kernel MPPI (jax_mppi.kmppi).\n\nGoal: Ensure that kernel-based interpolation works correctly and that optimization occurs effectively in the reduced control point space.\nScope:\n\nKernels: Tests the properties of time-domain kernels (e.g., RBFKernel), such as shape and distance decay.\nInterpolation: Verifies that control points (theta) are correctly mapped to full trajectories (U) via _kernel_interpolate, preserving values at control points.\nOptimization: Checks that the MPPI update rule is applied to the control points (theta) rather than the full trajectory.\nSmoothness: Confirms that the resulting trajectories are smooth due to the kernel properties (e.g., by checking second derivatives).\n\n\n\n\n\n\n\ntests/test_pendulum.py: End-to-end integration tests using a Pendulum environment.\n\nGoal: Validate that the algorithms can solve a concrete, non-linear control task.\nScope:\n\nStabilization: Tests if MPPI can stabilize the pendulum at the upright position.\nSwing-up: Tests the more difficult task of swinging up from a hanging position.\nPhysics: Sanity checks the pendulum dynamics and cost functions.\n\n\n\n\n\n\n\ntests/test_autotune.py: Unit tests for the autotuning framework (jax_mppi.autotune).\n\nGoal: Verify the components of the hyperparameter optimization system.\n\ntests/test_autotune_integration.py: Integration tests for autotuning.\n\nGoal: Ensure that the autotuner can successfully improve performance on a benchmark task (finding better parameters than the default).",
    "crumbs": [
      "Home",
      "Development",
      "Testing Guide"
    ]
  },
  {
    "objectID": "testing.html#writing-new-tests",
    "href": "testing.html#writing-new-tests",
    "title": "Testing Guide",
    "section": "",
    "text": "When adding new features or fixing bugs, please add corresponding tests.\n\nLocate the appropriate test file: If you are modifying mppi.py, add tests to tests/test_mppi.py.\nUse Class-Based Structure: Group related tests into classes (e.g., TestMPPIBasics, TestMPPICommand).\nProperty-Based Testing: Where possible, test properties (e.g., “output shape depends on input shape in this way”) rather than just hardcoded values.\nIntegration Tests: For significant algorithmic changes, ensure that tests/test_pendulum.py still passes or add a similar simple control task to verify efficacy.\nJAX Compatibility: Ensure tests check that functions can be JIT-compiled if they are intended to be used within jax.jit.\n\n\n\ndef test_new_feature(self):\n    nx, nu = 2, 1\n    config, state = mppi.create(nx=nx, nu=nu, noise_sigma=jnp.eye(nu))\n\n    # ... perform action ...\n    action, new_state = mppi.command(config, state, ...)\n\n    # ... assert expected behavior ...\n    assert action.shape == (nu,)",
    "crumbs": [
      "Home",
      "Development",
      "Testing Guide"
    ]
  },
  {
    "objectID": "releasing.html",
    "href": "releasing.html",
    "title": "Releasing Guide",
    "section": "",
    "text": "This guide describes how to release a new version of jax_mppi to PyPI.\n\n\nThe release process is automated using GitHub Actions, but it requires the repository to be configured as a Trusted Publisher on PyPI.\n\n\n\nLog in to your PyPI account.\nGo to Publishing in your project settings (or create a new project if this is the first release).\nAdd a new Trusted Publisher.\nSelect GitHub.\nEnter the following details:\n\nOwner: riccardo-enr\nRepository name: jax_mppi\nWorkflow name: publish.yml\nEnvironment name: (Leave empty)\n\nClick Add.\n\nThis allows the GitHub Action to authenticate with PyPI using OIDC tokens without needing a long-lived API token or password.\n\n\n\n\nTo release a new version:\n\nUpdate Version: Update the version number in pyproject.toml:\n[project]\nversion = \"0.1.6\"  # Example version\nCommit and Push: Commit the version change and push to main.\ngit add pyproject.toml\ngit commit -m \"Bump version to 0.1.6\"\ngit push origin main\nCreate Tag: Create a git tag for the release. The tag must start with v.\ngit tag v0.1.6\ngit push origin v0.1.6\nWait for Action: The Publish to PyPI GitHub Action will automatically run when the tag is pushed. It will build the package and upload it to PyPI.\nVerify: Check the PyPI page to confirm the new version is available.",
    "crumbs": [
      "Home",
      "Development",
      "Releasing Guide"
    ]
  },
  {
    "objectID": "releasing.html#prerequisites",
    "href": "releasing.html#prerequisites",
    "title": "Releasing Guide",
    "section": "",
    "text": "The release process is automated using GitHub Actions, but it requires the repository to be configured as a Trusted Publisher on PyPI.\n\n\n\nLog in to your PyPI account.\nGo to Publishing in your project settings (or create a new project if this is the first release).\nAdd a new Trusted Publisher.\nSelect GitHub.\nEnter the following details:\n\nOwner: riccardo-enr\nRepository name: jax_mppi\nWorkflow name: publish.yml\nEnvironment name: (Leave empty)\n\nClick Add.\n\nThis allows the GitHub Action to authenticate with PyPI using OIDC tokens without needing a long-lived API token or password.",
    "crumbs": [
      "Home",
      "Development",
      "Releasing Guide"
    ]
  },
  {
    "objectID": "releasing.html#release-process",
    "href": "releasing.html#release-process",
    "title": "Releasing Guide",
    "section": "",
    "text": "To release a new version:\n\nUpdate Version: Update the version number in pyproject.toml:\n[project]\nversion = \"0.1.6\"  # Example version\nCommit and Push: Commit the version change and push to main.\ngit add pyproject.toml\ngit commit -m \"Bump version to 0.1.6\"\ngit push origin main\nCreate Tag: Create a git tag for the release. The tag must start with v.\ngit tag v0.1.6\ngit push origin v0.1.6\nWait for Action: The Publish to PyPI GitHub Action will automatically run when the tag is pushed. It will build the package and upload it to PyPI.\nVerify: Check the PyPI page to confirm the new version is available.",
    "crumbs": [
      "Home",
      "Development",
      "Releasing Guide"
    ]
  },
  {
    "objectID": "i_mppi.html",
    "href": "i_mppi.html",
    "title": "I-MPPI: Informative Model Predictive Path Integral",
    "section": "",
    "text": "The field of autonomous Unmanned Aerial Vehicle (UAV) exploration has transitioned from simple geometric coverage to complex, information-driven strategic maneuvers. In unstructured environments, a robot faces a fundamental duality: the global coverage problem (“where to go”) and the reactive control problem (“how to move safely”). Traditional approaches often decouple these modules, resulting in “myopic” local planners that fail to escape local minima of uncertainty, or deterministic global planners that produce coarse, “jagged” paths unsuitable for the high-speed, non-linear dynamics of agile flight.\nThis document describes the Hierarchical Informative Model Predictive Path Integral (I-MPPI) framework. This architecture synthesizes global strategic planning, analytical viewpoint refinement via Fast Shannon Mutual Information (FSMI), and reactive Biased-MPPI control with sensitivity-based feedback.\n\n\n\n\n\n\nNoteRepository Scope\n\n\n\nThe focus of this repository is the high-performance CUDA implementation of Layer 2 (Local Refinement) and Layer 3/4 (Reactive Control). The Global Planner (Layer 1), such as FUEL, is considered an external input that provides the mission context and global waypoints.",
    "crumbs": [
      "Home",
      "Algorithms",
      "I-MPPI: Informative Model Predictive Path Integral"
    ]
  },
  {
    "objectID": "i_mppi.html#optimal-control-duality-free-energy",
    "href": "i_mppi.html#optimal-control-duality-free-energy",
    "title": "I-MPPI: Informative Model Predictive Path Integral",
    "section": "Optimal Control Duality & Free Energy",
    "text": "Optimal Control Duality & Free Energy\nThe mathematical derivation of the MPPI algorithm is rooted in the definition of the free energy of the dynamical system. The value function \\(V(x, t)\\) of a stochastic system can be linearized through a logarithmic transformation, leading to the Path Integral formulation. The Free Energy (\\(\\mathcal{F}\\)) of the dynamical system is defined as:\n\\[ \\mathcal{F}(x_0) = -\\lambda \\log \\mathbb{E}_{\\mathbb{P}} \\left[ \\exp \\left( -\\frac{1}{\\lambda} S(\\tau) \\right) \\right] \\]\n\n\\(\\tau\\): The state-control trajectory \\(\\{x_0, u_0, x_1, u_1, \\dots, x_T\\}\\).\n\\(\\mathbb{P}\\): The base distribution, representing the stochastic trajectories of the “passive” system.\n\\(S(\\tau)\\): The cumulative cost (Action) of a trajectory \\(\\tau\\).\n\\(\\lambda\\): The temperature parameter, representing the noise variance.\n\nThe “optimal trajectory” is the mean of the distribution \\(\\mathbb{Q}^*\\) that minimizes the KL-divergence to the distribution of “low-cost” paths, leading to the thermodynamic weight update rule:\n\\[ \\omega^k = \\frac{\\exp\\left(-\\frac{1}{\\lambda}(J^k - \\rho)\\right)}{\\sum_{j=1}^{K} \\exp\\left(-\\frac{1}{\\lambda}(J^j - \\rho)\\right)} \\]",
    "crumbs": [
      "Home",
      "Algorithms",
      "I-MPPI: Informative Model Predictive Path Integral"
    ]
  },
  {
    "objectID": "i_mppi.html#shannon-mutual-information",
    "href": "i_mppi.html#shannon-mutual-information",
    "title": "I-MPPI: Informative Model Predictive Path Integral",
    "section": "Shannon Mutual Information",
    "text": "Shannon Mutual Information\nThe informative reward is the Shannon Mutual Information (MI) between the map \\(M\\) and a future sensor measurement \\(Z\\): \\[ I(M; Z) = H(M) - H(M|Z) \\] where \\(H(M)\\) is the map entropy. High MI indicates regions of unknown space or uncertain areas.",
    "crumbs": [
      "Home",
      "Algorithms",
      "I-MPPI: Informative Model Predictive Path Integral"
    ]
  },
  {
    "objectID": "i_mppi.html#fast-shannon-mutual-information-fsmi",
    "href": "i_mppi.html#fast-shannon-mutual-information-fsmi",
    "title": "I-MPPI: Informative Model Predictive Path Integral",
    "section": "Fast Shannon Mutual Information (FSMI)",
    "text": "Fast Shannon Mutual Information (FSMI)\nComputing Shannon MI analytically for a sensor beam involves:\n\nRaycasting: Cast a ray intersecting \\(n\\) cells with occupancy \\(o_i\\).\nVisibility (\\(P(e_j)\\)): Probability that the beam reaches cell \\(j\\): \\[ P(e_j) = o_j \\prod_{k=1}^{j-1} (1 - o_k) \\]\nInformation Density (\\(C_k\\)): Cumulative info gain based on inverse sensor model.\nAnalytic Summation: \\[ I_{FSMI} = \\sum_{j=0}^{n} \\sum_{k=1}^{n} P(e_j) C_k G_{k,j} \\]\n\nThe Uniform-FSMI variant simplifies this to \\(O(n)\\) complexity: \\[ I \\approx \\sum_{j=0}^{n} P(e_j) \\frac{D_{j+H} - D_{j-H-1}}{2H + 1} \\]",
    "crumbs": [
      "Home",
      "Algorithms",
      "I-MPPI: Informative Model Predictive Path Integral"
    ]
  },
  {
    "objectID": "i_mppi.html#exploration-campaign",
    "href": "i_mppi.html#exploration-campaign",
    "title": "I-MPPI: Informative Model Predictive Path Integral",
    "section": "Exploration Campaign",
    "text": "Exploration Campaign\nTo verify the effectiveness of the informative reward, we conducted a simulation campaign in a “corridor with a hole” scenario. Both controllers are tasked with reaching a common goal at \\((9, 5)\\). The map contains two high-entropy “unknown” zones located away from the direct path.\n\nHierarchical Architecture Specification\nThe goal for the I-MPPI simulation is to verify the effectiveness of the implementation of layer 2 and layer 3 of the hierarchical architecture.\nA higher-level FSMI-driven trajectory generator should precede the I-MPPI module. - Frequency: Runs at a lower rate (1-5 Hz). - Logic: 1. Explore Area 1: Prioritize regions with high information interest and lower dynamical cost. 2. Transition: As Area 1 is explored, its information value drops to zero. 3. Explore Area 2: Move to the next area of interest. 4. Drive to Goal: When information cost is depleted, drive to the goal minimizing only the dynamical cost.\n\nStandard MPPI: Simply moves towards the target, ignoring the high-entropy regions.\nInformative MPPI: Proactively deviates from the shortest path to explore the unknown areas, gathering information before proceeding to the goal.\nThe two zones are marked with Yellow Rectangles. They should have a high importance in the cost function. But after they get explored, their importance should decrease, thus the I-MPPI is free to move towards the goal. This should simulate the behaviour of the FSMI only trajectory refiner which gives a goal taking into account the uncertainty of the map.\n\n\n\n\nI-MPPI Exploration Campaign. The heatmap shows the cost field (darker is better). The Gold Star is the common goal. Yellow Rectangles denote high-interest (unknown) zones. The Cyan Lines represent walls. Standard MPPI (black dashed) goes direct, while Informative MPPI (white solid) deviates to explore both blobs before reaching the goal.\n\n\n\n\nPerformance Comparison\nThe Informative MPPI demonstrates a clear bias towards high-entropy regions of the map. By minimizing the Unified Cost Function, the controller naturally seeks out viewpoints that maximize sensor coverage of unknown space, effectively “falling” into informative gravity wells as predicted by the theory.",
    "crumbs": [
      "Home",
      "Algorithms",
      "I-MPPI: Informative Model Predictive Path Integral"
    ]
  },
  {
    "objectID": "examples/pendulum.html",
    "href": "examples/pendulum.html",
    "title": "Pendulum Swing-Up",
    "section": "",
    "text": "This example demonstrates how to use jax_mppi to control an inverted pendulum. The goal is to swing the pendulum up from a hanging position and stabilize it at the top.\n\n\nThe full example code is available in examples/pendulum.py.\n\n\n\nThe pendulum dynamics are defined as a pure function:\ndef pendulum_dynamics(state: jax.Array, action: jax.Array) -&gt; jax.Array:\n    \"\"\"Pendulum dynamics.\n\n    State: [theta, theta_dot]\n        theta: angle from upright (0 = upright, pi = hanging down)\n        theta_dot: angular velocity\n    Action: [torque]\n        torque: applied torque (control input)\n    \"\"\"\n    g = 10.0  # gravity\n    m = 1.0  # mass\n    l = 1.0  # length\n    dt = 0.05  # timestep\n\n    theta, theta_dot = state[0], state[1]\n    torque = action[0]\n\n    # Clip torque to reasonable bounds\n    torque = jnp.clip(torque, -2.0, 2.0)\n\n    # Pendulum dynamics: theta_ddot = (torque - m*g*l*sin(theta)) / (m*l^2)\n    theta_ddot = (torque - m * g * l * jnp.sin(theta)) / (m * l * l)\n\n    # Euler integration\n    theta_dot_next = theta_dot + theta_ddot * dt\n    theta_next = theta + theta_dot_next * dt\n\n    # Normalize angle to [-pi, pi]\n    theta_next = ((theta_next + jnp.pi) % (2 * jnp.pi)) - jnp.pi\n\n    return jnp.array([theta_next, theta_dot_next])\n\n\n\nThe running cost penalizes deviation from the upright position and high control effort:\ndef pendulum_cost(state: jax.Array, action: jax.Array) -&gt; jax.Array:\n    theta, theta_dot = state[0], state[1]\n    torque = action[0]\n\n    # Cost for being away from upright (theta=0)\n    angle_cost = theta**2\n\n    # Cost for high angular velocity\n    velocity_cost = 0.1 * theta_dot**2\n\n    # Cost for using torque\n    control_cost = 0.01 * torque**2\n\n    return angle_cost + velocity_cost + control_cost\n\n\n\nYou can run the example using:\npython examples/pendulum.py --visualize",
    "crumbs": [
      "Home",
      "Examples",
      "Pendulum Swing-Up"
    ]
  },
  {
    "objectID": "examples/pendulum.html#code",
    "href": "examples/pendulum.html#code",
    "title": "Pendulum Swing-Up",
    "section": "",
    "text": "The full example code is available in examples/pendulum.py.",
    "crumbs": [
      "Home",
      "Examples",
      "Pendulum Swing-Up"
    ]
  },
  {
    "objectID": "examples/pendulum.html#dynamics",
    "href": "examples/pendulum.html#dynamics",
    "title": "Pendulum Swing-Up",
    "section": "",
    "text": "The pendulum dynamics are defined as a pure function:\ndef pendulum_dynamics(state: jax.Array, action: jax.Array) -&gt; jax.Array:\n    \"\"\"Pendulum dynamics.\n\n    State: [theta, theta_dot]\n        theta: angle from upright (0 = upright, pi = hanging down)\n        theta_dot: angular velocity\n    Action: [torque]\n        torque: applied torque (control input)\n    \"\"\"\n    g = 10.0  # gravity\n    m = 1.0  # mass\n    l = 1.0  # length\n    dt = 0.05  # timestep\n\n    theta, theta_dot = state[0], state[1]\n    torque = action[0]\n\n    # Clip torque to reasonable bounds\n    torque = jnp.clip(torque, -2.0, 2.0)\n\n    # Pendulum dynamics: theta_ddot = (torque - m*g*l*sin(theta)) / (m*l^2)\n    theta_ddot = (torque - m * g * l * jnp.sin(theta)) / (m * l * l)\n\n    # Euler integration\n    theta_dot_next = theta_dot + theta_ddot * dt\n    theta_next = theta + theta_dot_next * dt\n\n    # Normalize angle to [-pi, pi]\n    theta_next = ((theta_next + jnp.pi) % (2 * jnp.pi)) - jnp.pi\n\n    return jnp.array([theta_next, theta_dot_next])",
    "crumbs": [
      "Home",
      "Examples",
      "Pendulum Swing-Up"
    ]
  },
  {
    "objectID": "examples/pendulum.html#cost-function",
    "href": "examples/pendulum.html#cost-function",
    "title": "Pendulum Swing-Up",
    "section": "",
    "text": "The running cost penalizes deviation from the upright position and high control effort:\ndef pendulum_cost(state: jax.Array, action: jax.Array) -&gt; jax.Array:\n    theta, theta_dot = state[0], state[1]\n    torque = action[0]\n\n    # Cost for being away from upright (theta=0)\n    angle_cost = theta**2\n\n    # Cost for high angular velocity\n    velocity_cost = 0.1 * theta_dot**2\n\n    # Cost for using torque\n    control_cost = 0.01 * torque**2\n\n    return angle_cost + velocity_cost + control_cost",
    "crumbs": [
      "Home",
      "Examples",
      "Pendulum Swing-Up"
    ]
  },
  {
    "objectID": "examples/pendulum.html#running-the-example",
    "href": "examples/pendulum.html#running-the-example",
    "title": "Pendulum Swing-Up",
    "section": "",
    "text": "You can run the example using:\npython examples/pendulum.py --visualize",
    "crumbs": [
      "Home",
      "Examples",
      "Pendulum Swing-Up"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "jax_mppi",
    "section": "",
    "text": "jax_mppi is a functional, JIT-compilable port of the pytorch_mppi library to JAX. It implements Model Predictive Path Integral (MPPI) control with a focus on performance and composability.\n\n\nThis library embraces JAX’s functional paradigm:\n\nPure Functions: Core logic is implemented as pure functions command(state, mppi_state) -&gt; (action, mppi_state).\nDataclass State: State is held in jax.tree_util.register_dataclass containers, allowing easy integration with jit, vmap, and grad.\nNo Side Effects: Unlike the PyTorch version, there is no mutable self. State transitions are explicit.\n\n\n\n\n\nCore MPPI: Robust implementation of the standard MPPI algorithm.\nSmooth MPPI (SMPPI): Maintains action sequences and smoothness costs for better trajectory generation.\nKernel MPPI (KMPPI): Uses kernel interpolation for control points, reducing the parameter space.\nAutotuning: Built-in hyperparameter optimization using CMA-ES, Ray Tune, and Quality Diversity.\nCUDA/C++ Backend: High-performance implementations of all controllers in CUDA/C++17, exposed to Python via `nanobind`.\nJAX Integration:\n\njax.vmap for efficient batch processing.\njax.lax.scan for fast horizon loops.\nFully compatible with JIT compilation for high-performance control loops.\n\n\n\n\n\n# Clone the repository\ngit clone https://github.com/yourusername/jax_mppi.git\ncd jax_mppi\n\n# Install dependencies\npip install -e .\n\n\n\nimport jax\nimport jax.numpy as jnp\nfrom jax_mppi import mppi\n\n# Define dynamics and cost functions\ndef dynamics(state, action):\n    # Your dynamics model here\n    return state + action\n\ndef running_cost(state, action):\n    # Your cost function here\n    return jnp.sum(state**2) + jnp.sum(action**2)\n\n# Create configuration and initial state\nconfig, mppi_state = mppi.create(\n    nx=4, nu=2,\n    noise_sigma=jnp.eye(2) * 0.1,\n    horizon=20,\n    lambda_=1.0\n)\n\n# Control loop\nkey = jax.random.PRNGKey(0)\ncurrent_obs = jnp.zeros(4)\n\n# JIT compile the command function for performance\njitted_command = jax.jit(mppi.command, static_argnames=['dynamics', 'running_cost'])\n\nfor _ in range(100):\n    key, subkey = jax.random.split(key)\n    action, mppi_state = jitted_command(\n        config,\n        mppi_state,\n        current_obs,\n        dynamics=dynamics,\n        running_cost=running_cost\n    )\n    # Apply action to environment...\n\n\n\njax_mppi/\n├── src/jax_mppi/\n│   ├── mppi.py              # Core MPPI implementation\n│   ├── smppi.py             # Smooth MPPI variant\n│   ├── kmppi.py             # Kernel MPPI variant\n│   ├── types.py             # Type definitions\n│   ├── autotune.py          # Autotuning core & CMA-ES\n│   ├── autotune_global.py   # Ray Tune integration\n│   └── autotune_qd.py       # Quality Diversity optimization\n├── examples/\n│   ├── pendulum.py          # Pendulum environment example\n│   ├── autotune_basic.py    # Basic autotuning example\n│   ├── autotune_pendulum.py # Autotuning pendulum\n│   └── smooth_comparison.py # Comparison of MPPI variants\n└── tests/                   # Unit and integration tests\n\n\n\nThe development is structured in phases:\n\nCore MPPI: Basic implementation with JAX parity.\nIntegration: Pendulum example and verification.\nSmooth MPPI: Implementation of smoothness constraints.\nKernel MPPI: Kernel-based control parameterization.\nComparisons: Benchmarking and visual comparisons.\nAutotuning: Parameter optimization using CMA-ES, Ray Tune, and QD.\n\n\n\n\nThis project is a direct port of pytorch_mppi. We aim to maintain parity with the original implementation while leveraging JAX’s unique features for performance and flexibility.\n\n\n\n\nPorting Plan\nEvosax Integration Plan\nCUDA MPPI Implementation Plan",
    "crumbs": [
      "Home",
      "Introduction",
      "jax_mppi"
    ]
  },
  {
    "objectID": "index.html#design-philosophy",
    "href": "index.html#design-philosophy",
    "title": "jax_mppi",
    "section": "",
    "text": "This library embraces JAX’s functional paradigm:\n\nPure Functions: Core logic is implemented as pure functions command(state, mppi_state) -&gt; (action, mppi_state).\nDataclass State: State is held in jax.tree_util.register_dataclass containers, allowing easy integration with jit, vmap, and grad.\nNo Side Effects: Unlike the PyTorch version, there is no mutable self. State transitions are explicit.",
    "crumbs": [
      "Home",
      "Introduction",
      "jax_mppi"
    ]
  },
  {
    "objectID": "index.html#key-features",
    "href": "index.html#key-features",
    "title": "jax_mppi",
    "section": "",
    "text": "Core MPPI: Robust implementation of the standard MPPI algorithm.\nSmooth MPPI (SMPPI): Maintains action sequences and smoothness costs for better trajectory generation.\nKernel MPPI (KMPPI): Uses kernel interpolation for control points, reducing the parameter space.\nAutotuning: Built-in hyperparameter optimization using CMA-ES, Ray Tune, and Quality Diversity.\nCUDA/C++ Backend: High-performance implementations of all controllers in CUDA/C++17, exposed to Python via `nanobind`.\nJAX Integration:\n\njax.vmap for efficient batch processing.\njax.lax.scan for fast horizon loops.\nFully compatible with JIT compilation for high-performance control loops.",
    "crumbs": [
      "Home",
      "Introduction",
      "jax_mppi"
    ]
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "jax_mppi",
    "section": "",
    "text": "# Clone the repository\ngit clone https://github.com/yourusername/jax_mppi.git\ncd jax_mppi\n\n# Install dependencies\npip install -e .",
    "crumbs": [
      "Home",
      "Introduction",
      "jax_mppi"
    ]
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "jax_mppi",
    "section": "",
    "text": "import jax\nimport jax.numpy as jnp\nfrom jax_mppi import mppi\n\n# Define dynamics and cost functions\ndef dynamics(state, action):\n    # Your dynamics model here\n    return state + action\n\ndef running_cost(state, action):\n    # Your cost function here\n    return jnp.sum(state**2) + jnp.sum(action**2)\n\n# Create configuration and initial state\nconfig, mppi_state = mppi.create(\n    nx=4, nu=2,\n    noise_sigma=jnp.eye(2) * 0.1,\n    horizon=20,\n    lambda_=1.0\n)\n\n# Control loop\nkey = jax.random.PRNGKey(0)\ncurrent_obs = jnp.zeros(4)\n\n# JIT compile the command function for performance\njitted_command = jax.jit(mppi.command, static_argnames=['dynamics', 'running_cost'])\n\nfor _ in range(100):\n    key, subkey = jax.random.split(key)\n    action, mppi_state = jitted_command(\n        config,\n        mppi_state,\n        current_obs,\n        dynamics=dynamics,\n        running_cost=running_cost\n    )\n    # Apply action to environment...",
    "crumbs": [
      "Home",
      "Introduction",
      "jax_mppi"
    ]
  },
  {
    "objectID": "index.html#project-structure",
    "href": "index.html#project-structure",
    "title": "jax_mppi",
    "section": "",
    "text": "jax_mppi/\n├── src/jax_mppi/\n│   ├── mppi.py              # Core MPPI implementation\n│   ├── smppi.py             # Smooth MPPI variant\n│   ├── kmppi.py             # Kernel MPPI variant\n│   ├── types.py             # Type definitions\n│   ├── autotune.py          # Autotuning core & CMA-ES\n│   ├── autotune_global.py   # Ray Tune integration\n│   └── autotune_qd.py       # Quality Diversity optimization\n├── examples/\n│   ├── pendulum.py          # Pendulum environment example\n│   ├── autotune_basic.py    # Basic autotuning example\n│   ├── autotune_pendulum.py # Autotuning pendulum\n│   └── smooth_comparison.py # Comparison of MPPI variants\n└── tests/                   # Unit and integration tests",
    "crumbs": [
      "Home",
      "Introduction",
      "jax_mppi"
    ]
  },
  {
    "objectID": "index.html#roadmap",
    "href": "index.html#roadmap",
    "title": "jax_mppi",
    "section": "",
    "text": "The development is structured in phases:\n\nCore MPPI: Basic implementation with JAX parity.\nIntegration: Pendulum example and verification.\nSmooth MPPI: Implementation of smoothness constraints.\nKernel MPPI: Kernel-based control parameterization.\nComparisons: Benchmarking and visual comparisons.\nAutotuning: Parameter optimization using CMA-ES, Ray Tune, and QD.",
    "crumbs": [
      "Home",
      "Introduction",
      "jax_mppi"
    ]
  },
  {
    "objectID": "index.html#credits",
    "href": "index.html#credits",
    "title": "jax_mppi",
    "section": "",
    "text": "This project is a direct port of pytorch_mppi. We aim to maintain parity with the original implementation while leveraging JAX’s unique features for performance and flexibility.",
    "crumbs": [
      "Home",
      "Introduction",
      "jax_mppi"
    ]
  },
  {
    "objectID": "index.html#completed-plans",
    "href": "index.html#completed-plans",
    "title": "jax_mppi",
    "section": "",
    "text": "Porting Plan\nEvosax Integration Plan\nCUDA MPPI Implementation Plan",
    "crumbs": [
      "Home",
      "Introduction",
      "jax_mppi"
    ]
  },
  {
    "objectID": "plan/performance_analysis.html",
    "href": "plan/performance_analysis.html",
    "title": "Performance Analysis of JAX-MPPI Autotuning",
    "section": "",
    "text": "This document outlines the performance bottlenecks and issues identified in the autotuning module of jax_mppi, specifically focusing on the evosax integration.\n\n\nThe primary reason why autotune_evosax.py does not achieve expected performance gains over cma (CPU-based) is a fundamental mismatch between the Autotune framework architecture and JAX’s functional programming model.\n\nCurrent Architecture: The Autotune class and TunableParameter interface rely on a shared, mutable ConfigStateHolder. The evaluate_fn is a black-box function that relies on this side-effect-laden state update mechanism.\nImpact: This prevents vmap-ing the evaluation function over a population of parameters. JAX requires pure functions to parallelize execution. Because TunableParameter.apply_parameter_value modifies the global holder in-place, it cannot be safely used within a jax.vmap or jax.lax.scan context without significant refactoring.\n\n\n\n\nIn src/jax_mppi/autotune_evosax.py, the optimize_step method performs the following loop:\n# Evaluate all solutions sequentially\nresults = []\nfitness_values = []\n\nfor x in solutions:\n    result = self.evaluate_fn(np.array(x))  # type: ignore\n    results.append(result)\n    # ...\n\nIssue: The population generated by evosax (on GPU) is iterated over in Python. Each candidate solution is converted to a NumPy array, transferred to CPU, and evaluated individually.\nConsequence: This completely negates the massive parallelization advantage of JAX. Instead of running N simulations in parallel on the GPU, they are run sequentially (or with limited batching if evaluate_fn internally batches, but typically evaluate_fn runs one configuration).\nComparison: While cma is CPU-based and expects sequential/parallel CPU evaluation, evosax is designed to run the entire ask-evaluate-tell loop on the GPU. The current implementation uses evosax only for the “ask” and “tell” steps, leaving the most expensive part (evaluation) to a slow Python loop.\n\n\n\n\nThe interface forces repeated data movement between device and host:\n\nsolutions (from es.ask) are JAX arrays on GPU.\nnp.array(x) moves individual solution vectors to CPU.\nevaluate_fn likely uses JAX internally, so it might move data back to GPU for simulation.\nResults are moved back to CPU.\nfitness_array = jnp.array(fitness_values) moves costs back to GPU for es.tell.\n\n\n\n\nevosax allows for the entire optimization process (multiple generations) to be JIT-compiled using jax.lax.scan.\n\nCurrent State: optimize_step is a Python method that cannot be JIT-compiled because it calls the Python-based evaluate_fn loop.\nUnused Code: _create_jax_evaluate_fn exists in autotune_evosax.py but is not utilized effectively to enable JAX-pure evaluation.\n\n\n\n\nThe same sequential evaluation pattern is present in src/jax_mppi/autotune_qd.py:\nfor solution in solutions:\n    result = self.evaluate_fn(solution)\n    results.append(result)\nThis limits the scalability of the QD algorithms (CMA-ME), which typically benefit from large population sizes.\n\n\n\n\nHardcoded PRNG Key: EvoSaxOptimizer.setup_optimization resets the random key to jax.random.PRNGKey(0). This forces deterministic behavior that resets on every setup call, which might not be desired if the user wants to continue optimization or run multiple independent trials.\nType Hinting: Autotune.optimize_all is typed to return EvaluationResult, but can return None if iterations=0.\nUnused Variable: self.jax_evaluate_fn in EvoSaxOptimizer is assigned but never used.\n\n\n\n\n\nRefactor TunableParameter: Create a functional interface where parameters can be applied to a config/state to produce a new config/state without side effects.\nVectorized Evaluation: Update Autotune to support a batched_evaluate_fn that accepts a batch of parameters (JAX array) and returns a batch of costs.\nJIT-compile Loop: Once evaluation is vectorized and pure, use jax.lax.scan to run the optimization loop entirely on the GPU."
  },
  {
    "objectID": "plan/performance_analysis.html#architectural-bottleneck-stateful-vs-functional",
    "href": "plan/performance_analysis.html#architectural-bottleneck-stateful-vs-functional",
    "title": "Performance Analysis of JAX-MPPI Autotuning",
    "section": "",
    "text": "The primary reason why autotune_evosax.py does not achieve expected performance gains over cma (CPU-based) is a fundamental mismatch between the Autotune framework architecture and JAX’s functional programming model.\n\nCurrent Architecture: The Autotune class and TunableParameter interface rely on a shared, mutable ConfigStateHolder. The evaluate_fn is a black-box function that relies on this side-effect-laden state update mechanism.\nImpact: This prevents vmap-ing the evaluation function over a population of parameters. JAX requires pure functions to parallelize execution. Because TunableParameter.apply_parameter_value modifies the global holder in-place, it cannot be safely used within a jax.vmap or jax.lax.scan context without significant refactoring."
  },
  {
    "objectID": "plan/performance_analysis.html#sequential-evaluation-in-evosax-optimization",
    "href": "plan/performance_analysis.html#sequential-evaluation-in-evosax-optimization",
    "title": "Performance Analysis of JAX-MPPI Autotuning",
    "section": "",
    "text": "In src/jax_mppi/autotune_evosax.py, the optimize_step method performs the following loop:\n# Evaluate all solutions sequentially\nresults = []\nfitness_values = []\n\nfor x in solutions:\n    result = self.evaluate_fn(np.array(x))  # type: ignore\n    results.append(result)\n    # ...\n\nIssue: The population generated by evosax (on GPU) is iterated over in Python. Each candidate solution is converted to a NumPy array, transferred to CPU, and evaluated individually.\nConsequence: This completely negates the massive parallelization advantage of JAX. Instead of running N simulations in parallel on the GPU, they are run sequentially (or with limited batching if evaluate_fn internally batches, but typically evaluate_fn runs one configuration).\nComparison: While cma is CPU-based and expects sequential/parallel CPU evaluation, evosax is designed to run the entire ask-evaluate-tell loop on the GPU. The current implementation uses evosax only for the “ask” and “tell” steps, leaving the most expensive part (evaluation) to a slow Python loop."
  },
  {
    "objectID": "plan/performance_analysis.html#data-transfer-overhead",
    "href": "plan/performance_analysis.html#data-transfer-overhead",
    "title": "Performance Analysis of JAX-MPPI Autotuning",
    "section": "",
    "text": "The interface forces repeated data movement between device and host:\n\nsolutions (from es.ask) are JAX arrays on GPU.\nnp.array(x) moves individual solution vectors to CPU.\nevaluate_fn likely uses JAX internally, so it might move data back to GPU for simulation.\nResults are moved back to CPU.\nfitness_array = jnp.array(fitness_values) moves costs back to GPU for es.tell."
  },
  {
    "objectID": "plan/performance_analysis.html#lack-of-end-to-end-jit-compilation",
    "href": "plan/performance_analysis.html#lack-of-end-to-end-jit-compilation",
    "title": "Performance Analysis of JAX-MPPI Autotuning",
    "section": "",
    "text": "evosax allows for the entire optimization process (multiple generations) to be JIT-compiled using jax.lax.scan.\n\nCurrent State: optimize_step is a Python method that cannot be JIT-compiled because it calls the Python-based evaluate_fn loop.\nUnused Code: _create_jax_evaluate_fn exists in autotune_evosax.py but is not utilized effectively to enable JAX-pure evaluation."
  },
  {
    "objectID": "plan/performance_analysis.html#issues-in-quality-diversity-qd-tuning",
    "href": "plan/performance_analysis.html#issues-in-quality-diversity-qd-tuning",
    "title": "Performance Analysis of JAX-MPPI Autotuning",
    "section": "",
    "text": "The same sequential evaluation pattern is present in src/jax_mppi/autotune_qd.py:\nfor solution in solutions:\n    result = self.evaluate_fn(solution)\n    results.append(result)\nThis limits the scalability of the QD algorithms (CMA-ME), which typically benefit from large population sizes."
  },
  {
    "objectID": "plan/performance_analysis.html#minor-issues",
    "href": "plan/performance_analysis.html#minor-issues",
    "title": "Performance Analysis of JAX-MPPI Autotuning",
    "section": "",
    "text": "Hardcoded PRNG Key: EvoSaxOptimizer.setup_optimization resets the random key to jax.random.PRNGKey(0). This forces deterministic behavior that resets on every setup call, which might not be desired if the user wants to continue optimization or run multiple independent trials.\nType Hinting: Autotune.optimize_all is typed to return EvaluationResult, but can return None if iterations=0.\nUnused Variable: self.jax_evaluate_fn in EvoSaxOptimizer is assigned but never used."
  },
  {
    "objectID": "plan/performance_analysis.html#recommendations-for-improvement",
    "href": "plan/performance_analysis.html#recommendations-for-improvement",
    "title": "Performance Analysis of JAX-MPPI Autotuning",
    "section": "",
    "text": "Refactor TunableParameter: Create a functional interface where parameters can be applied to a config/state to produce a new config/state without side effects.\nVectorized Evaluation: Update Autotune to support a batched_evaluate_fn that accepts a batch of parameters (JAX array) and returns a batch of costs.\nJIT-compile Loop: Once evaluation is vectorized and pure, use jax.lax.scan to run the optimization loop entirely on the GPU."
  },
  {
    "objectID": "plan/completed/move_i_mppi_to_src.html",
    "href": "plan/completed/move_i_mppi_to_src.html",
    "title": "Move I-MPPI Modules Into src/jax_mppi",
    "section": "",
    "text": "Inspect existing I-MPPI module layout and all references.\nMove I-MPPI modules from examples/i_mppi_modules into src/jax_mppi.\nUpdate example imports to use the new package path.\nRun targeted tests/checks and verify imports."
  },
  {
    "objectID": "plan/completed/move_i_mppi_to_src.html#steps",
    "href": "plan/completed/move_i_mppi_to_src.html#steps",
    "title": "Move I-MPPI Modules Into src/jax_mppi",
    "section": "",
    "text": "Inspect existing I-MPPI module layout and all references.\nMove I-MPPI modules from examples/i_mppi_modules into src/jax_mppi.\nUpdate example imports to use the new package path.\nRun targeted tests/checks and verify imports."
  },
  {
    "objectID": "plan/completed/cuda_mppi_implementation.html",
    "href": "plan/completed/cuda_mppi_implementation.html",
    "title": "CUDA/C++ MPPI Implementation Plan",
    "section": "",
    "text": "Implement CUDA/C++ versions of MPPI, SMPPI, and KMPPI controllers within the src/cuda_mppi directory, using ../MPPI-Generic as a reference for high-performance CUDA implementation patterns.\n\n\n\n\nCreate a C++/CUDA project structure within src/cuda_mppi.\nImplement the standard MPPI algorithm (mirroring src/jax_mppi/mppi.py).\nImplement the Smooth MPPI (SMPPI) algorithm (mirroring src/jax_mppi/smppi.py).\nImplement the Kernel MPPI (KMPPI) algorithm (mirroring src/jax_mppi/kmppi.py).\nEnsure the implementations are self-contained or have clear interfaces (even if not fully hooked up to Python yet).\n\n\n\n\nWe will create src/cuda_mppi with the following structure:\nsrc/cuda_mppi/\n├── CMakeLists.txt              # Build configuration\n├── include/\n│   └── mppi/\n│       ├── controllers/\n│       │   ├── mppi.cuh        # Standard MPPI header\n│       │   ├── smppi.cuh       # Smooth MPPI header\n│       │   └── kmppi.cuh       # Kernel MPPI header\n│       ├── core/\n│       │   ├── mppi_common.cuh # Common structures and utilities\n│       │   └── kernels.cuh     # Shared CUDA kernels (rollout, cost, etc.)\n│       ├── dynamics/\n│       │   └── dynamics.cuh    # Dynamics interface and base classes\n│       ├── costs/\n│       │   └── costs.cuh       # Cost function interface\n│       └── utils/\n│           └── cuda_utils.cuh  # CUDA helper functions\n└── src/\n    ├── controllers/\n    │   ├── mppi.cu             # Standard MPPI implementation\n    │   ├── smppi.cu            # Smooth MPPI implementation\n    │   └── kmppi.cu            # Kernel MPPI implementation\n    ├── core/\n    │   └── kernels.cu          # Kernel implementations\n    └── utils/\n        └── cuda_utils.cu       # Utility implementations\n\n\n\n\n\n\nmppi_common.cuh: Define data structures for state, configuration, and control sequences.\nkernels.cuh:\n\nrollout_kernel: Generic kernel to propagate dynamics and compute costs for \\(K\\) samples over \\(T\\) timesteps.\nreduce_cost_kernel: Kernel to compute weighted averages of trajectories.\n\n\n\n\n\n\nDefine template interfaces or base classes for Dynamics and RunningCost so that specific system models (like Quadrotor) can be plugged in.\nNote: Since we are focusing on the controllers, we will provide a simple example dynamics (e.g., Double Integrator or simple Quadrotor) to verify compilation, but the main focus is the controller logic.\n\n\n\n\n\n\n\nLogic:\n\nSample noise \\(\\epsilon \\sim \\mathcal{N}(0, \\Sigma)\\).\nCompute \\(u_{per} = u_{nom} + \\epsilon\\).\nRollout dynamics using \\(u_{per}\\).\nCompute costs \\(J(\\tau)\\).\nCompute weights \\(w \\propto \\exp(-J/\\lambda)\\).\nUpdate \\(u_{nom} \\leftarrow u_{nom} + \\sum w \\epsilon\\).\n\nCUDA: Use block-per-sample or thread-per-sample approach depending on horizon/state size. MPPI-Generic often uses block-y striding.\n\n\n\n\n\nLogic:\n\nSample noise in velocity space \\(\\delta v\\).\nIntegrate to get actions \\(u\\).\nAdd smoothness cost \\(\\sum (\\Delta u)^2\\).\nUpdate velocity sequence \\(v_{nom}\\).\n\nCUDA: Needs a kernel that handles the integration step (velocity -&gt; action) before the rollout.\n\n\n\n\n\nLogic:\n\nControl trajectory parameterized by control points \\(\\theta\\) and kernel \\(K(\\cdot, \\cdot)\\).\nSample noise on \\(\\theta\\).\nInterpolate \\(\\theta \\to u(t)\\).\nRollout.\nUpdate \\(\\theta\\).\n\nCUDA: Needs a kernel multiplication/interpolation step before rollout.\n\n\n\n\n\n\n\nSetup: Create directory structure and CMakeLists.txt.\nCommon: Implement mppi_common.cuh and basic cuda_utils.cuh.\nDynamics/Cost: Define minimal interfaces.\nMPPI: Implement mppi.cuh and mppi.cu.\nSMPPI: Implement smppi.cuh and smppi.cu.\nKMPPI: Implement kmppi.cuh and kmppi.cu.\nVerification: Create a dummy main.cu to instantiate these controllers and verify they compile.\n\n\n\n\n\n\nExpose the C++ MPPI controllers to Python to allow direct usage from the jax_mppi package, potentially replacing the JAX implementation for performance-critical sections.\n\n\n\n\nBinding Library: Use nanobind (efficient, small footprint) to create Python bindings for the C++ classes.\nData Transfer:\n\nBasic: Accept NumPy arrays (CPU) and copy to GPU in C++.\nAdvanced (Zero-Copy): Accept DLPack capsules (from jax.Array or torch.Tensor) to pass GPU pointers directly to the C++ controllers, avoiding CPU-GPU transfers.\n\n\n\n\n\n\nProject Config:\n\nUpdate pyproject.toml to support C++ extensions (e.g., using scikit-build-core).\nAdd dependencies: nanobind, scikit-build-core.\n\nBindings Code:\n\nCreate src/cuda_mppi/bindings/bindings.cpp.\nExpose MPPIConfig struct as a Python class.\nExpose MPPIController, SMPPIController, KMPPIController classes.\nBind methods like compute(state) and get_action().\nImplement type casters for Eigen::VectorXf &lt;-&gt; numpy.ndarray (using nanobind/eigen/dense.h).\n\nCMake Update:\n\nAdd nanobind_add_module target.\nLink against cuda_mppi and CUDA libraries.\n\nIntegration:\n\nCreate a Python wrapper module (e.g., jax_mppi.cuda) that imports the extension.\nAdd tests in tests/ to verify correctness against the JAX implementation.\n\n\n\n\n\n\n\n\nAllow users to define dynamics and cost functions in Python (initially as C++ code strings, or eventually transpiled from JAX) and compile the specialized MPPI controller at runtime. This avoids the need to recompile the shared library for every new system.\n\n\n\n\nNVRTC (NVIDIA Runtime Compilation): Use NVRTC to compile CUDA C++ code strings into PTX at runtime.\nCUDA Driver API: Use the Driver API (cuModuleLoadData, cuLaunchKernel) to load the compiled PTX and launch the rollout_kernel.\nWarm Start: The compilation happens once during the “warm start” phase (controller initialization), enabling high-performance rollouts thereafter.\n\n\n\n\n\nBuild Config: Link against nvrtc and cuda (Driver API).\nJIT Compiler Class (src/cuda_mppi/include/mppi/jit/jit_compiler.hpp):\n\nInputs: Strings for dynamics_struct_code and cost_struct_code.\nAction: Constructs the full .cu source code (headers + user structs + template instantiation).\nOutput: Compiles to PTX using nvrtcProgramCompile.\nUpdated wrapper generation to work with Driver API.\n\nJIT Controller (JITMPPIController):\n\nA generic controller class that holds CUfunction handles instead of hardcoded kernels.\ncompute() method launches the generated kernel via cuLaunchKernel.\nImplemented in include/mppi/controllers/jit_mppi.hpp and src/jit/jit_mppi_controller.cpp.\n\nPython Interface:\n\nExpose JITMPPIController to Python via nanobind.\nExample usage:\ndynamics_code = \"\"\"\nstruct UserDynamics {\n    __device__ void step(...) { ... }\n};\n\"\"\"\ncost_code = \"\"\"\nstruct UserCost {\n    __device__ float compute(...) { ... }\n    __device__ float terminal_cost(...) { ... }\n};\n\"\"\"\ncontroller = cuda_mppi.JITMPPIController(config, dynamics_code, cost_code, include_paths)\n\nVerification & Examples:\n\nImplemented examples/cuda_pendulum_jit.py - complete pendulum swing-up example with matplotlib plotting.\nCreated include/mppi/jit/examples.hpp with example templates for common systems:\n\nPendulum dynamics and cost\nDouble integrator dynamics and cost\nCart-pole dynamics and cost\n\nCreated examples/JIT_EXAMPLES_README.md with comprehensive documentation.\nNote: Interactive pygame visualization can be added in future enhancement.\n\n\n\n\n\n\nNew Files:\n\ninclude/mppi/controllers/jit_mppi.hpp - JIT controller header\nsrc/jit/jit_mppi_controller.cpp - JIT controller implementation\ninclude/mppi/jit/examples.hpp - Example code templates\nexamples/cuda_pendulum_jit.py - Pendulum swing-up example\nexamples/JIT_EXAMPLES_README.md - JIT examples documentation\n\nModified Files:\n\nsrc/jit/jit_compiler.cpp - Updated wrapper generation for Driver API compatibility\nCMakeLists.txt - Added JIT sources to build\nbindings/bindings.cu - Added Python bindings for JITMPPIController\n\n\n\n\n\nfrom jax_mppi import cuda_mppi\nimport numpy as np\nimport os\n\n# Set include path\nos.environ['CUDA_MPPI_INCLUDE_DIR'] = '/path/to/src/cuda_mppi/include'\n\n# Configure MPPI\nconfig = cuda_mppi.MPPIConfig(\n    num_samples=1000,\n    horizon=50,\n    nx=2, nu=1,\n    lambda_=1.0,\n    dt=0.02,\n    u_scale=5.0,\n    w_action_seq_cost=0.0,\n    num_support_pts=10\n)\n\n# Define custom dynamics and cost\ndynamics_code = \"\"\"...\"\"\"  # See examples.hpp for templates\ncost_code = \"\"\"...\"\"\"\n\n# Create JIT controller (compilation happens here, ~1-5 seconds)\ncontroller = cuda_mppi.JITMPPIController(\n    config, dynamics_code, cost_code,\n    [os.environ['CUDA_MPPI_INCLUDE_DIR']]\n)\n\n# Use controller\nstate = np.array([1.0, 0.0], dtype=np.float32)\ncontroller.compute(state)\naction = controller.get_action()\ncontroller.shift()\n\n\n\n\n\nsrc/jax_mppi/*.py (Main reference for the MPPI Implementation)\n../MPPI-Generic (Reference for CUDA patterns)\nNVRTC Documentation"
  },
  {
    "objectID": "plan/completed/cuda_mppi_implementation.html#objective",
    "href": "plan/completed/cuda_mppi_implementation.html#objective",
    "title": "CUDA/C++ MPPI Implementation Plan",
    "section": "",
    "text": "Implement CUDA/C++ versions of MPPI, SMPPI, and KMPPI controllers within the src/cuda_mppi directory, using ../MPPI-Generic as a reference for high-performance CUDA implementation patterns."
  },
  {
    "objectID": "plan/completed/cuda_mppi_implementation.html#goals",
    "href": "plan/completed/cuda_mppi_implementation.html#goals",
    "title": "CUDA/C++ MPPI Implementation Plan",
    "section": "",
    "text": "Create a C++/CUDA project structure within src/cuda_mppi.\nImplement the standard MPPI algorithm (mirroring src/jax_mppi/mppi.py).\nImplement the Smooth MPPI (SMPPI) algorithm (mirroring src/jax_mppi/smppi.py).\nImplement the Kernel MPPI (KMPPI) algorithm (mirroring src/jax_mppi/kmppi.py).\nEnsure the implementations are self-contained or have clear interfaces (even if not fully hooked up to Python yet)."
  },
  {
    "objectID": "plan/completed/cuda_mppi_implementation.html#directory-structure",
    "href": "plan/completed/cuda_mppi_implementation.html#directory-structure",
    "title": "CUDA/C++ MPPI Implementation Plan",
    "section": "",
    "text": "We will create src/cuda_mppi with the following structure:\nsrc/cuda_mppi/\n├── CMakeLists.txt              # Build configuration\n├── include/\n│   └── mppi/\n│       ├── controllers/\n│       │   ├── mppi.cuh        # Standard MPPI header\n│       │   ├── smppi.cuh       # Smooth MPPI header\n│       │   └── kmppi.cuh       # Kernel MPPI header\n│       ├── core/\n│       │   ├── mppi_common.cuh # Common structures and utilities\n│       │   └── kernels.cuh     # Shared CUDA kernels (rollout, cost, etc.)\n│       ├── dynamics/\n│       │   └── dynamics.cuh    # Dynamics interface and base classes\n│       ├── costs/\n│       │   └── costs.cuh       # Cost function interface\n│       └── utils/\n│           └── cuda_utils.cuh  # CUDA helper functions\n└── src/\n    ├── controllers/\n    │   ├── mppi.cu             # Standard MPPI implementation\n    │   ├── smppi.cu            # Smooth MPPI implementation\n    │   └── kmppi.cu            # Kernel MPPI implementation\n    ├── core/\n    │   └── kernels.cu          # Kernel implementations\n    └── utils/\n        └── cuda_utils.cu       # Utility implementations"
  },
  {
    "objectID": "plan/completed/cuda_mppi_implementation.html#implementation-details",
    "href": "plan/completed/cuda_mppi_implementation.html#implementation-details",
    "title": "CUDA/C++ MPPI Implementation Plan",
    "section": "",
    "text": "mppi_common.cuh: Define data structures for state, configuration, and control sequences.\nkernels.cuh:\n\nrollout_kernel: Generic kernel to propagate dynamics and compute costs for \\(K\\) samples over \\(T\\) timesteps.\nreduce_cost_kernel: Kernel to compute weighted averages of trajectories.\n\n\n\n\n\n\nDefine template interfaces or base classes for Dynamics and RunningCost so that specific system models (like Quadrotor) can be plugged in.\nNote: Since we are focusing on the controllers, we will provide a simple example dynamics (e.g., Double Integrator or simple Quadrotor) to verify compilation, but the main focus is the controller logic.\n\n\n\n\n\n\n\nLogic:\n\nSample noise \\(\\epsilon \\sim \\mathcal{N}(0, \\Sigma)\\).\nCompute \\(u_{per} = u_{nom} + \\epsilon\\).\nRollout dynamics using \\(u_{per}\\).\nCompute costs \\(J(\\tau)\\).\nCompute weights \\(w \\propto \\exp(-J/\\lambda)\\).\nUpdate \\(u_{nom} \\leftarrow u_{nom} + \\sum w \\epsilon\\).\n\nCUDA: Use block-per-sample or thread-per-sample approach depending on horizon/state size. MPPI-Generic often uses block-y striding.\n\n\n\n\n\nLogic:\n\nSample noise in velocity space \\(\\delta v\\).\nIntegrate to get actions \\(u\\).\nAdd smoothness cost \\(\\sum (\\Delta u)^2\\).\nUpdate velocity sequence \\(v_{nom}\\).\n\nCUDA: Needs a kernel that handles the integration step (velocity -&gt; action) before the rollout.\n\n\n\n\n\nLogic:\n\nControl trajectory parameterized by control points \\(\\theta\\) and kernel \\(K(\\cdot, \\cdot)\\).\nSample noise on \\(\\theta\\).\nInterpolate \\(\\theta \\to u(t)\\).\nRollout.\nUpdate \\(\\theta\\).\n\nCUDA: Needs a kernel multiplication/interpolation step before rollout."
  },
  {
    "objectID": "plan/completed/cuda_mppi_implementation.html#execution-plan",
    "href": "plan/completed/cuda_mppi_implementation.html#execution-plan",
    "title": "CUDA/C++ MPPI Implementation Plan",
    "section": "",
    "text": "Setup: Create directory structure and CMakeLists.txt.\nCommon: Implement mppi_common.cuh and basic cuda_utils.cuh.\nDynamics/Cost: Define minimal interfaces.\nMPPI: Implement mppi.cuh and mppi.cu.\nSMPPI: Implement smppi.cuh and smppi.cu.\nKMPPI: Implement kmppi.cuh and kmppi.cu.\nVerification: Create a dummy main.cu to instantiate these controllers and verify they compile."
  },
  {
    "objectID": "plan/completed/cuda_mppi_implementation.html#python-integration-phase-2",
    "href": "plan/completed/cuda_mppi_implementation.html#python-integration-phase-2",
    "title": "CUDA/C++ MPPI Implementation Plan",
    "section": "",
    "text": "Expose the C++ MPPI controllers to Python to allow direct usage from the jax_mppi package, potentially replacing the JAX implementation for performance-critical sections.\n\n\n\n\nBinding Library: Use nanobind (efficient, small footprint) to create Python bindings for the C++ classes.\nData Transfer:\n\nBasic: Accept NumPy arrays (CPU) and copy to GPU in C++.\nAdvanced (Zero-Copy): Accept DLPack capsules (from jax.Array or torch.Tensor) to pass GPU pointers directly to the C++ controllers, avoiding CPU-GPU transfers.\n\n\n\n\n\n\nProject Config:\n\nUpdate pyproject.toml to support C++ extensions (e.g., using scikit-build-core).\nAdd dependencies: nanobind, scikit-build-core.\n\nBindings Code:\n\nCreate src/cuda_mppi/bindings/bindings.cpp.\nExpose MPPIConfig struct as a Python class.\nExpose MPPIController, SMPPIController, KMPPIController classes.\nBind methods like compute(state) and get_action().\nImplement type casters for Eigen::VectorXf &lt;-&gt; numpy.ndarray (using nanobind/eigen/dense.h).\n\nCMake Update:\n\nAdd nanobind_add_module target.\nLink against cuda_mppi and CUDA libraries.\n\nIntegration:\n\nCreate a Python wrapper module (e.g., jax_mppi.cuda) that imports the extension.\nAdd tests in tests/ to verify correctness against the JAX implementation."
  },
  {
    "objectID": "plan/completed/cuda_mppi_implementation.html#phase-3-runtime-dynamics-compilation-nvrtc",
    "href": "plan/completed/cuda_mppi_implementation.html#phase-3-runtime-dynamics-compilation-nvrtc",
    "title": "CUDA/C++ MPPI Implementation Plan",
    "section": "",
    "text": "Allow users to define dynamics and cost functions in Python (initially as C++ code strings, or eventually transpiled from JAX) and compile the specialized MPPI controller at runtime. This avoids the need to recompile the shared library for every new system.\n\n\n\n\nNVRTC (NVIDIA Runtime Compilation): Use NVRTC to compile CUDA C++ code strings into PTX at runtime.\nCUDA Driver API: Use the Driver API (cuModuleLoadData, cuLaunchKernel) to load the compiled PTX and launch the rollout_kernel.\nWarm Start: The compilation happens once during the “warm start” phase (controller initialization), enabling high-performance rollouts thereafter.\n\n\n\n\n\nBuild Config: Link against nvrtc and cuda (Driver API).\nJIT Compiler Class (src/cuda_mppi/include/mppi/jit/jit_compiler.hpp):\n\nInputs: Strings for dynamics_struct_code and cost_struct_code.\nAction: Constructs the full .cu source code (headers + user structs + template instantiation).\nOutput: Compiles to PTX using nvrtcProgramCompile.\nUpdated wrapper generation to work with Driver API.\n\nJIT Controller (JITMPPIController):\n\nA generic controller class that holds CUfunction handles instead of hardcoded kernels.\ncompute() method launches the generated kernel via cuLaunchKernel.\nImplemented in include/mppi/controllers/jit_mppi.hpp and src/jit/jit_mppi_controller.cpp.\n\nPython Interface:\n\nExpose JITMPPIController to Python via nanobind.\nExample usage:\ndynamics_code = \"\"\"\nstruct UserDynamics {\n    __device__ void step(...) { ... }\n};\n\"\"\"\ncost_code = \"\"\"\nstruct UserCost {\n    __device__ float compute(...) { ... }\n    __device__ float terminal_cost(...) { ... }\n};\n\"\"\"\ncontroller = cuda_mppi.JITMPPIController(config, dynamics_code, cost_code, include_paths)\n\nVerification & Examples:\n\nImplemented examples/cuda_pendulum_jit.py - complete pendulum swing-up example with matplotlib plotting.\nCreated include/mppi/jit/examples.hpp with example templates for common systems:\n\nPendulum dynamics and cost\nDouble integrator dynamics and cost\nCart-pole dynamics and cost\n\nCreated examples/JIT_EXAMPLES_README.md with comprehensive documentation.\nNote: Interactive pygame visualization can be added in future enhancement.\n\n\n\n\n\n\nNew Files:\n\ninclude/mppi/controllers/jit_mppi.hpp - JIT controller header\nsrc/jit/jit_mppi_controller.cpp - JIT controller implementation\ninclude/mppi/jit/examples.hpp - Example code templates\nexamples/cuda_pendulum_jit.py - Pendulum swing-up example\nexamples/JIT_EXAMPLES_README.md - JIT examples documentation\n\nModified Files:\n\nsrc/jit/jit_compiler.cpp - Updated wrapper generation for Driver API compatibility\nCMakeLists.txt - Added JIT sources to build\nbindings/bindings.cu - Added Python bindings for JITMPPIController\n\n\n\n\n\nfrom jax_mppi import cuda_mppi\nimport numpy as np\nimport os\n\n# Set include path\nos.environ['CUDA_MPPI_INCLUDE_DIR'] = '/path/to/src/cuda_mppi/include'\n\n# Configure MPPI\nconfig = cuda_mppi.MPPIConfig(\n    num_samples=1000,\n    horizon=50,\n    nx=2, nu=1,\n    lambda_=1.0,\n    dt=0.02,\n    u_scale=5.0,\n    w_action_seq_cost=0.0,\n    num_support_pts=10\n)\n\n# Define custom dynamics and cost\ndynamics_code = \"\"\"...\"\"\"  # See examples.hpp for templates\ncost_code = \"\"\"...\"\"\"\n\n# Create JIT controller (compilation happens here, ~1-5 seconds)\ncontroller = cuda_mppi.JITMPPIController(\n    config, dynamics_code, cost_code,\n    [os.environ['CUDA_MPPI_INCLUDE_DIR']]\n)\n\n# Use controller\nstate = np.array([1.0, 0.0], dtype=np.float32)\ncontroller.compute(state)\naction = controller.get_action()\ncontroller.shift()"
  },
  {
    "objectID": "plan/completed/cuda_mppi_implementation.html#references",
    "href": "plan/completed/cuda_mppi_implementation.html#references",
    "title": "CUDA/C++ MPPI Implementation Plan",
    "section": "",
    "text": "src/jax_mppi/*.py (Main reference for the MPPI Implementation)\n../MPPI-Generic (Reference for CUDA patterns)\nNVRTC Documentation"
  },
  {
    "objectID": "plan/completed/porting_pytorch_jax.html",
    "href": "plan/completed/porting_pytorch_jax.html",
    "title": "JAX MPPI Implementation Plan",
    "section": "",
    "text": "Port pytorch_mppi to JAX, producing a functional, JIT-compilable MPPI library.\n\n\nOverall Progress: Phase 6 complete (Autotuning system fully implemented with CMA-ES, Ray Tune, and CMA-ME support).\n\n\n\nPhase 1: Core MPPI ✅ COMPLETE\n\n353 lines implemented in src/jax_mppi/mppi.py\nAll core features from pytorch_mppi ported\n115 lines of unit tests in tests/test_mppi.py\n\nPhase 2: Pendulum Integration ✅ COMPLETE\n\n270 lines in examples/pendulum.py (full-featured example with CLI)\n282 lines in tests/test_pendulum.py (8 comprehensive integration tests)\nAll tests passing, swing-up and stabilization verified\n\nPhase 3: Smooth MPPI (SMPPI) ✅ COMPLETE\n\n634 lines implemented in src/jax_mppi/smppi.py\nAll SMPPI features: action_sequence, smoothness cost, dual bounds, integration\n580 lines in tests/test_smppi.py (18 comprehensive tests)\nAll tests passing\n\nPhase 4: Kernel MPPI (KMPPI) ✅ COMPLETE\n\n660 lines implemented in src/jax_mppi/kmppi.py\nRBFKernel, kernel interpolation, control point optimization\n595 lines in tests/test_kmppi.py (23 comprehensive tests)\nAll tests passing (53/53 total tests pass)\n\nPhase 5: Smooth Comparison Example ✅ COMPLETE\n\n442 lines in examples/smooth_comparison.py\nCompares MPPI, SMPPI, and KMPPI on 2D navigation with obstacle avoidance\nIncludes visualization with 4 subplots: trajectories, costs, controls, smoothness\nSupporting modules: src/jax_mppi/costs/ and src/jax_mppi/dynamics/\n\nPhase 6: Autotuning ✅ COMPLETE\n\n656 lines in src/jax_mppi/autotune.py - Core CMA-ES autotuning\n375 lines in src/jax_mppi/autotune_global.py - Ray Tune global search\n218 lines in src/jax_mppi/autotune_qd.py - CMA-ME quality diversity\n305 lines in tests/test_autotune.py (21 unit tests)\n247 lines in tests/test_autotune_integration.py (4 integration tests)\n321 lines in examples/autotune_pendulum.py - Full demonstration\n90 lines in examples/autotune_basic.py - Minimal example\nAll 25 tests passing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPackage\nCore Code\nTests\nExamples\nTotal\n\n\n\n\npytorch_mppi\n1214 lines\n~500 lines\n~800 lines\n~2500 lines\n\n\njax_mppi (current)\n2919 lines\n2124 lines\n681 lines\n5724 lines\n\n\nCompletion %\n240%\n425%\n85%\n229%\n\n\n\nCore code now includes: mppi.py (353), smppi.py (634), kmppi.py (660), autotune.py (656), autotune_global.py (375), autotune_qd.py (218), plus supporting modules.\n\n\n\n\n\n\nFeature\npytorch_mppi\njax_mppi\nStatus\n\n\n\n\nCore MPPI Algorithm\n✓\n✓\n✅ Complete\n\n\nBasic sampling & weighting\n✓\n✓\n✅\n\n\nControl bounds (u_min/u_max)\n✓\n✓\n✅\n\n\nControl scaling (u_scale)\n✓\n✓\n✅\n\n\nPartial updates (u_per_command)\n✓\n✓\n✅\n\n\nStep-dependent dynamics\n✓\n✓\n✅\n\n\nStochastic dynamics (rollout_samples)\n✓\n✓\n✅\n\n\nSample null action\n✓\n✓\n✅\n\n\nNoise absolute cost\n✓\n✓\n✅\n\n\nTerminal cost function\n✓\n✓\n✅\n\n\nShift nominal trajectory\n✓\n✓\n✅\n\n\nGet rollouts (visualization)\n✓\n✓\n✅\n\n\nReset controller\n✓\n✓\n✅\n\n\nSmooth MPPI (SMPPI)\n✓\n✓\n✅ Complete\n\n\nAction sequence tracking\n✓\n✓\n✅\n\n\nSmoothness penalty\n✓\n✓\n✅\n\n\nSeparate action/control bounds\n✓\n✓\n✅\n\n\nDelta_t integration\n✓\n✓\n✅\n\n\nShift with continuity\n✓\n✓\n✅\n\n\nKernel MPPI (KMPPI)\n✓\n✓\n✅ Complete\n\n\nKernel interpolation\n✓\n✓\n✅\n\n\nRBF kernel\n✓\n✓\n✅\n\n\nSupport point optimization\n✓\n✓\n✅\n\n\nTime grid management (Tk/Hs)\n✓\n✓\n✅\n\n\nSolve-based interpolation\n✓\n✓\n✅\n\n\nAutotuning\n✓\n✓\n✅ Complete\n\n\nCMA-ES local tuning\n✓\n✓\n✅\n\n\nRay Tune global search\n✓\n✓\n✅\n\n\nCMA-ME quality diversity\n✓\n✓\n✅\n\n\nParameter types (lambda, sigma, mu, horizon)\n✓\n✓\n✅\n\n\nAll MPPI variants support\n✓\n✓\n✅\n\n\nExamples\n\n\n\n\n\nPendulum swing-up\n✓\n✓\n✅ Complete\n\n\nSmooth MPPI comparison\n✓\n✓\n✅ Complete\n\n\nAutotuning example\n✓\n✓\n✅ Complete\n\n\nPendulum with learned dynamics\n✓\n✗\n🔴 Not planned\n\n\n\n\n\n\njax_mppi/\n├── pyproject.toml              ✅ Exists\n├── README.md                   ✅ Exists\n├── LICENSE                     ✅ Exists  \n├── src/jax_mppi/\n│   ├── __init__.py            ✅ Exists (updated for autotune)\n│   ├── types.py               ✅ Exists (9 lines)\n│   ├── mppi.py                ✅ Exists (353 lines) - COMPLETE\n│   ├── smppi.py               ✅ Exists (634 lines) - COMPLETE\n│   ├── kmppi.py               ✅ Exists (660 lines) - COMPLETE\n│   ├── autotune.py            ✅ Exists (656 lines) - COMPLETE\n│   ├── autotune_global.py     ✅ Exists (375 lines) - COMPLETE\n│   ├── autotune_qd.py         ✅ Exists (218 lines) - COMPLETE\n│   ├── costs/                 ✅ Exists (supporting modules)\n│   └── dynamics/              ✅ Exists (supporting modules)\n├── tests/\n│   ├── test_mppi.py           ✅ Exists (115 lines) - COMPLETE\n│   ├── test_pendulum.py       ✅ Exists (282 lines) - COMPLETE\n│   ├── test_smppi.py          ✅ Exists (580 lines) - COMPLETE\n│   ├── test_autotune.py       ✅ Exists (305 lines, 21 tests) - COMPLETE\n│   └── test_autotune_integration.py ✅ Exists (247 lines, 4 tests) - COMPLETE\n│   └── test_kmppi.py          ✅ Exists (595 lines) - COMPLETE\n├── examples/\n│   ├── pendulum.py            ✅ Exists (270 lines) - COMPLETE\n│   ├── smooth_comparison.py   ✅ Exists (442 lines) - COMPLETE\n│   ├── autotune_pendulum.py   ✅ Exists (321 lines) - COMPLETE\n│   └── autotune_basic.py      ✅ Exists (90 lines) - COMPLETE\n└── docs/\n    └── plan/\n        └── porting_pytorch_jax.md ✅ This file\n\n\n\nPriority Order:\n\nPhase 3: SMPPI Implementation (High Priority)\n\nCore functionality that adds smoothness to control\nEstimated ~250-300 lines for smppi.py\nEstimated ~150-200 lines for tests\nReference: ../pytorch_mppi/src/pytorch_mppi/mppi.py (SMPPI class)\n\nPhase 4: KMPPI Implementation (High Priority)\n\nNovel contribution with kernel interpolation\nEstimated ~300-350 lines for kmppi.py\nEstimated ~150-200 lines for tests\nReference: ../pytorch_mppi/src/pytorch_mppi/mppi.py (KMPPI class)\n\nPhase 5: Smooth Comparison Example (Medium Priority)\n\nDemonstrates value of SMPPI and KMPPI\nEstimated ~200-250 lines\nReference: ../pytorch_mppi/tests/smooth_mppi.py\n\nAdditional Examples (Low Priority)\n\nPendulum with learned dynamics\nMore complex environments\n\nPhase 6: Autotuning (Optional/Stretch)\n\nAdvanced feature for hyperparameter optimization\nEstimated ~300-400 lines\nReference: ../pytorch_mppi/src/pytorch_mppi/autotune.py\n\n\n\n\n\n\n\n\nUse @jax.tree_util.register_dataclass (or flax.struct.dataclass) to hold MPPI state (nominal trajectory U, PRNG key, config). All core functions are pure: command(state, mppi_state) -&gt; (action, mppi_state).\nRationale: Idiomatic JAX — pure functions compose with jit, vmap, grad. No mutable self. Avoids heavyweight dependencies like Equinox for what is fundamentally a numerical algorithm.\n\n\n\n\n\n\n\n\n\n\nPyTorch\nJAX\n\n\n\n\ntorch.distributions.MultivariateNormal\njax.random.multivariate_normal\n\n\ntensor.to(device)\njax.device_put / automatic\n\n\nPython for-loop over horizon\njax.lax.scan\n\n\n@handle_batch_input decorator\njax.vmap\n\n\ntorch.roll\njnp.roll\n\n\ntorch.linalg.solve\njnp.linalg.solve\n\n\nIn-place mutation (self.U = ...)\nReturn new state (pytree)\n\n\n\n\n\n\n\n\nActionable parity items to carry over:\n\nSMPPI semantics: maintains action_sequence separately from lifted control U; integrates with delta_t; smoothness cost from diff(action_sequence).\nSMPPI bounds: support action_min/action_max distinct from u_min/u_max (control-derivative bounds).\nKMPPI internals: keep theta as control points; build Tk/Hs time grids; kernel interpolation via solve(Ktktk, K); batch interpolation with vmap.\nSampling options: rollout_samples (M), sample_null_action, noise_abs_cost (abs(noise) in action cost).\nRollouts: get_rollouts handles state batch and dynamics that may augment state (take first nx).\n\n\n\n\n\njax_mppi/\n├── pyproject.toml\n├── README.md\n├── LICENSE\n├── src/jax_mppi/\n│   ├── __init__.py          # Public API exports\n│   ├── mppi.py              # Core MPPI (MPPIConfig, MPPIState, command, reset, etc.)\n│   ├── smppi.py             # Smooth MPPI variant\n│   ├── kmppi.py             # Kernel MPPI variant + TimeKernel / RBFKernel\n│   ├── types.py             # Type aliases, protocols for Dynamics/Cost callables\n│   └── autotune.py          # Autotuning (CMA-ES wrapper, parameter search)\n├── tests/\n│   ├── test_mppi.py         # Unit tests for core MPPI\n│   ├── test_smppi.py        # Unit tests for SMPPI\n│   ├── test_kmppi.py        # Unit tests for KMPPI\n│   └── test_pendulum.py     # Integration test with pendulum env\n├── examples/\n│   ├── pendulum.py          # Gym pendulum with true dynamics\n│   ├── pendulum_approximate.py  # Learned dynamics\n│   └── smooth_comparison.py # MPPI vs SMPPI vs KMPPI\n└── docs/\n    └── plan/\n\n\n\n\n\n\nFiles: pyproject.toml, src/jax_mppi/types.py, src/jax_mppi/mppi.py, src/jax_mppi/__init__.py\n\npyproject.toml — project metadata, deps: jax[cuda13], jaxlib, optional gymnasium for examples.\ntypes.py — Type definitions:\n# Dynamics: (state, action) -&gt; next_state  or  (state, action, t) -&gt; next_state\nDynamicsFn = Callable[..., jax.Array]\n# Cost: (state, action) -&gt; scalar_cost  or  (state, action, t) -&gt; scalar_cost\nRunningCostFn = Callable[..., jax.Array]\n# Terminal: (states, actions) -&gt; scalar_cost\nTerminalCostFn = Callable[[jax.Array, jax.Array], jax.Array]\nmppi.py — Core implementation:\nData structures (registered as JAX pytrees):\n@dataclass\nclass MPPIConfig:\n    # Static config (not traced through JAX)\n    num_samples: int       # K\n    horizon: int           # T\n    nx: int\n    nu: int\n    lambda_: float\n    u_scale: float\n    u_per_command: int\n    step_dependent_dynamics: bool\n    rollout_samples: int   # M\n    rollout_var_cost: float\n    rollout_var_discount: float\n    sample_null_action: bool\n    noise_abs_cost: bool\n\n@dataclass\nclass MPPIState:\n    # Dynamic state (carried through JAX transforms)\n    U: jax.Array           # (T, nu) nominal trajectory\n    u_init: jax.Array      # (nu,) default action for shift\n    noise_mu: jax.Array    # (nu,)\n    noise_sigma: jax.Array # (nu, nu)\n    noise_sigma_inv: jax.Array\n    u_min: jax.Array | None\n    u_max: jax.Array | None\n    key: jax.Array         # PRNG key\nFunctions:\ndef create(\n    nx, nu, noise_sigma, num_samples=100, horizon=15, lambda_=1.0,\n    noise_mu=None, u_min=None, u_max=None, u_init=None, U_init=None,\n    u_scale=1, u_per_command=1, step_dependent_dynamics=False,\n    rollout_samples=1, rollout_var_cost=0., rollout_var_discount=0.95,\n    sample_null_action=False, noise_abs_cost=False, key=None,\n) -&gt; tuple[MPPIConfig, MPPIState]:\n    \"\"\"Factory: create config + initial state.\"\"\"\n\ndef command(\n    config: MPPIConfig,\n    mppi_state: MPPIState,\n    current_obs: jax.Array,\n    dynamics: DynamicsFn,\n    running_cost: RunningCostFn,\n    terminal_cost: TerminalCostFn | None = None,\n    shift: bool = True,\n) -&gt; tuple[jax.Array, MPPIState]:\n    \"\"\"Compute optimal action and return updated state.\"\"\"\n\ndef reset(config: MPPIConfig, mppi_state: MPPIState, key: jax.Array) -&gt; MPPIState:\n    \"\"\"Reset nominal trajectory.\"\"\"\n\ndef get_rollouts(\n    config: MPPIConfig, mppi_state: MPPIState,\n    current_obs: jax.Array, dynamics: DynamicsFn,\n    num_rollouts: int = 1,\n) -&gt; jax.Array:\n    \"\"\"Forward-simulate trajectories for visualization.\"\"\"\nInternal functions (all JIT-compatible):\n\n_shift_nominal(mppi_state) -&gt; MPPIState — jnp.roll + set last to u_init\n_sample_noise(key, K, T, noise_mu, noise_sigma) -&gt; (noise, new_key) — sample from multivariate normal\n_compute_rollout_costs(config, current_obs, perturbed_actions, dynamics, running_cost, terminal_cost) — uses jax.lax.scan over horizon, jax.vmap over K samples\n_compute_weights(costs, lambda_) — softmax importance weighting\n_bound_action(action, u_min, u_max) — jnp.clip\n\nKey JAX patterns:\n\nRollout loop: jax.lax.scan with carry = (state,), xs = actions[t]\nBatch over K samples: jax.vmap(_single_rollout, in_axes=(0, None, ...))\nBatch over M rollout samples (stochastic dynamics): nested vmap or scan\nAll internal functions decorated with @jax.jit or called inside a top-level jitted command\n\nUnit test: tests/test_mppi.py\n\nTest create() produces valid config/state\nTest command() returns correct shape\nTest cost reduction over iterations on simple 1D problem\nTest bounds are respected\n\n\n\n\n\nFiles: examples/pendulum.py, tests/test_pendulum.py\n\nImplement pendulum dynamics as a pure JAX function (no gym dependency for core test)\nRun MPPI loop, verify convergence (swing-up or stabilization)\nOptional: gym rendering wrapper for visualization\n\n\n\n\nFiles: src/jax_mppi/smppi.py, tests/test_smppi.py\n\nData structures:\n@dataclass\nclass SMPPIState(MPPIState):\n    action_sequence: jax.Array  # (T, nu) actual actions\n    w_action_seq_cost: float\n    delta_t: float\n    action_min: jax.Array | None\n    action_max: jax.Array | None\nFunctions: Same API as mppi.py but with:\n\n_shift_nominal shifts both U (velocity) and action_sequence\n_compute_perturbed_actions integrates velocity to get actions\n_compute_total_cost adds smoothness penalty: ||diff(actions)||^2\nreset() zeros both U and action_sequence\nchange_horizon() keeps both U and action_sequence in sync (truncate/extend)\n\nTest: Verify smoother trajectories than base MPPI on 2D navigation\n\n\n\n\nFiles: src/jax_mppi/kmppi.py, tests/test_kmppi.py\n\nKernel abstractions:\ndef rbf_kernel(t, tk, sigma=1.0):\n    d = jnp.sum((t[:, None] - tk) ** 2, axis=-1)\n    return jnp.exp(-d / (2 * sigma ** 2 + 1e-8))\n\ndef kernel_interpolate(t, tk, coeffs, kernel_fn):\n    K_t_tk = kernel_fn(t, tk)\n    K_tk_tk = kernel_fn(tk, tk)\n    weights = jnp.linalg.solve(K_tk_tk, K_t_tk.T).T\n    return weights @ coeffs\nData structures:\n@dataclass\nclass KMPPIState(MPPIState):\n    theta: jax.Array         # (num_support_pts, nu)\n    num_support_pts: int\nFunctions: Override _compute_perturbed_actions to sample sparse + interpolate. Update theta instead of U.\n\nBuild Tk and Hs time grids on init and on horizon changes\nUse kernel_interpolate() with solve(Ktktk, K) (avoid explicit inverse)\nBatch interpolate with jax.vmap for K samples\n\nTest: Verify fewer parameters produce smooth trajectories\n\n\n\n\nFiles: examples/smooth_comparison.py\n\nSide-by-side MPPI vs SMPPI vs KMPPI on 2D navigation\nPlot trajectories and control signals\n\n\n\n\nFiles: src/jax_mppi/autotune.py\n\nWrap CMA-ES (cmaes or evosax for JAX-native) for sigma/lambda/horizon tuning\nSimpler than pytorch_mppi’s framework — skip Ray Tune and QD initially\nFunctional API: tune_step(eval_fn, params, optimizer_state) -&gt; (params, optimizer_state)\n\n\n\n\n\n\n\nUnit tests (per phase): pytest tests/ — shape checks, cost reduction, bounds\nPendulum benchmark: Compare convergence (total reward) against pytorch_mppi on same scenario\nJIT correctness: Ensure jax.jit(command) produces identical results to non-jitted version\nPerformance: Benchmark command() latency vs pytorch_mppi (JAX should win after warmup due to XLA compilation)\nSmooth variants: Visual comparison of trajectory smoothness\n\n\n\nIMPORTANT: You should always use the virtual environment. To run the tests and all of the other python files.\n\nOption A: add a tests/conftest.py to insert src into sys.path.\nOption B: run tests after uv pip install -e . (editable install).\n\n\n\n\n\nCore: jax[cuda13], jaxlib, numpy Testing: pytest, gymnasium[classic_control] Autotuning (optional): cmaes or evosax Examples (optional): matplotlib, gymnasium\n\n\n\n\n\n\n\nMirror pytorch_mppi signature flags: rollout_samples, sample_null_action, noise_abs_cost.\nImplement get_rollouts handling: accept single or batched state; allow dynamics that augment state (take :nx).\nAdd shift_nominal_trajectory via jnp.roll + u_init fill.\nImplement action cost with optional abs(noise) branch.\nAdd u_per_command slicing and u_scale application in command.\n\n\n\n\n\nCarry action_sequence in state and integrate U with delta_t.\nImplement distinct action bounds (action_min/action_max) vs control bounds (u_min/u_max).\nAdd smoothness cost from diff(action_sequence) and weight w_action_seq_cost.\nEnsure reset() updates both U and action_sequence.\nImplement proper shift with action continuity (hold last value).\nImplement dual bounding system (_bound_control and _bound_action).\nRecompute effective noise after bounding for accurate cost.\n\n\n\n\n\nImplement theta control points + interpolation kernel (RBF by default).\nBuild Tk/Hs grids and re-build on horizon changes.\nUse solve(Ktktk, K) for interpolation weights (no explicit inverse).\nShift theta via interpolation when shifting nominal trajectory.\nImplement RBFKernel with configurable sigma.\nNoise sampling in control point space.\nBatched interpolation with vmap.\n\n\n\n\n\nMirror autotune interface from pytorch_mppi/autotune*.py at a minimal level (evaluation fn + optimizer loop).\nPort tests/auto_tune_parameters.py logic into a JAX-friendly example."
  },
  {
    "objectID": "plan/completed/porting_pytorch_jax.html#status-jan-31-2026",
    "href": "plan/completed/porting_pytorch_jax.html#status-jan-31-2026",
    "title": "JAX MPPI Implementation Plan",
    "section": "",
    "text": "Overall Progress: Phase 6 complete (Autotuning system fully implemented with CMA-ES, Ray Tune, and CMA-ME support).\n\n\n\nPhase 1: Core MPPI ✅ COMPLETE\n\n353 lines implemented in src/jax_mppi/mppi.py\nAll core features from pytorch_mppi ported\n115 lines of unit tests in tests/test_mppi.py\n\nPhase 2: Pendulum Integration ✅ COMPLETE\n\n270 lines in examples/pendulum.py (full-featured example with CLI)\n282 lines in tests/test_pendulum.py (8 comprehensive integration tests)\nAll tests passing, swing-up and stabilization verified\n\nPhase 3: Smooth MPPI (SMPPI) ✅ COMPLETE\n\n634 lines implemented in src/jax_mppi/smppi.py\nAll SMPPI features: action_sequence, smoothness cost, dual bounds, integration\n580 lines in tests/test_smppi.py (18 comprehensive tests)\nAll tests passing\n\nPhase 4: Kernel MPPI (KMPPI) ✅ COMPLETE\n\n660 lines implemented in src/jax_mppi/kmppi.py\nRBFKernel, kernel interpolation, control point optimization\n595 lines in tests/test_kmppi.py (23 comprehensive tests)\nAll tests passing (53/53 total tests pass)\n\nPhase 5: Smooth Comparison Example ✅ COMPLETE\n\n442 lines in examples/smooth_comparison.py\nCompares MPPI, SMPPI, and KMPPI on 2D navigation with obstacle avoidance\nIncludes visualization with 4 subplots: trajectories, costs, controls, smoothness\nSupporting modules: src/jax_mppi/costs/ and src/jax_mppi/dynamics/\n\nPhase 6: Autotuning ✅ COMPLETE\n\n656 lines in src/jax_mppi/autotune.py - Core CMA-ES autotuning\n375 lines in src/jax_mppi/autotune_global.py - Ray Tune global search\n218 lines in src/jax_mppi/autotune_qd.py - CMA-ME quality diversity\n305 lines in tests/test_autotune.py (21 unit tests)\n247 lines in tests/test_autotune_integration.py (4 integration tests)\n321 lines in examples/autotune_pendulum.py - Full demonstration\n90 lines in examples/autotune_basic.py - Minimal example\nAll 25 tests passing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPackage\nCore Code\nTests\nExamples\nTotal\n\n\n\n\npytorch_mppi\n1214 lines\n~500 lines\n~800 lines\n~2500 lines\n\n\njax_mppi (current)\n2919 lines\n2124 lines\n681 lines\n5724 lines\n\n\nCompletion %\n240%\n425%\n85%\n229%\n\n\n\nCore code now includes: mppi.py (353), smppi.py (634), kmppi.py (660), autotune.py (656), autotune_global.py (375), autotune_qd.py (218), plus supporting modules.\n\n\n\n\n\n\nFeature\npytorch_mppi\njax_mppi\nStatus\n\n\n\n\nCore MPPI Algorithm\n✓\n✓\n✅ Complete\n\n\nBasic sampling & weighting\n✓\n✓\n✅\n\n\nControl bounds (u_min/u_max)\n✓\n✓\n✅\n\n\nControl scaling (u_scale)\n✓\n✓\n✅\n\n\nPartial updates (u_per_command)\n✓\n✓\n✅\n\n\nStep-dependent dynamics\n✓\n✓\n✅\n\n\nStochastic dynamics (rollout_samples)\n✓\n✓\n✅\n\n\nSample null action\n✓\n✓\n✅\n\n\nNoise absolute cost\n✓\n✓\n✅\n\n\nTerminal cost function\n✓\n✓\n✅\n\n\nShift nominal trajectory\n✓\n✓\n✅\n\n\nGet rollouts (visualization)\n✓\n✓\n✅\n\n\nReset controller\n✓\n✓\n✅\n\n\nSmooth MPPI (SMPPI)\n✓\n✓\n✅ Complete\n\n\nAction sequence tracking\n✓\n✓\n✅\n\n\nSmoothness penalty\n✓\n✓\n✅\n\n\nSeparate action/control bounds\n✓\n✓\n✅\n\n\nDelta_t integration\n✓\n✓\n✅\n\n\nShift with continuity\n✓\n✓\n✅\n\n\nKernel MPPI (KMPPI)\n✓\n✓\n✅ Complete\n\n\nKernel interpolation\n✓\n✓\n✅\n\n\nRBF kernel\n✓\n✓\n✅\n\n\nSupport point optimization\n✓\n✓\n✅\n\n\nTime grid management (Tk/Hs)\n✓\n✓\n✅\n\n\nSolve-based interpolation\n✓\n✓\n✅\n\n\nAutotuning\n✓\n✓\n✅ Complete\n\n\nCMA-ES local tuning\n✓\n✓\n✅\n\n\nRay Tune global search\n✓\n✓\n✅\n\n\nCMA-ME quality diversity\n✓\n✓\n✅\n\n\nParameter types (lambda, sigma, mu, horizon)\n✓\n✓\n✅\n\n\nAll MPPI variants support\n✓\n✓\n✅\n\n\nExamples\n\n\n\n\n\nPendulum swing-up\n✓\n✓\n✅ Complete\n\n\nSmooth MPPI comparison\n✓\n✓\n✅ Complete\n\n\nAutotuning example\n✓\n✓\n✅ Complete\n\n\nPendulum with learned dynamics\n✓\n✗\n🔴 Not planned\n\n\n\n\n\n\njax_mppi/\n├── pyproject.toml              ✅ Exists\n├── README.md                   ✅ Exists\n├── LICENSE                     ✅ Exists  \n├── src/jax_mppi/\n│   ├── __init__.py            ✅ Exists (updated for autotune)\n│   ├── types.py               ✅ Exists (9 lines)\n│   ├── mppi.py                ✅ Exists (353 lines) - COMPLETE\n│   ├── smppi.py               ✅ Exists (634 lines) - COMPLETE\n│   ├── kmppi.py               ✅ Exists (660 lines) - COMPLETE\n│   ├── autotune.py            ✅ Exists (656 lines) - COMPLETE\n│   ├── autotune_global.py     ✅ Exists (375 lines) - COMPLETE\n│   ├── autotune_qd.py         ✅ Exists (218 lines) - COMPLETE\n│   ├── costs/                 ✅ Exists (supporting modules)\n│   └── dynamics/              ✅ Exists (supporting modules)\n├── tests/\n│   ├── test_mppi.py           ✅ Exists (115 lines) - COMPLETE\n│   ├── test_pendulum.py       ✅ Exists (282 lines) - COMPLETE\n│   ├── test_smppi.py          ✅ Exists (580 lines) - COMPLETE\n│   ├── test_autotune.py       ✅ Exists (305 lines, 21 tests) - COMPLETE\n│   └── test_autotune_integration.py ✅ Exists (247 lines, 4 tests) - COMPLETE\n│   └── test_kmppi.py          ✅ Exists (595 lines) - COMPLETE\n├── examples/\n│   ├── pendulum.py            ✅ Exists (270 lines) - COMPLETE\n│   ├── smooth_comparison.py   ✅ Exists (442 lines) - COMPLETE\n│   ├── autotune_pendulum.py   ✅ Exists (321 lines) - COMPLETE\n│   └── autotune_basic.py      ✅ Exists (90 lines) - COMPLETE\n└── docs/\n    └── plan/\n        └── porting_pytorch_jax.md ✅ This file\n\n\n\nPriority Order:\n\nPhase 3: SMPPI Implementation (High Priority)\n\nCore functionality that adds smoothness to control\nEstimated ~250-300 lines for smppi.py\nEstimated ~150-200 lines for tests\nReference: ../pytorch_mppi/src/pytorch_mppi/mppi.py (SMPPI class)\n\nPhase 4: KMPPI Implementation (High Priority)\n\nNovel contribution with kernel interpolation\nEstimated ~300-350 lines for kmppi.py\nEstimated ~150-200 lines for tests\nReference: ../pytorch_mppi/src/pytorch_mppi/mppi.py (KMPPI class)\n\nPhase 5: Smooth Comparison Example (Medium Priority)\n\nDemonstrates value of SMPPI and KMPPI\nEstimated ~200-250 lines\nReference: ../pytorch_mppi/tests/smooth_mppi.py\n\nAdditional Examples (Low Priority)\n\nPendulum with learned dynamics\nMore complex environments\n\nPhase 6: Autotuning (Optional/Stretch)\n\nAdvanced feature for hyperparameter optimization\nEstimated ~300-400 lines\nReference: ../pytorch_mppi/src/pytorch_mppi/autotune.py"
  },
  {
    "objectID": "plan/completed/porting_pytorch_jax.html#design-decisions",
    "href": "plan/completed/porting_pytorch_jax.html#design-decisions",
    "title": "JAX MPPI Implementation Plan",
    "section": "",
    "text": "Use @jax.tree_util.register_dataclass (or flax.struct.dataclass) to hold MPPI state (nominal trajectory U, PRNG key, config). All core functions are pure: command(state, mppi_state) -&gt; (action, mppi_state).\nRationale: Idiomatic JAX — pure functions compose with jit, vmap, grad. No mutable self. Avoids heavyweight dependencies like Equinox for what is fundamentally a numerical algorithm.\n\n\n\n\n\n\n\n\n\n\nPyTorch\nJAX\n\n\n\n\ntorch.distributions.MultivariateNormal\njax.random.multivariate_normal\n\n\ntensor.to(device)\njax.device_put / automatic\n\n\nPython for-loop over horizon\njax.lax.scan\n\n\n@handle_batch_input decorator\njax.vmap\n\n\ntorch.roll\njnp.roll\n\n\ntorch.linalg.solve\njnp.linalg.solve\n\n\nIn-place mutation (self.U = ...)\nReturn new state (pytree)"
  },
  {
    "objectID": "plan/completed/porting_pytorch_jax.html#notes-from-..pytorch_mppi-review-jan-2026",
    "href": "plan/completed/porting_pytorch_jax.html#notes-from-..pytorch_mppi-review-jan-2026",
    "title": "JAX MPPI Implementation Plan",
    "section": "",
    "text": "Actionable parity items to carry over:\n\nSMPPI semantics: maintains action_sequence separately from lifted control U; integrates with delta_t; smoothness cost from diff(action_sequence).\nSMPPI bounds: support action_min/action_max distinct from u_min/u_max (control-derivative bounds).\nKMPPI internals: keep theta as control points; build Tk/Hs time grids; kernel interpolation via solve(Ktktk, K); batch interpolation with vmap.\nSampling options: rollout_samples (M), sample_null_action, noise_abs_cost (abs(noise) in action cost).\nRollouts: get_rollouts handles state batch and dynamics that may augment state (take first nx)."
  },
  {
    "objectID": "plan/completed/porting_pytorch_jax.html#package-structure",
    "href": "plan/completed/porting_pytorch_jax.html#package-structure",
    "title": "JAX MPPI Implementation Plan",
    "section": "",
    "text": "jax_mppi/\n├── pyproject.toml\n├── README.md\n├── LICENSE\n├── src/jax_mppi/\n│   ├── __init__.py          # Public API exports\n│   ├── mppi.py              # Core MPPI (MPPIConfig, MPPIState, command, reset, etc.)\n│   ├── smppi.py             # Smooth MPPI variant\n│   ├── kmppi.py             # Kernel MPPI variant + TimeKernel / RBFKernel\n│   ├── types.py             # Type aliases, protocols for Dynamics/Cost callables\n│   └── autotune.py          # Autotuning (CMA-ES wrapper, parameter search)\n├── tests/\n│   ├── test_mppi.py         # Unit tests for core MPPI\n│   ├── test_smppi.py        # Unit tests for SMPPI\n│   ├── test_kmppi.py        # Unit tests for KMPPI\n│   └── test_pendulum.py     # Integration test with pendulum env\n├── examples/\n│   ├── pendulum.py          # Gym pendulum with true dynamics\n│   ├── pendulum_approximate.py  # Learned dynamics\n│   └── smooth_comparison.py # MPPI vs SMPPI vs KMPPI\n└── docs/\n    └── plan/"
  },
  {
    "objectID": "plan/completed/porting_pytorch_jax.html#phased-implementation",
    "href": "plan/completed/porting_pytorch_jax.html#phased-implementation",
    "title": "JAX MPPI Implementation Plan",
    "section": "",
    "text": "Files: pyproject.toml, src/jax_mppi/types.py, src/jax_mppi/mppi.py, src/jax_mppi/__init__.py\n\npyproject.toml — project metadata, deps: jax[cuda13], jaxlib, optional gymnasium for examples.\ntypes.py — Type definitions:\n# Dynamics: (state, action) -&gt; next_state  or  (state, action, t) -&gt; next_state\nDynamicsFn = Callable[..., jax.Array]\n# Cost: (state, action) -&gt; scalar_cost  or  (state, action, t) -&gt; scalar_cost\nRunningCostFn = Callable[..., jax.Array]\n# Terminal: (states, actions) -&gt; scalar_cost\nTerminalCostFn = Callable[[jax.Array, jax.Array], jax.Array]\nmppi.py — Core implementation:\nData structures (registered as JAX pytrees):\n@dataclass\nclass MPPIConfig:\n    # Static config (not traced through JAX)\n    num_samples: int       # K\n    horizon: int           # T\n    nx: int\n    nu: int\n    lambda_: float\n    u_scale: float\n    u_per_command: int\n    step_dependent_dynamics: bool\n    rollout_samples: int   # M\n    rollout_var_cost: float\n    rollout_var_discount: float\n    sample_null_action: bool\n    noise_abs_cost: bool\n\n@dataclass\nclass MPPIState:\n    # Dynamic state (carried through JAX transforms)\n    U: jax.Array           # (T, nu) nominal trajectory\n    u_init: jax.Array      # (nu,) default action for shift\n    noise_mu: jax.Array    # (nu,)\n    noise_sigma: jax.Array # (nu, nu)\n    noise_sigma_inv: jax.Array\n    u_min: jax.Array | None\n    u_max: jax.Array | None\n    key: jax.Array         # PRNG key\nFunctions:\ndef create(\n    nx, nu, noise_sigma, num_samples=100, horizon=15, lambda_=1.0,\n    noise_mu=None, u_min=None, u_max=None, u_init=None, U_init=None,\n    u_scale=1, u_per_command=1, step_dependent_dynamics=False,\n    rollout_samples=1, rollout_var_cost=0., rollout_var_discount=0.95,\n    sample_null_action=False, noise_abs_cost=False, key=None,\n) -&gt; tuple[MPPIConfig, MPPIState]:\n    \"\"\"Factory: create config + initial state.\"\"\"\n\ndef command(\n    config: MPPIConfig,\n    mppi_state: MPPIState,\n    current_obs: jax.Array,\n    dynamics: DynamicsFn,\n    running_cost: RunningCostFn,\n    terminal_cost: TerminalCostFn | None = None,\n    shift: bool = True,\n) -&gt; tuple[jax.Array, MPPIState]:\n    \"\"\"Compute optimal action and return updated state.\"\"\"\n\ndef reset(config: MPPIConfig, mppi_state: MPPIState, key: jax.Array) -&gt; MPPIState:\n    \"\"\"Reset nominal trajectory.\"\"\"\n\ndef get_rollouts(\n    config: MPPIConfig, mppi_state: MPPIState,\n    current_obs: jax.Array, dynamics: DynamicsFn,\n    num_rollouts: int = 1,\n) -&gt; jax.Array:\n    \"\"\"Forward-simulate trajectories for visualization.\"\"\"\nInternal functions (all JIT-compatible):\n\n_shift_nominal(mppi_state) -&gt; MPPIState — jnp.roll + set last to u_init\n_sample_noise(key, K, T, noise_mu, noise_sigma) -&gt; (noise, new_key) — sample from multivariate normal\n_compute_rollout_costs(config, current_obs, perturbed_actions, dynamics, running_cost, terminal_cost) — uses jax.lax.scan over horizon, jax.vmap over K samples\n_compute_weights(costs, lambda_) — softmax importance weighting\n_bound_action(action, u_min, u_max) — jnp.clip\n\nKey JAX patterns:\n\nRollout loop: jax.lax.scan with carry = (state,), xs = actions[t]\nBatch over K samples: jax.vmap(_single_rollout, in_axes=(0, None, ...))\nBatch over M rollout samples (stochastic dynamics): nested vmap or scan\nAll internal functions decorated with @jax.jit or called inside a top-level jitted command\n\nUnit test: tests/test_mppi.py\n\nTest create() produces valid config/state\nTest command() returns correct shape\nTest cost reduction over iterations on simple 1D problem\nTest bounds are respected\n\n\n\n\n\nFiles: examples/pendulum.py, tests/test_pendulum.py\n\nImplement pendulum dynamics as a pure JAX function (no gym dependency for core test)\nRun MPPI loop, verify convergence (swing-up or stabilization)\nOptional: gym rendering wrapper for visualization\n\n\n\n\nFiles: src/jax_mppi/smppi.py, tests/test_smppi.py\n\nData structures:\n@dataclass\nclass SMPPIState(MPPIState):\n    action_sequence: jax.Array  # (T, nu) actual actions\n    w_action_seq_cost: float\n    delta_t: float\n    action_min: jax.Array | None\n    action_max: jax.Array | None\nFunctions: Same API as mppi.py but with:\n\n_shift_nominal shifts both U (velocity) and action_sequence\n_compute_perturbed_actions integrates velocity to get actions\n_compute_total_cost adds smoothness penalty: ||diff(actions)||^2\nreset() zeros both U and action_sequence\nchange_horizon() keeps both U and action_sequence in sync (truncate/extend)\n\nTest: Verify smoother trajectories than base MPPI on 2D navigation\n\n\n\n\nFiles: src/jax_mppi/kmppi.py, tests/test_kmppi.py\n\nKernel abstractions:\ndef rbf_kernel(t, tk, sigma=1.0):\n    d = jnp.sum((t[:, None] - tk) ** 2, axis=-1)\n    return jnp.exp(-d / (2 * sigma ** 2 + 1e-8))\n\ndef kernel_interpolate(t, tk, coeffs, kernel_fn):\n    K_t_tk = kernel_fn(t, tk)\n    K_tk_tk = kernel_fn(tk, tk)\n    weights = jnp.linalg.solve(K_tk_tk, K_t_tk.T).T\n    return weights @ coeffs\nData structures:\n@dataclass\nclass KMPPIState(MPPIState):\n    theta: jax.Array         # (num_support_pts, nu)\n    num_support_pts: int\nFunctions: Override _compute_perturbed_actions to sample sparse + interpolate. Update theta instead of U.\n\nBuild Tk and Hs time grids on init and on horizon changes\nUse kernel_interpolate() with solve(Ktktk, K) (avoid explicit inverse)\nBatch interpolate with jax.vmap for K samples\n\nTest: Verify fewer parameters produce smooth trajectories\n\n\n\n\nFiles: examples/smooth_comparison.py\n\nSide-by-side MPPI vs SMPPI vs KMPPI on 2D navigation\nPlot trajectories and control signals\n\n\n\n\nFiles: src/jax_mppi/autotune.py\n\nWrap CMA-ES (cmaes or evosax for JAX-native) for sigma/lambda/horizon tuning\nSimpler than pytorch_mppi’s framework — skip Ray Tune and QD initially\nFunctional API: tune_step(eval_fn, params, optimizer_state) -&gt; (params, optimizer_state)"
  },
  {
    "objectID": "plan/completed/porting_pytorch_jax.html#verification-strategy",
    "href": "plan/completed/porting_pytorch_jax.html#verification-strategy",
    "title": "JAX MPPI Implementation Plan",
    "section": "",
    "text": "Unit tests (per phase): pytest tests/ — shape checks, cost reduction, bounds\nPendulum benchmark: Compare convergence (total reward) against pytorch_mppi on same scenario\nJIT correctness: Ensure jax.jit(command) produces identical results to non-jitted version\nPerformance: Benchmark command() latency vs pytorch_mppi (JAX should win after warmup due to XLA compilation)\nSmooth variants: Visual comparison of trajectory smoothness\n\n\n\nIMPORTANT: You should always use the virtual environment. To run the tests and all of the other python files.\n\nOption A: add a tests/conftest.py to insert src into sys.path.\nOption B: run tests after uv pip install -e . (editable install)."
  },
  {
    "objectID": "plan/completed/porting_pytorch_jax.html#dependencies",
    "href": "plan/completed/porting_pytorch_jax.html#dependencies",
    "title": "JAX MPPI Implementation Plan",
    "section": "",
    "text": "Core: jax[cuda13], jaxlib, numpy Testing: pytest, gymnasium[classic_control] Autotuning (optional): cmaes or evosax Examples (optional): matplotlib, gymnasium"
  },
  {
    "objectID": "plan/completed/porting_pytorch_jax.html#actionable-task-checklist",
    "href": "plan/completed/porting_pytorch_jax.html#actionable-task-checklist",
    "title": "JAX MPPI Implementation Plan",
    "section": "",
    "text": "Mirror pytorch_mppi signature flags: rollout_samples, sample_null_action, noise_abs_cost.\nImplement get_rollouts handling: accept single or batched state; allow dynamics that augment state (take :nx).\nAdd shift_nominal_trajectory via jnp.roll + u_init fill.\nImplement action cost with optional abs(noise) branch.\nAdd u_per_command slicing and u_scale application in command.\n\n\n\n\n\nCarry action_sequence in state and integrate U with delta_t.\nImplement distinct action bounds (action_min/action_max) vs control bounds (u_min/u_max).\nAdd smoothness cost from diff(action_sequence) and weight w_action_seq_cost.\nEnsure reset() updates both U and action_sequence.\nImplement proper shift with action continuity (hold last value).\nImplement dual bounding system (_bound_control and _bound_action).\nRecompute effective noise after bounding for accurate cost.\n\n\n\n\n\nImplement theta control points + interpolation kernel (RBF by default).\nBuild Tk/Hs grids and re-build on horizon changes.\nUse solve(Ktktk, K) for interpolation weights (no explicit inverse).\nShift theta via interpolation when shifting nominal trajectory.\nImplement RBFKernel with configurable sigma.\nNoise sampling in control point space.\nBatched interpolation with vmap.\n\n\n\n\n\nMirror autotune interface from pytorch_mppi/autotune*.py at a minimal level (evaluation fn + optimizer loop).\nPort tests/auto_tune_parameters.py logic into a JAX-friendly example."
  },
  {
    "objectID": "theory.html",
    "href": "theory.html",
    "title": "Theoretical Background",
    "section": "",
    "text": "This section provides the mathematical foundations for the Model Predictive Path Integral (MPPI) control algorithm and its variants implemented in jax_mppi.\n\n\nModel Predictive Path Integral (MPPI) control is a sampling-based model predictive control algorithm derived from information-theoretic principles. It solves the stochastic optimal control problem by simulating multiple trajectories and updating the control policy based on their costs.\n\n\nWe consider a discrete-time dynamical system with dynamics:\n[ _{t+1} = f(_t, _t) + _t ]\nwhere \\(\\mathbf{x}_t \\in \\mathbb{R}^{n_x}\\) is the state, \\(\\mathbf{u}_t \\in \\mathbb{R}^{n_u}\\) is the control input, and \\(\\mathbf{v}_t \\sim \\mathcal{N}(0, \\Sigma)\\) is Gaussian noise.\nThe objective is to find the control sequence \\(U = \\{\\mathbf{u}_0, \\dots, \\mathbf{u}_{T-1}\\}\\) that minimizes the expected cost:\n[ J(U) = ]\nwhere \\(\\phi(\\mathbf{x}_T)\\) is the terminal cost and \\(q(\\mathbf{x}_t)\\) is the state-dependent running cost. The term \\(\\frac{1}{2} \\mathbf{u}_t^T \\Sigma^{-1} \\mathbf{u}_t\\) represents the control effort cost.\n\n\n\nMPPI relies on the duality between free energy and relative entropy (KL divergence). The optimal control distribution \\(p^*\\) is proportional to the exponential of the trajectory cost:\n[ p^*() (- S()) ]\nwhere \\(S(\\tau)\\) is the cost of a trajectory \\(\\tau\\) and \\(\\lambda\\) is a temperature parameter.\n\n\n\nIn practice, we approximate the optimal control by sampling \\(K\\) trajectories around a nominal control sequence \\(\\mathbf{u}_{nom}\\). For each sample \\(k\\), we apply a perturbation \\(\\epsilon_k \\sim \\mathcal{N}(0, \\Sigma)\\):\n[ {t, k} = {nom, t} + _{t, k} ]\nThe cost for the \\(k\\)-th trajectory is computed as:\n[ S_k = ({T, k}) + {t=0}^{T-1} ( q({t, k}) + {nom, t}^T ^{-1} _{t, k} ) ]\nThe weights for each trajectory are computed using the softmax function:\n[ w_k = ]\nwhere \\(\\beta = \\min_k S_k\\) for numerical stability.\nThe control sequence is then updated by computing the weighted average of the perturbations:\n[ {new, t} = {nom, t} + {k=1}^K w_k {t, k} ]\n\n\n\n\nStandard MPPI assumes the control inputs are independent across time steps, which can lead to jerky or non-smooth control signals. Smooth MPPI (SMPPI) addresses this by lifting the control problem to a higher-order space (e.g., controlling acceleration instead of velocity).\n\n\nIn SMPPI, the nominal trajectory \\(U\\) represents the derivative of the actual action (e.g., acceleration). The actual action \\(\\mathbf{a}_t\\) is part of the state or computed by integrating \\(U\\).\nLet \\(\\mathbf{u}_t\\) be the command at time \\(t\\) (from the optimizer). The action applied to the system is \\(\\mathbf{a}_t\\), updated as:\n[ _{t+1} = _t + _t t ]\n\n\n\nSMPPI explicitly penalizes changes in the action sequence to encourage smoothness. The cost function includes a term for the magnitude of the command \\(\\mathbf{u}_t\\) (which corresponds to the change in action):\n[ J_{smooth} = _{t=0}^{T-1} ||t||^2 = {t=0}^{T-1} ||||^2 ]\nThis formulation ensures that the generated trajectories are smooth and feasible for systems with actuation limits or bandwidth constraints.\n\n\n\n\nKernel MPPI (KMPPI) parameterizes the control trajectory using a set of basis functions or kernels, rather than optimizing the control input at every time step independently. This reduces the dimensionality of the optimization problem and implicitly enforces smoothness.\n\n\nWe assume the control trajectory \\(\\mathbf{u}(t)\\) lies in a Reproducing Kernel Hilbert Space (RKHS) defined by a kernel \\(k(t, t')\\). The control is represented as a linear combination of basis functions centered at support points \\(t_i\\):\n[ (t) = _{i=1}^{M} _i k(t, t_i) ]\nwhere \\(M\\) is the number of support points (often \\(M &lt; T\\)), and \\(\\alpha_i\\) are the weights (parameters) to be optimized.\n\n\n\nInstead of perturbing the control inputs \\(\\mathbf{u}_t\\) directly, KMPPI perturbs the parameters \\(\\alpha_i\\) (or equivalent control points).\nLet \\(\\theta\\) represent the parameters. We sample perturbations \\(\\delta \\theta_k \\sim \\mathcal{N}(0, \\Sigma_\\theta)\\). The corresponding control trajectory is:\n[ _k(t) = (+ _k) ]\nThe update rule is applied to \\(\\theta\\):\n[ {new} = {nom} + _{k=1}^K w_k _k ]\nBy choosing an appropriate kernel (e.g., RBF kernel), we can control the smoothness and frequency content of the resulting trajectories.",
    "crumbs": [
      "Home",
      "Introduction",
      "Theoretical Background"
    ]
  },
  {
    "objectID": "theory.html#standard-mppi",
    "href": "theory.html#standard-mppi",
    "title": "Theoretical Background",
    "section": "",
    "text": "Model Predictive Path Integral (MPPI) control is a sampling-based model predictive control algorithm derived from information-theoretic principles. It solves the stochastic optimal control problem by simulating multiple trajectories and updating the control policy based on their costs.\n\n\nWe consider a discrete-time dynamical system with dynamics:\n[ _{t+1} = f(_t, _t) + _t ]\nwhere \\(\\mathbf{x}_t \\in \\mathbb{R}^{n_x}\\) is the state, \\(\\mathbf{u}_t \\in \\mathbb{R}^{n_u}\\) is the control input, and \\(\\mathbf{v}_t \\sim \\mathcal{N}(0, \\Sigma)\\) is Gaussian noise.\nThe objective is to find the control sequence \\(U = \\{\\mathbf{u}_0, \\dots, \\mathbf{u}_{T-1}\\}\\) that minimizes the expected cost:\n[ J(U) = ]\nwhere \\(\\phi(\\mathbf{x}_T)\\) is the terminal cost and \\(q(\\mathbf{x}_t)\\) is the state-dependent running cost. The term \\(\\frac{1}{2} \\mathbf{u}_t^T \\Sigma^{-1} \\mathbf{u}_t\\) represents the control effort cost.\n\n\n\nMPPI relies on the duality between free energy and relative entropy (KL divergence). The optimal control distribution \\(p^*\\) is proportional to the exponential of the trajectory cost:\n[ p^*() (- S()) ]\nwhere \\(S(\\tau)\\) is the cost of a trajectory \\(\\tau\\) and \\(\\lambda\\) is a temperature parameter.\n\n\n\nIn practice, we approximate the optimal control by sampling \\(K\\) trajectories around a nominal control sequence \\(\\mathbf{u}_{nom}\\). For each sample \\(k\\), we apply a perturbation \\(\\epsilon_k \\sim \\mathcal{N}(0, \\Sigma)\\):\n[ {t, k} = {nom, t} + _{t, k} ]\nThe cost for the \\(k\\)-th trajectory is computed as:\n[ S_k = ({T, k}) + {t=0}^{T-1} ( q({t, k}) + {nom, t}^T ^{-1} _{t, k} ) ]\nThe weights for each trajectory are computed using the softmax function:\n[ w_k = ]\nwhere \\(\\beta = \\min_k S_k\\) for numerical stability.\nThe control sequence is then updated by computing the weighted average of the perturbations:\n[ {new, t} = {nom, t} + {k=1}^K w_k {t, k} ]",
    "crumbs": [
      "Home",
      "Introduction",
      "Theoretical Background"
    ]
  },
  {
    "objectID": "theory.html#smooth-mppi-smppi",
    "href": "theory.html#smooth-mppi-smppi",
    "title": "Theoretical Background",
    "section": "",
    "text": "Standard MPPI assumes the control inputs are independent across time steps, which can lead to jerky or non-smooth control signals. Smooth MPPI (SMPPI) addresses this by lifting the control problem to a higher-order space (e.g., controlling acceleration instead of velocity).\n\n\nIn SMPPI, the nominal trajectory \\(U\\) represents the derivative of the actual action (e.g., acceleration). The actual action \\(\\mathbf{a}_t\\) is part of the state or computed by integrating \\(U\\).\nLet \\(\\mathbf{u}_t\\) be the command at time \\(t\\) (from the optimizer). The action applied to the system is \\(\\mathbf{a}_t\\), updated as:\n[ _{t+1} = _t + _t t ]\n\n\n\nSMPPI explicitly penalizes changes in the action sequence to encourage smoothness. The cost function includes a term for the magnitude of the command \\(\\mathbf{u}_t\\) (which corresponds to the change in action):\n[ J_{smooth} = _{t=0}^{T-1} ||t||^2 = {t=0}^{T-1} ||||^2 ]\nThis formulation ensures that the generated trajectories are smooth and feasible for systems with actuation limits or bandwidth constraints.",
    "crumbs": [
      "Home",
      "Introduction",
      "Theoretical Background"
    ]
  },
  {
    "objectID": "theory.html#kernel-mppi-kmppi",
    "href": "theory.html#kernel-mppi-kmppi",
    "title": "Theoretical Background",
    "section": "",
    "text": "Kernel MPPI (KMPPI) parameterizes the control trajectory using a set of basis functions or kernels, rather than optimizing the control input at every time step independently. This reduces the dimensionality of the optimization problem and implicitly enforces smoothness.\n\n\nWe assume the control trajectory \\(\\mathbf{u}(t)\\) lies in a Reproducing Kernel Hilbert Space (RKHS) defined by a kernel \\(k(t, t')\\). The control is represented as a linear combination of basis functions centered at support points \\(t_i\\):\n[ (t) = _{i=1}^{M} _i k(t, t_i) ]\nwhere \\(M\\) is the number of support points (often \\(M &lt; T\\)), and \\(\\alpha_i\\) are the weights (parameters) to be optimized.\n\n\n\nInstead of perturbing the control inputs \\(\\mathbf{u}_t\\) directly, KMPPI perturbs the parameters \\(\\alpha_i\\) (or equivalent control points).\nLet \\(\\theta\\) represent the parameters. We sample perturbations \\(\\delta \\theta_k \\sim \\mathcal{N}(0, \\Sigma_\\theta)\\). The corresponding control trajectory is:\n[ _k(t) = (+ _k) ]\nThe update rule is applied to \\(\\theta\\):\n[ {new} = {nom} + _{k=1}^K w_k _k ]\nBy choosing an appropriate kernel (e.g., RBF kernel), we can control the smoothness and frequency content of the resulting trajectories.",
    "crumbs": [
      "Home",
      "Introduction",
      "Theoretical Background"
    ]
  }
]