[
  {
    "objectID": "src/dynamics.html",
    "href": "src/dynamics.html",
    "title": "Quadrotor Dynamics",
    "section": "",
    "text": "This section documents the 6-DOF quadrotor dynamics model used for trajectory rollouts in jax_mppi. The implementation follows the NED-FRD convention:",
    "crumbs": [
      "Home",
      "Theory",
      "Quadrotor Dynamics"
    ]
  },
  {
    "objectID": "src/dynamics.html#state-and-control-spaces",
    "href": "src/dynamics.html#state-and-control-spaces",
    "title": "Quadrotor Dynamics",
    "section": "State and Control Spaces",
    "text": "State and Control Spaces\n\nState Vector (13D)\n\\[\n\\mathbf{x} = \\begin{bmatrix} \\mathbf{p} \\\\ \\mathbf{v} \\\\ \\mathbf{q} \\\\ \\boldsymbol{\\omega} \\end{bmatrix} \\in \\mathbb{R}^{13}\n\\]\n\n\n\n\n\n\n\n\n\n\nComponent\nSymbol\nDimension\nFrame\nDescription\n\n\n\n\nPosition\n\\(\\mathbf{p} = [p_x, p_y, p_z]^T\\)\n3\nNED world\nCartesian position\n\n\nVelocity\n\\(\\mathbf{v} = [v_x, v_y, v_z]^T\\)\n3\nNED world\nLinear velocity\n\n\nQuaternion\n\\(\\mathbf{q} = [q_w, q_x, q_y, q_z]^T\\)\n4\nBody‚ÜíWorld\nOrientation (unit norm)\n\n\nAngular velocity\n\\(\\boldsymbol{\\omega} = [\\omega_x, \\omega_y, \\omega_z]^T\\)\n3\nFRD body\nAngular rates\n\n\n\n\n\nControl Vector (4D)\n\\[\n\\mathbf{u} = \\begin{bmatrix} T \\\\ \\omega_x^{cmd} \\\\ \\omega_y^{cmd} \\\\ \\omega_z^{cmd} \\end{bmatrix} \\in \\mathbb{R}^4\n\\]\n\n\n\n\n\n\n\n\nComponent\nDescription\nDefault Bounds\n\n\n\n\n\\(T\\)\nThrust magnitude (acts in \\(-\\mathbf{z}_B\\) direction, i.e.¬†upward in FRD)\n\\([0,\\; 4mg]\\)\n\n\n\\(\\omega_x^{cmd}\\)\nRoll rate command\n\\([-10,\\; 10]\\) rad/s\n\n\n\\(\\omega_y^{cmd}\\)\nPitch rate command\n\\([-10,\\; 10]\\) rad/s\n\n\n\\(\\omega_z^{cmd}\\)\nYaw rate command\n\\([-10,\\; 10]\\) rad/s",
    "crumbs": [
      "Home",
      "Theory",
      "Quadrotor Dynamics"
    ]
  },
  {
    "objectID": "src/dynamics.html#equations-of-motion",
    "href": "src/dynamics.html#equations-of-motion",
    "title": "Quadrotor Dynamics",
    "section": "Equations of Motion",
    "text": "Equations of Motion\n\nTranslational Dynamics\nThe translational acceleration in the NED world frame is:\n\\[\n\\dot{\\mathbf{v}} = \\frac{1}{m}\\left(\\mathbf{f}_g + \\mathbf{R}(\\mathbf{q})\\,\\mathbf{f}_T\\right)\n\\]\nwhere:\n\n\\(\\mathbf{f}_g = [0,\\; 0,\\; mg]^T\\) is gravity (positive-\\(z\\) = downward in NED)\n\\(\\mathbf{f}_T = [0,\\; 0,\\; -T]^T\\) is thrust in the body frame (upward in FRD)\n\\(\\mathbf{R}(\\mathbf{q})\\) rotates from body FRD to world NED\n\nPosition evolves as \\(\\dot{\\mathbf{p}} = \\mathbf{v}\\).\n\n\nQuaternion Kinematics\nThe quaternion derivative relates angular velocity in the body frame to orientation change:\n\\[\n\\dot{\\mathbf{q}} = \\frac{1}{2}\\,\\boldsymbol{\\Omega}(\\boldsymbol{\\omega})\\,\\mathbf{q}\n\\]\nwhere \\(\\boldsymbol{\\Omega}(\\boldsymbol{\\omega})\\) is the quaternion multiplication matrix. Expanding component-wise:\n\\[\n\\begin{aligned}\n\\dot{q}_w &= -\\tfrac{1}{2}(\\omega_x q_x + \\omega_y q_y + \\omega_z q_z) \\\\\n\\dot{q}_x &= \\phantom{-}\\tfrac{1}{2}(\\omega_x q_w + \\omega_z q_y - \\omega_y q_z) \\\\\n\\dot{q}_y &= \\phantom{-}\\tfrac{1}{2}(\\omega_y q_w - \\omega_z q_x + \\omega_x q_z) \\\\\n\\dot{q}_z &= \\phantom{-}\\tfrac{1}{2}(\\omega_z q_w + \\omega_y q_x - \\omega_x q_y)\n\\end{aligned}\n\\]\n\n\nRotation Matrix\nThe rotation matrix \\(\\mathbf{R}(\\mathbf{q})\\) from body FRD to world NED is:\n\\[\n\\mathbf{R} = \\begin{bmatrix}\n1 - 2(q_y^2 + q_z^2) & 2(q_xq_y - q_wq_z) & 2(q_xq_z + q_wq_y) \\\\\n2(q_xq_y + q_wq_z) & 1 - 2(q_x^2 + q_z^2) & 2(q_yq_z - q_wq_x) \\\\\n2(q_xq_z - q_wq_y) & 2(q_yq_z + q_wq_x) & 1 - 2(q_x^2 + q_y^2)\n\\end{bmatrix}\n\\]\n\n\nRotational Dynamics\nThe angular velocity is modeled as a first-order system tracking the commanded rates:\n\\[\n\\dot{\\boldsymbol{\\omega}} = \\frac{\\boldsymbol{\\omega}^{cmd} - \\boldsymbol{\\omega}}{\\tau_\\omega}\n\\]\nwhere \\(\\tau_\\omega\\) is the angular velocity time constant (default 0.05 s). This simplified model abstracts away the full Euler equations and motor dynamics, assuming a fast inner-loop attitude controller.",
    "crumbs": [
      "Home",
      "Theory",
      "Quadrotor Dynamics"
    ]
  },
  {
    "objectID": "src/dynamics.html#numerical-integration",
    "href": "src/dynamics.html#numerical-integration",
    "title": "Quadrotor Dynamics",
    "section": "Numerical Integration",
    "text": "Numerical Integration\n\nRK4 Step\nThe continuous-time dynamics \\(\\dot{\\mathbf{x}} = g(\\mathbf{x}, \\mathbf{u})\\) are integrated using the classical 4th-order Runge-Kutta method:\n\\[\n\\begin{aligned}\n\\mathbf{k}_1 &= g(\\mathbf{x}_t, \\mathbf{u}_t) \\\\\n\\mathbf{k}_2 &= g(\\mathbf{x}_t + \\tfrac{\\Delta t}{2}\\mathbf{k}_1, \\mathbf{u}_t) \\\\\n\\mathbf{k}_3 &= g(\\mathbf{x}_t + \\tfrac{\\Delta t}{2}\\mathbf{k}_2, \\mathbf{u}_t) \\\\\n\\mathbf{k}_4 &= g(\\mathbf{x}_t + \\Delta t\\,\\mathbf{k}_3, \\mathbf{u}_t) \\\\[4pt]\n\\mathbf{x}_{t+1} &= \\mathbf{x}_t + \\frac{\\Delta t}{6}(\\mathbf{k}_1 + 2\\mathbf{k}_2 + 2\\mathbf{k}_3 + \\mathbf{k}_4)\n\\end{aligned}\n\\]\n\n\nQuaternion Renormalization\nAfter each RK4 step, the quaternion component is renormalized to maintain the unit-norm constraint:\n\\[\n\\mathbf{q}_{t+1} \\leftarrow \\frac{\\mathbf{q}_{t+1}}{\\|\\mathbf{q}_{t+1}\\| + \\varepsilon}\n\\]\nwith \\(\\varepsilon = 10^{-8}\\) for numerical safety. This is necessary because RK4 does not preserve the constraint \\(\\|\\mathbf{q}\\| = 1\\) exactly.",
    "crumbs": [
      "Home",
      "Theory",
      "Quadrotor Dynamics"
    ]
  },
  {
    "objectID": "src/dynamics.html#augmented-state-for-i-mppi",
    "href": "src/dynamics.html#augmented-state-for-i-mppi",
    "title": "Quadrotor Dynamics",
    "section": "Augmented State for I-MPPI",
    "text": "Augmented State for I-MPPI\nFor informative path planning (I-MPPI), the quadrotor state is augmented with information zone levels, yielding a 16D state vector:\n\\[\n\\mathbf{x}_{\\text{aug}} = \\begin{bmatrix} \\mathbf{x}_{\\text{quad}} \\\\ \\mathbf{i} \\end{bmatrix} \\in \\mathbb{R}^{16}\n\\]\nwhere \\(\\mathbf{i} = [i_1, i_2, i_3]^T\\) tracks the remaining information content of each zone (\\(i_j \\in [0, 100]\\)).\nThe augmented dynamics are:\n\\[\n\\begin{aligned}\n\\mathbf{x}_{\\text{quad}}^{(t+1)} &= f_{\\text{RK4}}(\\mathbf{x}_{\\text{quad}}^{(t)}, \\mathbf{u}^{(t)}) \\\\\ni_j^{(t+1)} &= i_j^{(t)} \\left(1 - \\alpha \\cdot c_j^{(t)}\\right)\n\\end{aligned}\n\\]\nwhere \\(\\alpha\\) is the depletion rate and \\(c_j^{(t)} \\in [0, 1]\\) is the FOV coverage of zone \\(j\\) at time \\(t\\). See the I-MPPI documentation for details on the coverage and depletion model.",
    "crumbs": [
      "Home",
      "Theory",
      "Quadrotor Dynamics"
    ]
  },
  {
    "objectID": "src/dynamics.html#parameters",
    "href": "src/dynamics.html#parameters",
    "title": "Quadrotor Dynamics",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nParameter\nSymbol\nDefault\nDescription\n\n\n\n\nMass\n\\(m\\)\n1.0 kg\nQuadrotor mass\n\n\nGravity\n\\(g\\)\n9.81 m/s\\(^2\\)\nGravitational acceleration (positive down in NED)\n\n\nAngular time constant\n\\(\\tau_\\omega\\)\n0.05 s\nFirst-order rate tracking bandwidth\n\n\nIntegration step\n\\(\\Delta t\\)\n0.01 s\nRK4 time step",
    "crumbs": [
      "Home",
      "Theory",
      "Quadrotor Dynamics"
    ]
  },
  {
    "objectID": "src/i_mppi.html",
    "href": "src/i_mppi.html",
    "title": "I-MPPI: Informative Model Predictive Path Integral",
    "section": "",
    "text": "TipQuick Start\n\n\n\nI-MPPI is a hierarchical framework combining information-theoretic planning with MPPI control for autonomous UAV exploration.\nCore Concept: Navigate to maximize information gain (reduce map uncertainty) while avoiding obstacles.\nTwo-Layer Architecture: - Layer 2 (FSMI): Generates informative reference trajectories (~5 Hz) - Layer 3 (Biased MPPI): Tracks trajectories with local information gathering (~50 Hz)\nKey Components: - Occupancy grid representing environment uncertainty - Fast Shannon Mutual Information (FSMI) for efficient information gain computation - Biased sampling integrating information potential into MPPI\nTry It: See examples/i_mppi/simulation.py or the interactive Colab notebook.",
    "crumbs": [
      "Home",
      "Algorithms",
      "I-MPPI: Informative Model Predictive Path Integral"
    ]
  },
  {
    "objectID": "src/i_mppi.html#optimal-control-duality-free-energy",
    "href": "src/i_mppi.html#optimal-control-duality-free-energy",
    "title": "I-MPPI: Informative Model Predictive Path Integral",
    "section": "Optimal Control Duality & Free Energy",
    "text": "Optimal Control Duality & Free Energy\nThe mathematical derivation of the MPPI algorithm is rooted in the definition of the free energy of the dynamical system. The value function \\(V(x, t)\\) of a stochastic system can be linearized through a logarithmic transformation, leading to the Path Integral formulation. The Free Energy (\\(\\mathcal{F}\\)) of the dynamical system is defined as:\n\\[ \\mathcal{F}(x_0) = -\\lambda \\log \\mathbb{E}_{\\mathbb{P}} \\left[ \\exp \\left( -\\frac{1}{\\lambda} S(\\tau) \\right) \\right] \\]\n\n\\(\\tau\\): The state-control trajectory \\(\\{x_0, u_0, x_1, u_1, \\dots, x_T\\}\\).\n\\(\\mathbb{P}\\): The base distribution, representing the stochastic trajectories of the ‚Äúpassive‚Äù system.\n\\(S(\\tau)\\): The cumulative cost (Action) of a trajectory \\(\\tau\\).\n\\(\\lambda\\): The temperature parameter, representing the noise variance.\n\nThe ‚Äúoptimal trajectory‚Äù is the mean of the distribution \\(\\mathbb{Q}^*\\) that minimizes the KL-divergence to the distribution of ‚Äúlow-cost‚Äù paths, leading to the thermodynamic weight update rule:\n\\[ \\omega^k = \\frac{\\exp\\left(-\\frac{1}{\\lambda}(J^k - \\rho)\\right)}{\\sum_{j=1}^{K} \\exp\\left(-\\frac{1}{\\lambda}(J^j - \\rho)\\right)} \\]",
    "crumbs": [
      "Home",
      "Algorithms",
      "I-MPPI: Informative Model Predictive Path Integral"
    ]
  },
  {
    "objectID": "src/i_mppi.html#representation",
    "href": "src/i_mppi.html#representation",
    "title": "I-MPPI: Informative Model Predictive Path Integral",
    "section": "Representation",
    "text": "Representation\n\n\n\nCell value\nInterpretation\n\n\n\n\n\\(p = 0.0\\)\nKnown free\n\n\n\\(p = 0.5\\)\nUnknown (maximum entropy)\n\n\n\\(p = 1.0\\)\nKnown occupied (wall)",
    "crumbs": [
      "Home",
      "Algorithms",
      "I-MPPI: Informative Model Predictive Path Integral"
    ]
  },
  {
    "objectID": "src/i_mppi.html#coordinate-transforms",
    "href": "src/i_mppi.html#coordinate-transforms",
    "title": "I-MPPI: Informative Model Predictive Path Integral",
    "section": "Coordinate Transforms",
    "text": "Coordinate Transforms\nThe grid is anchored at a world-frame origin \\(\\mathbf{o} = [o_x, o_y]\\) with resolution \\(r\\) (meters per cell). Conversion between world coordinates \\(\\mathbf{p}_w\\) and grid indices \\(\\mathbf{p}_g\\):\n\\[\n\\mathbf{p}_g = \\frac{\\mathbf{p}_w - \\mathbf{o}}{r}, \\qquad\n\\mathbf{p}_w = \\mathbf{o} + (\\mathbf{p}_g + 0.5)\\,r\n\\]",
    "crumbs": [
      "Home",
      "Algorithms",
      "I-MPPI: Informative Model Predictive Path Integral"
    ]
  },
  {
    "objectID": "src/i_mppi.html#rasterization",
    "href": "src/i_mppi.html#rasterization",
    "title": "I-MPPI: Informative Model Predictive Path Integral",
    "section": "Rasterization",
    "text": "Rasterization\nThe grid is constructed by rasterizing the environment:\n\nWalls are line segments \\([x_1,y_1,x_2,y_2]\\). Each cell within 0.2 m of a wall segment is set to \\(p = 1.0\\).\nInformation zones are axis-aligned rectangles \\([c_x, c_y, w, h]\\). Cells inside a zone are set to \\(p = 0.5\\) (unknown).\nPriority: walls override zones; zones override free space.",
    "crumbs": [
      "Home",
      "Algorithms",
      "I-MPPI: Informative Model Predictive Path Integral"
    ]
  },
  {
    "objectID": "src/i_mppi.html#shannon-mutual-information",
    "href": "src/i_mppi.html#shannon-mutual-information",
    "title": "I-MPPI: Informative Model Predictive Path Integral",
    "section": "Shannon Mutual Information",
    "text": "Shannon Mutual Information\nThe informative reward is the Shannon Mutual Information (MI) between the map \\(M\\) and a future sensor measurement \\(Z\\): \\[ I(M; Z) = H(M) - H(M|Z) \\]\nwhere \\(H(M)\\) is the map entropy. High MI indicates regions of unknown space or uncertain areas.\nIn (Charrow et al. 2015), MI is approximated using the Cauchy-Swartz Quadratic Mutual Information (CSQMI).\n\\[\nI_{\\text{CS}}[m; z_\\tau] = -\\log \\frac{\\left(\\sum \\int p(m, z_\\tau) p(m) p(z_\\tau) \\, dz_\\tau\\right)^2}{\\sum \\int p^2(m, z_\\tau) \\, dz_\\tau \\sum \\int p^2(m) p^2(z_\\tau) \\, dz_\\tau}\n\\]\nwhere \\(p(m, z_\\tau)\\) is the joint distribution of map and measurement, and \\(p(m)\\), \\(p(z_\\tau)\\) are the marginals.\n\nCSQMI - Measurement Model\nA measurement at time \\(k\\) consists of \\(B\\) one-dimensional beams covering the sensor‚Äôs field of view.\n\n\n\n\n\n\nNote\n\n\n\nThis is for a 2D occupancy grid. The UAV is assumed to have a 3D field of view.\n\n\n\\[\nz_k = [z_{k1}, \\ldots, z_{kB}]\n\\]\nEach beam returns the distance to the first occupied cell, perturbed by Gaussian noise:\n\\[\np(z|d) = \\mathcal{N}(z-d, \\sigma^2)\n\\]\nFor a beam intersecting cells \\(c\\), the measurement distribution is:\n\\[\np(z_t^b | x_t) = \\sum_c p(c) \\, p(z_t^b | x_t, c)\n\\]\nThis creates a Gaussian mixture model where each cell along the ray contributes a Gaussian component weighted by its occupancy probability.",
    "crumbs": [
      "Home",
      "Algorithms",
      "I-MPPI: Informative Model Predictive Path Integral"
    ]
  },
  {
    "objectID": "src/i_mppi.html#fast-shannon-mutual-information-fsmi",
    "href": "src/i_mppi.html#fast-shannon-mutual-information-fsmi",
    "title": "I-MPPI: Informative Model Predictive Path Integral",
    "section": "Fast Shannon Mutual Information (FSMI)",
    "text": "Fast Shannon Mutual Information (FSMI)\nComputing Shannon MI analytically for a sensor beam involves casting rays through the occupancy grid and evaluating the expected information gain. The FSMI algorithm (Zhang et al. (2020)) provides a closed-form solution.\n\nBeam Termination Probability\nFor a ray passing through \\(n\\) cells with occupancy probabilities \\(o_1, \\dots, o_n\\), the probability that the beam terminates at cell \\(j\\) (i.e., the first occupied cell along the ray) is:\n\\[\nP(e_j) = o_j \\prod_{\\ell=1}^{j-1} (1 - o_\\ell)\n\\]\nThis is computed efficiently via a cumulative product of \\((1 - o_\\ell)\\).\n\n\nInformation Gain Function\nThe information gain from updating a cell with odds ratio \\(r = p/(1-p)\\) using an inverse sensor model with odds ratio \\(\\Delta\\) is:\n\\[\nf(r, \\Delta) = \\ln\\frac{r + 1}{r + 1/\\Delta} - \\frac{\\ln \\Delta}{r\\Delta + 1}\n\\]\nTwo instances are used:\n\nOccupied measurement: \\(\\Delta_{\\text{occ}} = \\exp(0.85)\\), giving \\(f_{\\text{occ}}(r)\\)\nEmpty measurement: \\(\\Delta_{\\text{emp}} = \\exp(-0.4)\\), giving \\(f_{\\text{emp}}(r)\\)\n\n\n\nInformation Potential\nThe information potential \\(C_k\\) represents the total information gained if a measurement falls at cell \\(k\\):\n\\[\nC_k = f_{\\text{occ}}(r_k) + \\sum_{i=1}^{k-1} f_{\\text{emp}}(r_i)\n\\]\nThe first term is the information from marking cell \\(k\\) as occupied; the summation accounts for all cells before \\(k\\) being marked as empty.\n\n\nSensor Noise Coupling\nThe geometry matrix \\(G_{kj}\\) couples the true termination cell \\(j\\) with the measured cell \\(k\\) through the sensor noise model:\n\\[\nG_{kj} = \\Phi\\!\\left(\\frac{l_{k+\\frac{1}{2}} - \\mu_j}{\\sigma}\\right) - \\Phi\\!\\left(\\frac{l_{k-\\frac{1}{2}} - \\mu_j}{\\sigma}\\right)\n\\]\nwhere \\(\\Phi\\) is the standard normal CDF, \\(l_{k \\pm \\frac{1}{2}}\\) are cell boundaries, \\(\\mu_j\\) is the distance to cell \\(j\\), and \\(\\sigma\\) is the sensor range noise standard deviation. A Gaussian truncation mask (default \\(3\\sigma\\)) zeroes out entries where \\(|k - j|\\) exceeds the truncation radius for computational efficiency.\n\n\nFull FSMI (Theorem 1)\nThe mutual information for a single beam is:\n\\[\nI_{\\text{FSMI}} = \\sum_{j=1}^{n} \\sum_{k=1}^{n} P(e_j) \\, C_k \\, G_{kj}\n\\]\nThis is \\(O(n^2)\\) per beam due to the double summation. In the implementation, the contraction is computed via jnp.einsum(\"j,k,kj-&gt;\", P_e, C_k, G_kj).\nThe total information for a viewpoint sums over all beams across the sensor FOV.\n\n\nUniform-FSMI (O(n) Approximation)\nFor short-range measurements where the sensor noise \\(\\sigma\\) is small relative to the cell size, the coupling matrix approaches the identity: \\(G_{kj} \\approx \\delta_{kj}\\). This yields the Uniform-FSMI approximation:\n\\[\nI_{\\text{Uniform}} \\approx \\sum_{j=1}^{n} P(e_j) \\, C_j\n\\]\nThis reduces complexity from \\(O(n^2)\\) to \\(O(n)\\) per beam, making it suitable for the Layer 3 reactive controller running at 50 Hz.",
    "crumbs": [
      "Home",
      "Algorithms",
      "I-MPPI: Informative Model Predictive Path Integral"
    ]
  },
  {
    "objectID": "src/i_mppi.html#csqmi-vs-fsmi-3d",
    "href": "src/i_mppi.html#csqmi-vs-fsmi-3d",
    "title": "I-MPPI: Informative Model Predictive Path Integral",
    "section": "CSQMI vs FSMI for 3D Field of View",
    "text": "CSQMI vs FSMI for 3D Field of View\nWhen the sensor provides a 3D field of view (e.g., RGB-D camera, 3D lidar), the number of beams \\(B\\) scales dramatically compared to a planar 2D scan:\n\n\n\nSensor\nBeams per scan\n\n\n\n\n2D lidar (Hokuyo)\n~1,080\n\n\n3D lidar (VLP-16)\n~28,800\n\n\n3D lidar (OS1-64)\n~131,072\n\n\nRGB-D camera (640√ó480)\n~307,200\n\n\n\nThis scaling from ~\\(10^3\\) to ~\\(10^5\\) beams fundamentally changes the computational trade-offs.\n\nComplexity Comparison\nLet \\(B\\) denote the number of beams, \\(n\\) the cells intersected per beam, and \\(n_r\\) the number of run-length encoded (RLE) segments per beam.\n\n\n\n\n\n\n\n\n\nVariant\nPer-beam\nTotal (3D)\nNotes\n\n\n\n\nCSQMI\n\\(O(n)\\)\n\\(O(Bn)\\)\nIndependence filtering via spatial hashing\n\n\nFSMI\n\\(O(n^2)\\)\n\\(O(Bn^2)\\)\nDominated by \\(G_{kj}\\) contraction\n\n\nFSMI-RLE\n\\(O(n_r^2)\\)\n\\(O(Bn_r^2)\\)\nExploits map sparsity\n\n\nApprox-FSMI\n\\(O(nD)\\)\n\\(O(BnD)\\)\n\\(D\\) = truncation radius\n\n\nUniform-FSMI\n\\(O(n)\\)\n\\(O(Bn)\\)\n\\(G_{kj} \\approx \\delta_{kj}\\)\n\n\n\n\n\nRun-Length Encoding (RLE) for Sparse Maps\nFSMI-RLE compresses consecutive cells with the same occupancy state into segments, exploiting the sparsity of 3D occupancy grids where most of space is empty. For a beam traversing \\(n = 50\\) cells that compress to \\(n_r = 5\\) RLE segments, the per-beam cost drops from \\(O(n^2) = 2{,}500\\) to \\(O(n_r^2) = 25\\) ‚Äî a 100√ó speedup.\nEmpirical compression ratios from 3D environments (Zhang2020-fsmi?):\n\n\n\nEnvironment\nCells/beam (\\(n\\))\nRLE segments (\\(n_r\\))\nCompression\n\n\n\n\nIndoor corridor\n45\n6\n7.5√ó\n\n\nOutdoor plaza\n80\n12\n6.7√ó\n\n\nCluttered room\n35\n8\n4.4√ó\n\n\n\nAverage compression of ~6√ó yields a ~36√ó speedup (\\(6^2\\) reduction in the double summation).\n\n\nMemory Footprint (3D scan, \\(B = 300\\text{K}\\))\n\n\n\nVariant\nPer-beam\nTotal\n\n\n\n\nCSQMI\n\\(O(n)\\)\n~60 MB\n\n\nFSMI\n\\(O(n^2)\\)\n~3 GB\n\n\nFSMI-RLE\n\\(O(n_r^2)\\)\n~30 MB\n\n\n\n\n\nGPU Parallelization\nBoth CSQMI and FSMI variants are embarrassingly parallel over beams: each beam‚Äôs information gain is independent, enabling direct vmap over \\(B\\) beams. CSQMI achieves slightly higher GPU utilization because its per-beam kernel is purely linear (\\(O(n)\\)), whereas FSMI-RLE still contains a small nested loop over \\(n_r^2\\) elements.\n\n\nRecommendations\nFor 2D occupancy grids (even with 3D sensors projected to a plane): CSQMI is preferred due to simpler implementation and linear per-beam cost.\nFor 3D occupancy grids (voxel/OctoMap representations): FSMI-RLE becomes competitive or superior, offering exact Shannon MI with lower memory and comparable speed on sparse maps.\n\n\n\n\n\n\nTipImplementation Strategy\n\n\n\nA two-stage approach can combine both: use CSQMI for fast trajectory ranking across many candidates, then refine the top-\\(k\\) with FSMI-RLE for more accurate final selection.",
    "crumbs": [
      "Home",
      "Algorithms",
      "I-MPPI: Informative Model Predictive Path Integral"
    ]
  },
  {
    "objectID": "src/i_mppi.html#trajectory-fsmi",
    "href": "src/i_mppi.html#trajectory-fsmi",
    "title": "I-MPPI: Informative Model Predictive Path Integral",
    "section": "Trajectory-Based FSMI",
    "text": "Trajectory-Based FSMI\nBoth FSMI and CSQMI can be extended from single-viewpoint evaluation to trajectory evaluation. The key is Assumption 2 from (Charrow et al. 2015):\n\nAssumption 2: Beam Independence Decomposition\n\\[\nI(M; Z_t | x_{1:t}, z_{1:t}) \\approx \\sum_{j=1}^{n_z} I(M_i; Z_t^j | x_{1:t}, z_{1:t})\n\\]\nTranslation: The MI between the map and all beams in a scan ‚âà sum of MI for each individual beam.\nBoth CSQMI and FSMI use this approximation! The difference:\n\nCSQMI: Explicitly filters dependent beams with isIndependent() spatial hashing\nFSMI: Implicitly assumes independence (no explicit filtering in base algorithm)\n\n\n\nMethod 1: Direct Summation (Simple, Fully Parallel)\nThe simplest trajectory extension:\n@jax.jit\ndef fsmi_trajectory_simple(trajectory_poses, occupancy_grid):\n    \"\"\"\n    Direct summation across trajectory\n    Assumption: Beams are approximately independent\n    \"\"\"\n    def fsmi_at_single_pose(pose):\n        beam_directions = get_beam_directions()  # (B, 3)\n\n        def fsmi_single_beam(beam_dir):\n            rle_beam = raycast_with_rle(beam_dir, pose, occupancy_grid)\n            return compute_fsmi_rle(rle_beam)  # O(n_r¬≤) per beam\n\n        return vmap(fsmi_single_beam)(beam_directions).sum()\n\n    # Fully parallel over all poses\n    info_per_pose = vmap(fsmi_at_single_pose)(trajectory_poses)\n    return jnp.sum(info_per_pose)\nComplexity: \\(O(T \\cdot B \\cdot n_r^2)\\) where \\(T\\) is trajectory length, \\(B\\) is beams per scan.\nAdvantages:\n\n‚úÖ Fully parallelizable (perfect for GPU)\n‚úÖ Simple implementation\n‚úÖ Fast with RLE compression\n\nLimitations:\n\n‚ö†Ô∏è May over-count information from overlapping views\n‚ö†Ô∏è No explicit handling of beam dependencies\n\n\n\nMethod 2: Conservative Parallel Filtering (GPU-Friendly)\nA parallelizable independence criterion:\n@jax.jit\ndef fsmi_trajectory_parallel_filtered(trajectory_poses, occupancy_grid):\n    \"\"\"\n    FSMI with parallelizable conservative independence filtering\n    \"\"\"\n    def compute_all_beams(pose):\n        beam_dirs = get_beam_directions()\n\n        def compute_beam_data(beam_dir):\n            rle_beam = raycast_with_rle(beam_dir, pose, occupancy_grid)\n            return {\n                'mi': compute_fsmi_rle(rle_beam),\n                'cell_mask': create_cell_mask(rle_beam, occupancy_grid),\n            }\n\n        return vmap(compute_beam_data)(beam_dirs)\n\n    # Parallel over all poses: (T, B, data)\n    all_beams = vmap(compute_all_beams)(trajectory_poses)\n\n    # Conservative independence mask (parallel)\n    flat_mi = all_beams['mi'].reshape(-1)\n    flat_masks = all_beams['cell_mask'].reshape(-1, num_cells)\n\n    # Mark beam as independent if it's the FIRST to hit each cell\n    independent_mask = compute_first_hit_mask(flat_masks)\n\n    return jnp.sum(flat_mi * independent_mask)\nAdvantages:\n\n‚úÖ Fully parallelizable\n‚úÖ Conservative (won‚Äôt over-count)\n‚úÖ Maintains GPU efficiency\n\nLimitations:\n\n‚ö†Ô∏è May be too conservative (under-count some valid independent beams)\n\n\n\nMethod 3: Discount Factor Approach (Novel)\nApply viewing-based discount without sequential bottleneck:\n@jax.jit\ndef fsmi_trajectory_with_discount(trajectory_poses, occupancy_grid):\n    \"\"\"\n    Apply discount factor based on viewing overlap\n    \"\"\"\n    def compute_pose_beams(pose_idx):\n        pose = trajectory_poses[pose_idx]\n        beam_dirs = get_beam_directions()\n\n        def compute_beam_with_discount(beam_dir):\n            rle_beam = raycast_with_rle(beam_dir, pose, occupancy_grid)\n            cells = rle_beam.cell_ids\n\n            # Base MI from FSMI\n            mi = compute_fsmi_rle(rle_beam)\n\n            # Discount based on previous views of these cells\n            total_previous_views = count_previous_views(\n                cells, pose_idx, trajectory_poses\n            )\n\n            # Exponential decay: 1st view = 1.0, 2nd = 0.5, 3rd = 0.25\n            discount = jnp.exp(-0.7 * total_previous_views)\n\n            return mi * jnp.mean(discount)\n\n        return vmap(compute_beam_with_discount)(beam_dirs).sum()\n\n    return vmap(compute_pose_beams)(jnp.arange(len(trajectory_poses))).sum()\nAdvantages:\n\n‚úÖ Fully parallelizable\n‚úÖ Gracefully handles overlaps\n‚úÖ Tunable discount parameter\n\nLimitations:\n\n‚ö†Ô∏è Not theoretically exact (empirical approximation)\n\n\n\nMethod Comparison\n\n\n\n\n\n\n\n\n\n\nMethod\nParallelization\nTheoretical Correctness\nComplexity\nBest For\n\n\n\n\nDirect Sum\n‚úÖ‚úÖ Perfect\n‚ö†Ô∏è May over-count\n\\(O(T \\cdot B \\cdot n_r^2)\\)\nFast prototyping\n\n\nConservative Filter\n‚úÖ‚úÖ Perfect\n‚úÖ Conservative\n\\(O(T \\cdot B \\cdot n_r^2)\\)\nGPU, large scale\n\n\nDiscount Factor\n‚úÖ‚úÖ Perfect\n‚ö†Ô∏è Approximate\n\\(O(T \\cdot B \\cdot n_r^2)\\)\nBalance speed/accuracy\n\n\nSequential Filter*\n‚ö†Ô∏è Bottleneck\n‚úÖ‚úÖ Exact\n\\(O(T \\cdot B \\cdot n_r^2)\\) + sequential\nAccuracy critical\n\n\n\n*Sequential filtering (CSQMI-style) omitted above due to GPU incompatibility.\n\n\nRecommended Implementation Strategy\nPhase 1: Prototype\n# Start with Method 1 (Direct Sum)\nmi_trajectory = fsmi_trajectory_simple(trajectory, map)\nPhase 2: Validate\n# Compare with conservative filtering\nmi_filtered = fsmi_trajectory_parallel_filtered(trajectory, map)\novercounting_ratio = mi_trajectory / mi_filtered\n\nif overcounting_ratio &lt; 1.2:\n    # Method 1 is sufficient\n    use_simple = True\nPhase 3: Optimize\n# If overcounting is significant, use discount factor\nmi_discounted = fsmi_trajectory_with_discount(trajectory, map)\n\n\n\n\n\n\nImportantFSMI vs CSQMI for Trajectories\n\n\n\nFSMI is inherently more parallelizable for trajectory evaluation because independence filtering is optional (approximation quality vs.¬†speed trade-off). CSQMI requires sequential filtering for correctness, creating a GPU bottleneck for trajectory-based planning.",
    "crumbs": [
      "Home",
      "Algorithms",
      "I-MPPI: Informative Model Predictive Path Integral"
    ]
  },
  {
    "objectID": "src/i_mppi.html#geometric-checks",
    "href": "src/i_mppi.html#geometric-checks",
    "title": "I-MPPI: Informative Model Predictive Path Integral",
    "section": "Geometric Checks",
    "text": "Geometric Checks\nFor each sample point in a zone, three conditions must hold:\n\nIn FOV: the bearing from the UAV to the point, relative to the UAV yaw, must be within \\(\\pm \\frac{\\theta_{\\text{FOV}}}{2}\\)\nIn range: the distance must be \\(\\leq r_{\\max}\\)\nLine-of-sight (optional): a ray from UAV to the point must not pass through any occupied cell (\\(p \\geq 0.7\\)) in the grid\n\nThe basic coverage without LOS is:\n\\[\nc_j = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{1}\\!\\left[|\\angle(\\mathbf{s}_i - \\mathbf{p}, \\psi)| \\leq \\tfrac{\\theta_{\\text{FOV}}}{2}\\right] \\cdot \\mathbb{1}\\!\\left[\\|\\mathbf{s}_i - \\mathbf{p}\\| \\leq r_{\\max}\\right]\n\\]\nwhere \\(\\mathbf{s}_i\\) are \\(N = 25\\) sample points uniformly distributed across the zone, \\(\\mathbf{p}\\) is the UAV position, and \\(\\psi\\) is yaw.",
    "crumbs": [
      "Home",
      "Algorithms",
      "I-MPPI: Informative Model Predictive Path Integral"
    ]
  },
  {
    "objectID": "src/i_mppi.html#ray-march-line-of-sight",
    "href": "src/i_mppi.html#ray-march-line-of-sight",
    "title": "I-MPPI: Informative Model Predictive Path Integral",
    "section": "Ray-March Line-of-Sight",
    "text": "Ray-March Line-of-Sight\nWhen LOS checking is enabled, the grid is sampled along a straight ray from the UAV to each sample point at fixed intervals (0.2 m). If any sample hits a cell with \\(p \\geq 0.7\\), the point is considered occluded:\n\\[\n\\text{LOS}(\\mathbf{p}, \\mathbf{s}) = \\prod_{l=0}^{L-1} \\mathbb{1}\\!\\left[G[\\mathbf{p} + l \\cdot \\Delta \\hat{\\mathbf{d}}] &lt; 0.7\\right]\n\\]",
    "crumbs": [
      "Home",
      "Algorithms",
      "I-MPPI: Informative Model Predictive Path Integral"
    ]
  },
  {
    "objectID": "src/i_mppi.html#entropy-weighted-coverage",
    "href": "src/i_mppi.html#entropy-weighted-coverage",
    "title": "I-MPPI: Informative Model Predictive Path Integral",
    "section": "Entropy-Weighted Coverage",
    "text": "Entropy-Weighted Coverage\nRaw FOV coverage treats all visible zone cells equally, meaning already-explored cells (\\(p \\approx 0.2\\), known free) contribute the same as unknown cells (\\(p = 0.5\\)). To create self-regulating depletion, each sample is weighted by an entropy proxy:\n\\[\nw(p) = 4\\,p\\,(1 - p)\n\\]\nThis function peaks at \\(w(0.5) = 1.0\\) (maximum uncertainty) and vanishes at \\(w(0) = w(1) = 0\\). The entropy-weighted coverage is:\n\\[\nc_j^{\\text{ew}} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{visible}(\\mathbf{s}_i) \\cdot \\text{LOS}(\\mathbf{p}, \\mathbf{s}_i) \\cdot w\\!\\left(G[\\mathbf{s}_i]\\right)\n\\]\nAs a zone is explored and its grid cells move from \\(p = 0.5\\) toward \\(p = 0.2\\), the entropy weight drops (\\(w(0.2) = 0.64 \\to 0\\)), naturally slowing depletion without requiring explicit thresholds.",
    "crumbs": [
      "Home",
      "Algorithms",
      "I-MPPI: Informative Model Predictive Path Integral"
    ]
  },
  {
    "objectID": "src/i_mppi.html#collision-cost",
    "href": "src/i_mppi.html#collision-cost",
    "title": "I-MPPI: Informative Model Predictive Path Integral",
    "section": "Collision Cost",
    "text": "Collision Cost\nProximity to wall segments incurs a large penalty:\n\\[\nJ_{\\text{collision}} = \\sum_{w \\in \\text{walls}} \\begin{cases} 1000 & \\text{if } \\mathbf{p} \\text{ within } r_{\\text{robot}} \\text{ of } w \\\\ 0 & \\text{otherwise} \\end{cases}\n\\]\nAdditionally, when a grid map is available, the cell at the UAV position is checked:\n\\[\nJ_{\\text{grid}} = \\begin{cases} 1000 & \\text{if } G[\\mathbf{p}] \\geq 0.7 \\\\ 0 & \\text{otherwise} \\end{cases}\n\\]",
    "crumbs": [
      "Home",
      "Algorithms",
      "I-MPPI: Informative Model Predictive Path Integral"
    ]
  },
  {
    "objectID": "src/i_mppi.html#information-cost-reward",
    "href": "src/i_mppi.html#information-cost-reward",
    "title": "I-MPPI: Informative Model Predictive Path Integral",
    "section": "Information Cost (Reward)",
    "text": "Information Cost (Reward)\nFor the basic I-MPPI cost, information reward is based on FOV coverage of each zone weighted by remaining info:\n\\[\nJ_{\\text{info}} = -50 \\sum_{j} \\tanh(i_j) \\cdot c_j\n\\]\nFor the Layer 3 Uniform-FSMI variant, information is computed directly from the grid:\n\\[\nJ_{\\text{info}} = -\\lambda_{\\text{info}} \\cdot I_{\\text{Uniform}}(\\mathbf{p}, \\psi)\n\\]",
    "crumbs": [
      "Home",
      "Algorithms",
      "I-MPPI: Informative Model Predictive Path Integral"
    ]
  },
  {
    "objectID": "src/i_mppi.html#target-attraction",
    "href": "src/i_mppi.html#target-attraction",
    "title": "I-MPPI: Informative Model Predictive Path Integral",
    "section": "Target Attraction",
    "text": "Target Attraction\n\\[\nJ_{\\text{target}} = w_{\\text{target}} \\|\\mathbf{p} - \\mathbf{p}_{\\text{ref}}(t)\\|\n\\]\nwhere \\(\\mathbf{p}_{\\text{ref}}(t)\\) is either a static goal or a point on the Layer 2 reference trajectory.",
    "crumbs": [
      "Home",
      "Algorithms",
      "I-MPPI: Informative Model Predictive Path Integral"
    ]
  },
  {
    "objectID": "src/i_mppi.html#bounds-and-height",
    "href": "src/i_mppi.html#bounds-and-height",
    "title": "I-MPPI: Informative Model Predictive Path Integral",
    "section": "Bounds and Height",
    "text": "Bounds and Height\n\\[\nJ_{\\text{bounds}} = 1000 \\cdot \\mathbb{1}[\\mathbf{p} \\notin \\text{workspace}], \\qquad\nJ_{\\text{height}} = w_h (p_z - p_z^*)^2\n\\]\nwhere \\(p_z^* = -2.0\\) m is the target altitude (NED convention: negative = up).",
    "crumbs": [
      "Home",
      "Algorithms",
      "I-MPPI: Informative Model Predictive Path Integral"
    ]
  },
  {
    "objectID": "src/i_mppi.html#control-regularization",
    "href": "src/i_mppi.html#control-regularization",
    "title": "I-MPPI: Informative Model Predictive Path Integral",
    "section": "Control Regularization",
    "text": "Control Regularization\n\\[\nJ_{\\text{control}} = \\lambda_u \\|\\mathbf{u}\\|^2\n\\]\nwith \\(\\lambda_u = 0.01\\).",
    "crumbs": [
      "Home",
      "Algorithms",
      "I-MPPI: Informative Model Predictive Path Integral"
    ]
  },
  {
    "objectID": "src/i_mppi.html#score-based-zone-selection",
    "href": "src/i_mppi.html#score-based-zone-selection",
    "title": "I-MPPI: Informative Model Predictive Path Integral",
    "section": "Score-Based Zone Selection",
    "text": "Score-Based Zone Selection\nEach information zone is scored by:\n\\[\n\\text{score}_j = i_j - w_d \\cdot \\|\\mathbf{p}_j - \\mathbf{p}_{\\text{uav}}\\|\n\\]\nwhere \\(i_j\\) is the remaining info level, \\(\\mathbf{p}_j\\) is the zone center, and \\(w_d\\) is a distance penalty weight.",
    "crumbs": [
      "Home",
      "Algorithms",
      "I-MPPI: Informative Model Predictive Path Integral"
    ]
  },
  {
    "objectID": "src/i_mppi.html#depletion-mask",
    "href": "src/i_mppi.html#depletion-mask",
    "title": "I-MPPI: Informative Model Predictive Path Integral",
    "section": "Depletion Mask",
    "text": "Depletion Mask\nZones with \\(i_j &lt; i_{\\text{threshold}}\\) are masked out (score set to \\(-\\infty\\)). The zone with the highest score is selected as the current target.",
    "crumbs": [
      "Home",
      "Algorithms",
      "I-MPPI: Informative Model Predictive Path Integral"
    ]
  },
  {
    "objectID": "src/i_mppi.html#goal-fallback",
    "href": "src/i_mppi.html#goal-fallback",
    "title": "I-MPPI: Informative Model Predictive Path Integral",
    "section": "Goal Fallback",
    "text": "Goal Fallback\nWhen no zone has remaining information above the threshold, the system falls back to the mission goal position \\(\\mathbf{p}_{\\text{goal}}\\), and the UAV proceeds directly to the goal.",
    "crumbs": [
      "Home",
      "Algorithms",
      "I-MPPI: Informative Model Predictive Path Integral"
    ]
  },
  {
    "objectID": "src/i_mppi.html#mixture-distribution",
    "href": "src/i_mppi.html#mixture-distribution",
    "title": "I-MPPI: Informative Model Predictive Path Integral",
    "section": "Mixture Distribution",
    "text": "Mixture Distribution\nThe \\(K\\) total samples are split into two groups:\n\\[\n\\mathbb{Q}_s = (1 - \\alpha)\\,\\mathcal{N}(\\bar{\\mathbf{u}}, \\Sigma) + \\alpha\\,\\mathcal{N}(\\bar{\\mathbf{u}} + \\delta_{\\text{ref}}, \\Sigma)\n\\]\nwhere:\n\n\\(K_{\\text{nom}} = \\lfloor(1-\\alpha) K\\rfloor\\) samples from the nominal distribution centered on the current control sequence \\(\\bar{\\mathbf{u}}\\)\n\\(K_{\\text{bias}} = K - K_{\\text{nom}}\\) samples biased toward the reference trajectory\n\nThe bias offset is:\n\\[\n\\delta_{\\text{ref}} = \\mathbf{U}_{\\text{ref}} - \\bar{\\mathbf{U}}\n\\]\nwhere \\(\\mathbf{U}_{\\text{ref}}\\) is the reference control sequence from Layer 2.",
    "crumbs": [
      "Home",
      "Algorithms",
      "I-MPPI: Informative Model Predictive Path Integral"
    ]
  },
  {
    "objectID": "src/i_mppi.html#mppi-variants",
    "href": "src/i_mppi.html#mppi-variants",
    "title": "I-MPPI: Informative Model Predictive Path Integral",
    "section": "MPPI Variants",
    "text": "MPPI Variants\nBiased sampling is implemented for all three MPPI variants:\n\nBiased MPPI: bias offset applied directly in control space\nBiased SMPPI: reference converted to velocity space via \\(\\delta_{\\text{ref}}^{vel} = (\\mathbf{U}_{\\text{ref}} - \\mathbf{a}_{\\text{seq}}) / \\Delta t\\)\nBiased KMPPI: reference projected to control-point space by solving \\(\\boldsymbol{\\theta}_{\\text{ref}} = (\\mathbf{K}^T\\mathbf{K})^{-1}\\mathbf{K}^T \\mathbf{U}_{\\text{ref}}\\), then bias offset \\(\\delta_{\\text{ref}} = \\boldsymbol{\\theta}_{\\text{ref}} - \\boldsymbol{\\theta}\\)",
    "crumbs": [
      "Home",
      "Algorithms",
      "I-MPPI: Informative Model Predictive Path Integral"
    ]
  },
  {
    "objectID": "src/i_mppi.html#architecture-overview",
    "href": "src/i_mppi.html#architecture-overview",
    "title": "I-MPPI: Informative Model Predictive Path Integral",
    "section": "Architecture Overview",
    "text": "Architecture Overview\n\n\n\n\n\ngraph TD\n    subgraph FSMI_Loop [\"FSMI Field Generator (5‚Äì10 Hz)\"]\n        Grid[Occupancy Grid] --&gt; VP[Coarse Viewpoint Grid]\n        VP --&gt; FSMI_Eval[\"vmap: FSMI(x, y, œà) over grid √ó yaws\"]\n        FSMI_Eval --&gt; MaxYaw[\"max over œà\"]\n        MaxYaw --&gt; Field[\"Information Potential Field ùìò(x, y)\"]\n    end\n\n    subgraph MPPI_Loop [\"MPPI Controller (50 Hz)\"]\n        State[\"UAV State x‚ÇÄ\"] --&gt; Rollouts[\"K Parallel Rollouts\"]\n        Rollouts --&gt; CostEval[\"Cost Evaluation\"]\n        CostEval --&gt; Weights[\"Importance Weights œâ_k\"]\n        Weights --&gt; Update[\"Weighted Average ‚Üí u*\"]\n    end\n\n    Field -.-&gt;|\"cached read\"| CostEval\n    State -.-&gt;|\"latest state\"| FSMI_Loop\n    Grid -.-&gt;|\"latest grid\"| MPPI_Loop\n    Update --&gt; Actuators[/Motor Commands/]\n\n    style FSMI_Loop fill:#e8f5e9,stroke:#1b5e20\n    style MPPI_Loop fill:#fff3e0,stroke:#e65100\n\n\n\n\n\n\nKey difference from sequential I-MPPI: there is no reference trajectory \\(\\tau_{\\text{ref}}\\) and no biased sampling. MPPI ‚Äúdiscovers‚Äù informative paths by following the spatial gradient of \\(\\mathcal{I}\\).",
    "crumbs": [
      "Home",
      "Algorithms",
      "I-MPPI: Informative Model Predictive Path Integral"
    ]
  },
  {
    "objectID": "src/i_mppi.html#information-potential-field",
    "href": "src/i_mppi.html#information-potential-field",
    "title": "I-MPPI: Informative Model Predictive Path Integral",
    "section": "Information Potential Field",
    "text": "Information Potential Field\nAt low rate (5‚Äì10 Hz), the FSMI field generator evaluates FSMI at a coarse grid of candidate viewpoints in the local workspace around the UAV.\n\nField Definition\nFor each grid position \\((x, y)\\), the information potential is the best achievable FSMI over all candidate yaw angles:\n\\[\n\\mathcal{I}(x, y) = \\max_{\\psi \\in \\Psi} \\; I_{\\text{FSMI}}(x, y, \\psi)\n\\]\nwhere \\(\\Psi = \\{0, \\frac{2\\pi}{N_\\psi}, \\dots, \\frac{2\\pi(N_\\psi - 1)}{N_\\psi}\\}\\) is a discrete set of yaw angles.\n\n\n\n\n\n\nTipYaw-dependent variant\n\n\n\nWhen the UAV‚Äôs sensor has a limited azimuthal FOV (e.g., forward-facing camera), the field can retain yaw dependence: \\(\\mathcal{I}(x, y, \\psi)\\). MPPI rollouts then query with their heading angle, adding yaw-steering incentive to the cost.\n\n\n\n\nField Computation Pseudocode\ndef compute_info_field(grid_map, uav_pos, field_res, field_extent, yaw_angles):\n    \"\"\"Compute information potential field over local workspace.\n\n    Args:\n        grid_map:     current occupancy grid (H, W)\n        uav_pos:      UAV position (x, y)\n        field_res:    grid spacing of the field [m]\n        field_extent: half-width of the local workspace [m]\n        yaw_angles:   (N_psi,) array of candidate yaw angles\n\n    Returns:\n        field: (Nx, Ny) information potential field\n        field_origin: (x0, y0) world coordinates of field[0, 0]\n    \"\"\"\n    # Build coarse grid of (x, y) positions centered on UAV\n    xs = jnp.arange(-field_extent, field_extent, field_res) + uav_pos[0]\n    ys = jnp.arange(-field_extent, field_extent, field_res) + uav_pos[1]\n    positions = jnp.stack(jnp.meshgrid(xs, ys, indexing=\"ij\"), axis=-1)  # (Nx, Ny, 2)\n\n    # Evaluate FSMI at every (position, yaw) pair\n    # vmap over positions (flattened), then vmap over yaw angles\n    flat_pos = positions.reshape(-1, 2)                # (Nx*Ny, 2)\n    fsmi_fn = lambda pos, psi: compute_fsmi(grid_map, pos, psi)\n    fsmi_all = vmap(vmap(fsmi_fn, in_axes=(None, 0)), in_axes=(0, None))\n    gains = fsmi_all(flat_pos, yaw_angles)             # (Nx*Ny, N_psi)\n\n    # Max over yaw ‚Üí information potential\n    field = gains.max(axis=-1).reshape(positions.shape[:2])  # (Nx, Ny)\n    field_origin = jnp.array([xs[0], ys[0]])\n    return field, field_origin\nComputational budget: For a \\(10 \\times 10\\) m workspace at 0.5 m resolution (\\(20 \\times 20 = 400\\) positions) with \\(N_\\psi = 8\\) yaw angles, the field requires 3,200 FSMI evaluations ‚Äî parallelized via vmap on GPU.",
    "crumbs": [
      "Home",
      "Algorithms",
      "I-MPPI: Informative Model Predictive Path Integral"
    ]
  },
  {
    "objectID": "src/i_mppi.html#mppi-cost-with-field-lookup",
    "href": "src/i_mppi.html#mppi-cost-with-field-lookup",
    "title": "I-MPPI: Informative Model Predictive Path Integral",
    "section": "MPPI Cost with Field Lookup",
    "text": "MPPI Cost with Field Lookup\nThe MPPI running cost incorporates the information field via bilinear interpolation at each rollout position:\n\\[\nJ_{\\text{info}}(\\mathbf{x}_t) = -\\lambda_{\\text{info}} \\cdot \\text{interp}_2\\!\\big(\\mathcal{I},\\; p_x(t),\\; p_y(t)\\big)\n\\]\nwhere \\(\\text{interp}_2\\) performs bilinear interpolation of the cached field \\(\\mathcal{I}\\) at the rollout position \\((p_x, p_y)\\).\n\nCombined Cost\nThe total per-timestep cost combines the field-based information reward with standard terms:\n\\[\nJ(\\mathbf{x}_t, \\mathbf{u}_t) = \\underbrace{J_{\\text{obs}}}_{\\text{obstacles}} + \\underbrace{J_{\\text{bounds}}}_{\\text{workspace}} + \\underbrace{J_{\\text{control}}}_{\\text{effort}} + \\underbrace{J_{\\text{info}}(\\mathbf{x}_t)}_{\\text{field lookup}} + \\underbrace{J_{\\text{local}}(\\mathbf{x}_t)}_{\\text{Uniform-FSMI}}\n\\]\nThe field lookup \\(J_{\\text{info}}\\) provides medium-range strategic guidance (which region to fly toward), while the Uniform-FSMI term \\(J_{\\text{local}}\\) provides immediate viewpoint-specific information (which way to look right now). Together they replace both the target attraction \\(J_{\\text{target}}\\) and the biased sampling of sequential I-MPPI.\n\n\n\n\n\n\nNoteWhy bilinear interpolation?\n\n\n\nThe field is computed on a coarse grid (0.5 m spacing), but MPPI rollouts have continuous positions. Bilinear interpolation is \\(O(1)\\) per query ‚Äî negligible compared to dynamics rollouts ‚Äî and provides smooth gradients that MPPI can exploit through its importance-weighted averaging.",
    "crumbs": [
      "Home",
      "Algorithms",
      "I-MPPI: Informative Model Predictive Path Integral"
    ]
  },
  {
    "objectID": "src/i_mppi.html#main-loop-pseudocode",
    "href": "src/i_mppi.html#main-loop-pseudocode",
    "title": "I-MPPI: Informative Model Predictive Path Integral",
    "section": "Main Loop Pseudocode",
    "text": "Main Loop Pseudocode\ndef parallel_imppi_loop(state, grid_map, mppi, fsmi_module, config):\n    \"\"\"Dual-rate parallel I-MPPI main loop.\n\n    MPPI runs every step (50 Hz).\n    FSMI field recomputes every N steps (5‚Äì10 Hz).\n    \"\"\"\n    field = None\n    field_origin = None\n\n    for step in range(config.max_steps):\n        # --- Low-rate: recompute information field ---\n        if step % config.field_update_interval == 0:\n            field, field_origin = compute_info_field(\n                grid_map=grid_map,\n                uav_pos=state.position[:2],\n                field_res=config.field_res,        # e.g., 0.5 m\n                field_extent=config.field_extent,   # e.g., 5.0 m\n                yaw_angles=config.yaw_angles,\n            )\n\n        # --- High-rate: MPPI with cached field ---\n        def cost_fn(x_traj):\n            \"\"\"Per-rollout cost using cached information field.\"\"\"\n            obs_cost = obstacle_cost(x_traj, grid_map)\n            ctrl_cost = control_cost(x_traj)\n            # Field lookup: bilinear interpolation at each (x, y)\n            info_cost = -config.lambda_info * interp2d(\n                field, field_origin, config.field_res, x_traj[:, :2]\n            )\n            # Local Uniform-FSMI for immediate viewpoint reward\n            local_cost = -config.lambda_local * uniform_fsmi(\n                grid_map, x_traj[-1, :2], x_traj[-1, 6]  # final yaw\n            )\n            return obs_cost + ctrl_cost + info_cost + local_cost\n\n        u_opt = mppi.step(state, cost_fn)\n\n        # --- Apply control and update state ---\n        state = dynamics(state, u_opt)\n        grid_map = update_grid_from_observation(grid_map, state)\n\nDual-Rate Scheduling\n\n\n\nParameter\nValue\nNotes\n\n\n\n\nMPPI rate\n50 Hz\nEvery control step\n\n\nField update rate\n5‚Äì10 Hz\nEvery 5‚Äì10 MPPI steps\n\n\nField resolution\n0.5 m\nCoarse enough for fast computation\n\n\nField extent\n5‚Äì10 m\nLocal workspace around UAV\n\n\nYaw discretization\n8 angles\n\\(45¬∞\\) increments\n\n\n\nThe field update is non-blocking in practice: on GPU, the field computation can overlap with the next MPPI step via asynchronous dispatch. The MPPI controller always reads the latest available field.",
    "crumbs": [
      "Home",
      "Algorithms",
      "I-MPPI: Informative Model Predictive Path Integral"
    ]
  },
  {
    "objectID": "src/i_mppi.html#sequential-vs-parallel-comparison",
    "href": "src/i_mppi.html#sequential-vs-parallel-comparison",
    "title": "I-MPPI: Informative Model Predictive Path Integral",
    "section": "Sequential vs Parallel Comparison",
    "text": "Sequential vs Parallel Comparison\n\n\n\n\n\n\n\n\nAspect\nSequential (current)\nParallel (proposed)\n\n\n\n\nArchitecture\nL2 plans \\(\\tau_{\\text{ref}}\\) ‚Üí L3 tracks it\nSingle MPPI + concurrent field\n\n\nInformation coupling\nReference trajectory encodes info\nSpatial potential field \\(\\mathcal{I}(x,y)\\)\n\n\nMPPI sampling\nBiased toward \\(\\tau_{\\text{ref}}\\)\nUnbiased; field shapes cost landscape\n\n\nLatency\nL3 waits for L2 replan (200 ms)\nField stale by \\(\\le\\) 1 update period\n\n\nPath discovery\nConstrained near reference\nMPPI explores full cost landscape\n\n\nCompute overlap\nSequential: L2 then L3\nConcurrent: field + MPPI on GPU\n\n\nFailure mode\nBad reference ‚Üí bad tracking\nStale field ‚Üí slightly suboptimal\n\n\nYaw planning\nImplicit in reference\nExplicit via field or \\(J_{\\text{local}}\\)\n\n\nImplementation\nRequires trajectory generator + biased sampling\nRequires field computation + interpolation",
    "crumbs": [
      "Home",
      "Algorithms",
      "I-MPPI: Informative Model Predictive Path Integral"
    ]
  },
  {
    "objectID": "src/i_mppi.html#relation-to-artificial-potential-fields",
    "href": "src/i_mppi.html#relation-to-artificial-potential-fields",
    "title": "I-MPPI: Informative Model Predictive Path Integral",
    "section": "Relation to Artificial Potential Fields",
    "text": "Relation to Artificial Potential Fields\nThe information potential field \\(\\mathcal{I}(x, y)\\) naturally evokes the Artificial Potential Field (APF) framework (Khatib1986?), where obstacles generate repulsive potentials and goals generate attractive potentials, guiding the robot via gradient descent. Parallel I-MPPI extends this paradigm in three key ways:\n\nInformation-driven attraction: Rather than a fixed goal point, the attractive potential \\(\\mathcal{I}(x, y)\\) represents the expected information gain from visiting \\((x, y)\\), computed via FSMI. The ‚Äúgoal‚Äù is emergent ‚Äî it is wherever the map is most uncertain.\nDynamic field evolution: Unlike static APFs, \\(\\mathcal{I}\\) is recomputed at 5‚Äì10 Hz as the occupancy grid updates, creating a time-varying potential landscape that naturally depletes as regions are explored. A visited region‚Äôs potential diminishes, preventing re-exploration.\nStochastic optimization over dynamics: Instead of instantaneous gradient descent (which is prone to local minima and ignores dynamics), MPPI performs trajectory optimization over the full UAV dynamics, escaping local minima through importance-weighted sampling while respecting control authority limits.\n\n\nClassical APF Limitations and I-MPPI Solutions\n\n\n\n\n\n\n\nClassical APF Limitation\nI-MPPI Solution\n\n\n\n\nLocal minima traps\nMPPI‚Äôs stochastic sampling escapes via exploration noise (\\(\\lambda\\))\n\n\nStatic potential field\n\\(\\mathcal{I}(x,y)\\) recomputed at 5‚Äì10 Hz as the map updates\n\n\nOscillations in narrow passages\nTrajectory-level optimization with smooth KMPPI basis functions\n\n\nNo dynamics consideration\nMPPI rollouts respect full 6-DOF quadrotor dynamics\n\n\nBinary goal attraction\nContinuous information potential that depletes with exploration\n\n\n\nThis formulation can be viewed as a thermodynamic APF, where the temperature parameter \\(\\lambda\\) controls exploration vs.¬†exploitation, and the free energy \\(\\mathcal{F}\\) naturally balances kinetic cost (control effort) with potential cost (information opportunity).\n\n\n\n\n\n\nTipIntuition for Robotics Readers\n\n\n\nIf you are familiar with APF-based planners, think of Parallel I-MPPI as replacing the fixed-goal attractive potential with a learned, dynamic information landscape ‚Äî and replacing gradient descent with stochastic trajectory sampling that respects full vehicle dynamics. The ‚ÄúSummary: Information as Energy‚Äù framing below makes this analogy precise through statistical mechanics.",
    "crumbs": [
      "Home",
      "Algorithms",
      "I-MPPI: Informative Model Predictive Path Integral"
    ]
  },
  {
    "objectID": "plan/index.html",
    "href": "plan/index.html",
    "title": "Development Plans",
    "section": "",
    "text": "This section contains various development plans and investigations for the jax-mppi project.\n\n\n\nParallel Trajectory Information Gain\nPerformance Analysis\nExample Performance Investigation\nFSMI Exploration\nI-MPPI Architecture Alignment\nVisualization Exploration (Issue #31)\n\n\n\n\n\nQuadrotor Trajectory Following\nEvosax Integration\nCUDA MPPI Implementation\nCUDA MPPI Submodule Plan\nPorting from PyTorch to JAX\nMove I-MPPI to src"
  },
  {
    "objectID": "plan/index.html#active-plans",
    "href": "plan/index.html#active-plans",
    "title": "Development Plans",
    "section": "",
    "text": "Parallel Trajectory Information Gain\nPerformance Analysis\nExample Performance Investigation\nFSMI Exploration\nI-MPPI Architecture Alignment\nVisualization Exploration (Issue #31)"
  },
  {
    "objectID": "plan/index.html#completed-plans",
    "href": "plan/index.html#completed-plans",
    "title": "Development Plans",
    "section": "",
    "text": "Quadrotor Trajectory Following\nEvosax Integration\nCUDA MPPI Implementation\nCUDA MPPI Submodule Plan\nPorting from PyTorch to JAX\nMove I-MPPI to src"
  },
  {
    "objectID": "plan/parallel_trajectory_ig.html",
    "href": "plan/parallel_trajectory_ig.html",
    "title": "Plan: Parallel Trajectory Information Gain & Parallel I-MPPI",
    "section": "",
    "text": "Branch: feat/parallel-traj-ig Reference: docs/src/i_mppi.qmd ‚Äî ¬ßTrajectory-Based FSMI, ¬ßParallel I-MPPI Architecture\n\n\nTwo related improvements to the I-MPPI architecture:\nPart A ‚Äî Trajectory-Level FSMI (improve Layer 2 trajectory evaluation): Replace the entropy-proxy compute_fsmi_gain() in _info_gain_grid() with true FSMI (Theorem 1), and add overlap-aware methods for dense trajectories.\nPart B ‚Äî Parallel I-MPPI (new architecture replacing sequential L2‚ÜíL3): Eliminate the reference trajectory. A single MPPI runs at 50 Hz and consults a precomputed information potential field \\(\\mathcal{I}(x,y)\\) updated concurrently at 5‚Äì10 Hz.\n\n\n\n\n\n\n_info_gain_grid() in fsmi.py evaluates trajectory-level information by:\n\nSubsampling the reference trajectory (every trajectory_subsample_rate=5 steps)\nVmapping compute_fsmi_gain() over sampled waypoints ‚Äî 360¬∞ rays, entropy proxy 4*p*(1-p)\nSumming scalar gains\n\nLimitations: - Uses entropy proxy, not true FSMI (Theorem 1) - No beam overlap handling between consecutive viewpoints - 360¬∞ rays ignore directional FOV\n\n\n\nFile: src/jax_mppi/i_mppi/fsmi.py\n\nAdd compute_fsmi_batch(grid_map, positions, yaws) to FSMIModule\nSignature: (jax.Array [H,W], jax.Array [N,2], jax.Array [N]) -&gt; jax.Array [N]\nImplementation: jax.vmap(self.compute_fsmi, in_axes=(None, 0, 0))(grid_map, positions, yaws)\nValidate: output matches N individual compute_fsmi calls\n\n\n\n\nFile: src/jax_mppi/i_mppi/fsmi.py ‚Äî update _info_gain_grid()\n\nReplace compute_fsmi_gain calls with FSMIModule.compute_fsmi_batch()\nCompute per-pose yaw from consecutive trajectory points: atan2(dy, dx)\nSum per-pose MI values (scaled by dt * subsample_rate)\nThis is the simplest upgrade: true FSMI instead of entropy proxy, same summation\n\n\n\n\nFile: src/jax_mppi/i_mppi/fsmi.py ‚Äî new function\n\nAdd _fsmi_trajectory_discounted(ref_traj, grid_map, fsmi_module, dt, decay=0.7)\nFor each sampled pose, compute a dense cell mask (N_poses, H, W) ‚Äî which cells are within FOV+range\nCompute previous_views[i, h, w] = number of earlier poses that see cell (h,w): cumsum(masks, axis=0) - masks\nPer-pose discount: weight[i] = exp(-decay * mean_over_cells(previous_views[i] * mask[i]))\nFinal MI: sum(mi[i] * weight[i])\nFully parallel via vmap for mask computation\n\n\n\n\nFile: src/jax_mppi/i_mppi/fsmi.py ‚Äî new function\n\nAdd _fsmi_trajectory_filtered(ref_traj, grid_map, fsmi_module, dt)\nCompute per-pose cell masks (N_poses, H, W) (same as A3)\nFirst-hit mask: first_hit[i] = (argmin_over_poses(any_mask, axis=0) == i) ‚Äî cell belongs to the first pose that sees it\nindependent_fraction[i] = sum(first_hit[i] & mask[i]) / sum(mask[i])\nScale: mi_filtered[i] = mi[i] * independent_fraction[i]\n\n\n\n\nFile: src/jax_mppi/i_mppi/fsmi.py\n\nAdd to FSMIConfig:\n\ntrajectory_ig_method: str = \"direct\" (options: \"direct\", \"discount\", \"filtered\")\ntrajectory_ig_decay: float = 0.7\n\nWire _info_gain_grid() to dispatch based on config.trajectory_ig_method\n\n\n\n\nDense boolean mask (N_poses, H, W): - For 10 subsampled poses on a 200√ó200 grid ‚Üí 400K bools ‚âà 0.4 MB. Acceptable. - First-hit via jnp.argmin along pose axis is fully parallel. - Sparse alternatives are harder in JAX and unnecessary at this grid size.\n\n\n\n\n\n\n\nSequential architecture:\nLayer 2 (5 Hz) ‚Üí reference trajectory œÑ_ref\nLayer 3 (50 Hz) ‚Üí biased_mppi_command(U_ref=œÑ_ref) + Uniform-FSMI\nProblems: - Layer 3 cannot act on new information until next L2 replan (200 ms latency) - Biased sampling anchored to a single reference limits path discovery - Two separate optimization problems that may conflict\n\n\n\nFSMI Field Generator (5‚Äì10 Hz) ‚Üí I(x,y) potential field (cached)\nSingle MPPI (50 Hz) ‚Üí cost = obstacles + bounds + field_lookup + local_uniform_fsmi\nNo reference trajectory. No biased sampling. MPPI discovers informative paths by following the spatial gradient of the information potential field.\n\n\n\nFile: src/jax_mppi/i_mppi/map.py\n\nAdd interp2d(field, field_origin, field_res, query_points) -&gt; jax.Array\n\nfield: (Nx, Ny) float array\nfield_origin: (2,) world coords of field[0,0]\nfield_res: scalar, meters per cell\nquery_points: (M, 2) world coordinates\nReturns: (M,) interpolated values\nImplementation:\ndef interp2d(field, origin, res, points):\n    # Convert to continuous grid indices\n    gx = (points[:, 0] - origin[0]) / res\n    gy = (points[:, 1] - origin[1]) / res\n    # Floor indices\n    ix = jnp.floor(gx).astype(jnp.int32)\n    iy = jnp.floor(gy).astype(jnp.int32)\n    # Fractional parts\n    fx = gx - ix\n    fy = gy - iy\n    # Clamp\n    Nx, Ny = field.shape\n    ix0 = jnp.clip(ix, 0, Nx - 2)\n    iy0 = jnp.clip(iy, 0, Ny - 2)\n    # Bilinear\n    v00 = field[ix0, iy0]\n    v10 = field[ix0 + 1, iy0]\n    v01 = field[ix0, iy0 + 1]\n    v11 = field[ix0 + 1, iy0 + 1]\n    return (v00 * (1-fx)*(1-fy) + v10 * fx*(1-fy)\n          + v01 * (1-fx)*fy + v11 * fx*fy)\nOut-of-bounds queries return 0.0 (no information outside field)\nMust be JIT-compatible and vmap-friendly\n\n\n\n\n\nFile: src/jax_mppi/i_mppi/fsmi.py ‚Äî new class or function\n\nAdd compute_info_field(fsmi_module, grid_map, uav_pos, config) -&gt; (field, field_origin)\nParameters (add to new InfoFieldConfig or extend FSMIConfig):\n\nfield_res: float = 0.5 ‚Äî meters per field cell\nfield_extent: float = 5.0 ‚Äî half-width of local workspace\nn_yaw: int = 8 ‚Äî number of candidate yaw angles\n\nAlgorithm:\n\nBuild coarse grid of (x, y) positions centered on UAV:\nxs = jnp.arange(-field_extent, field_extent, field_res) + uav_pos[0]\nys = jnp.arange(-field_extent, field_extent, field_res) + uav_pos[1]\npositions = jnp.stack(jnp.meshgrid(xs, ys, indexing=\"ij\"), axis=-1)  # (Nx, Ny, 2)\nDefine yaw angles: psis = jnp.linspace(0, 2*pi, n_yaw, endpoint=False)\nEvaluate FSMI at every (position, yaw) pair via double vmap:\nflat_pos = positions.reshape(-1, 2)  # (Nx*Ny, 2)\nfsmi_fn = lambda pos, psi: fsmi_module.compute_fsmi(grid_map, pos, psi)\ngains = vmap(vmap(fsmi_fn, in_axes=(None, 0)), in_axes=(0, None))(flat_pos, psis)\n# gains: (Nx*Ny, n_yaw)\nMax over yaw: field = gains.max(axis=-1).reshape(Nx, Ny)\nReturn (field, jnp.array([xs[0], ys[0]]))\n\nComputational budget:\n\n10√ó10 m at 0.5 m ‚Üí 20√ó20 = 400 positions √ó 8 yaws = 3,200 FSMI evals\nEach FSMI eval: 16 beams √ó ~50 ray steps ‚Üí ~25K flops\nTotal: ~80M flops ‚Äî well within GPU budget at 5‚Äì10 Hz\n\nStatic shape consideration:\n\nfield_extent and field_res determine array shapes ‚Üí use static_argnames or precompute grid positions outside JIT and pass as argument\n\n\n\n\n\nFile: src/jax_mppi/i_mppi/environment.py ‚Äî new cost function\n\nAdd parallel_imppi_running_cost(state, action, t, *, grid_map, grid_origin, grid_resolution, info_field, field_origin, field_res, uniform_fsmi_fn, lambda_info, lambda_local)\nCost terms:\nJ = J_collision + J_grid + J_bounds + J_height + J_control\n  + J_field(x,y)           ‚Üê NEW: medium-range strategic guidance\n  + J_local(x,y,yaw)       ‚Üê existing: Uniform-FSMI for immediate viewpoint\n\nJ_field = -lambda_info * interp2d(info_field, field_origin, field_res, pos_xy)\nJ_local = -lambda_local * uniform_fsmi_fn(grid_map, pos_xy, yaw)\nDrop J_target (no reference trajectory to track)\nKeep all safety costs (collision, bounds, height, grid obstacle)\n\nKey difference from informative_running_cost:\n\nNo target parameter\ninfo_field + field_origin + field_res replace reference trajectory tracking\nlambda_info and lambda_local are separate weights (tunable)\n\n\n\n\n\nFile: docs/examples/sim_utils.py ‚Äî new function build_parallel_sim_fn()\n\nImplement dual-rate main loop:\ndef build_parallel_sim_fn(config, fsmi_module, uniform_fsmi, grid_map_obj, ...):\n    # Precompute field grid positions (static shapes)\n    field_xs = jnp.arange(-field_extent, field_extent, field_res)\n    field_ys = jnp.arange(-field_extent, field_extent, field_res)\n    Nx, Ny = len(field_xs), len(field_ys)\n\n    def step_fn(carry, t):\n        state, ctrl_state, info_field, field_origin, grid, done_step = carry\n\n        # --- Low-rate: recompute field every N steps ---\n        recompute = (t % field_update_interval == 0)\n        uav_pos = state[:3]\n        new_field, new_origin = jax.lax.cond(\n            recompute,\n            lambda: compute_info_field(fsmi_module, grid, uav_pos, ...),\n            lambda: (info_field, field_origin),\n        )\n\n        # --- High-rate: standard MPPI (no bias) with field cost ---\n        def cost_fn(x, u, t_step):\n            return parallel_imppi_running_cost(\n                x, u, t_step,\n                grid_map=grid, info_field=new_field,\n                field_origin=new_origin, ...\n            )\n\n        action, new_ctrl = mppi_command(config, ctrl_state, state, dynamics, cost_fn)\n\n        # --- Apply control, update state & grid ---\n        new_state = dynamics(state, action)\n        new_grid = _update_grid_from_info(initial_grid, zone_masks, new_state[13:])\n\n        return (new_state, new_ctrl, new_field, new_origin, new_grid, done_step), (new_state,)\n\n    return jax.lax.scan(step_fn, init_carry, jnp.arange(sim_steps))\nNote on jax.lax.cond for field update: Both branches must have the same output shape. The false branch returns the cached field unchanged. Since field dimensions are fixed (determined by field_extent/field_res), this is safe.\nAlternative: always recompute but mask. If lax.cond causes tracing issues with the FSMI vmap, always compute the field but use jnp.where(recompute, new_field, old_field). This wastes compute but avoids shape ambiguity.\n\n\n\n\nFile: src/jax_mppi/i_mppi/fsmi.py\n\nAdd InfoFieldConfig dataclass:\n@dataclass\nclass InfoFieldConfig:\n    field_res: float = 0.5           # meters per field cell\n    field_extent: float = 5.0        # half-width of local workspace [m]\n    n_yaw: int = 8                   # candidate yaw angles\n    field_update_interval: int = 10  # MPPI steps between field updates\n    lambda_info: float = 10.0        # field lookup weight\n    lambda_local: float = 5.0        # Uniform-FSMI weight\n\n\n\n\nThe Parallel I-MPPI architecture removes explicit target selection. However, the field naturally provides directional guidance:\n\nUnknown zones have high FSMI ‚Üí high field values ‚Üí MPPI steers toward them\nDepleted zones have low FSMI ‚Üí low field values ‚Üí MPPI ignores them\nWhen all zones are explored, field is ~0 everywhere ‚Üí need goal fallback\nAdd goal attraction as a fallback cost term when max field value &lt; threshold:\ngoal_active = jnp.max(info_field) &lt; min_field_threshold\nJ_goal = jnp.where(goal_active, w_goal * jnp.linalg.norm(pos - goal_pos), 0.0)\nOr: always include a small goal attraction that is dominated by info cost when zones remain:\nJ_goal = w_goal_base * dist_to_goal  # small constant pull toward goal\n\n\n\n\n\n\n\n\n\ntest_compute_fsmi_batch: batch output matches N individual calls\ntest_direct_summation: known grid with one unknown zone ‚Üí MI &gt; 0\ntest_discount_leq_direct: MI_discount &lt;= MI_direct for overlapping trajectory\ntest_filtered_leq_direct: MI_filtered &lt;= MI_direct\ntest_filtered_eq_direct_nonoverlapping: non-overlapping poses ‚Üí MI_filtered == MI_direct\ntest_all_methods_jit: all three methods compile and run under jax.jit\n\n\n\n\n\ntest_interp2d_corners: interpolation at grid points equals exact values\ntest_interp2d_midpoint: value at midpoint is average of corners\ntest_interp2d_oob: out-of-bounds returns 0.0\ntest_compute_info_field_shape: output shape matches expected (Nx, Ny)\ntest_info_field_unknown_zone: field peaks near unknown zone centers\ntest_info_field_explored: field values drop after zone depletion\n\n\n\n\n\nRun build_parallel_sim_fn() for ~200 steps on the standard 3-zone environment\nVerify: UAV moves toward zones (not random walk)\nVerify: info levels deplete during simulation\nVerify: field updates occur every field_update_interval steps\nCompare with sequential I-MPPI on same scenario (qualitative)\n\n\n\n\n\nWall-clock: 3 trajectory FSMI methods on 50-waypoint trajectory, 16 beams\nWall-clock: compute_info_field for 20√ó20 grid, 8 yaws (target: &lt; 50 ms on GPU)\nWall-clock: full parallel I-MPPI step including field lookup (target: &lt; 20 ms at 50 Hz)\n\n\n\n\n\n\n\n\n\nPriority\nStep\nDescription\nDepends On\n\n\n\n\n1\nA1\ncompute_fsmi_batch\n‚Äî\n\n\n2\nA2\nDirect summation (replace entropy proxy)\nA1\n\n\n3\nB1\ninterp2d utility\n‚Äî\n\n\n4\nB2\ncompute_info_field\nA1, B1\n\n\n5\nB3\nField-based cost function\nB1, B2\n\n\n6\nB4\nParallel I-MPPI sim loop\nB2, B3\n\n\n7\nB5\nConfiguration\nB2, B3\n\n\n8\nB6\nGoal fallback\nB3\n\n\n9\nA3\nDiscount factor method\nA1\n\n\n10\nA4\nConservative filtering\nA1\n\n\n11\nA5\nConfig & dispatch for Part A\nA2, A3, A4\n\n\n12\nC1‚ÄìC4\nTests & benchmarks\nall above\n\n\n\nRationale: B1‚ÄìB4 (Parallel I-MPPI) is the higher-value deliverable. A3/A4 (overlap methods) are refinements that can come later since direct summation is often sufficient when trajectory subsampling is coarse.\n\n\n\n\n\n\n\n\n\n\n\n\nScenario\nRecommendation\n\n\n\n\nSparse unknown zones, open space\nParallel I-MPPI (field) ‚Äî MPPI can freely explore\n\n\nDense obstacles, narrow corridors\nSequential I-MPPI (reference) ‚Äî biased sampling helps\n\n\nReal-time on GPU\nParallel ‚Äî fewer sequential dependencies\n\n\nCPU-only\nSequential ‚Äî field computation is expensive without GPU\n\n\n\nBoth architectures should remain available. The parallel variant is an alternative sim loop, not a replacement.\n\n\n\nThe field is stale by up to field_update_interval / mppi_rate seconds (e.g., 10/50 = 0.2 s). This is acceptable because: - FSMI values change slowly (grid updates are incremental) - The Uniform-FSMI local term provides immediate viewpoint feedback - MPPI‚Äôs stochastic sampling can adapt between field updates\n\n\n\nStart with yaw-independent (max over yaw). This is simpler and works well for omnidirectional or wide-FOV sensors. Yaw-dependent variant ((Nx, Ny, N_psi) field with 3D interpolation) can be added later if needed for narrow-FOV cameras.\n\n\n\n\n\n\nParameter\nValue\nGrid Size\nFSMI Evals\nNotes\n\n\n\n\n5m, 0.5m\ndefault\n20√ó20\n3,200\nGood for local planning\n\n\n10m, 1.0m\ncoarse\n20√ó20\n3,200\nSame cost, wider view\n\n\n10m, 0.5m\nfine\n40√ó40\n12,800\n4√ó cost, better resolution\n\n\n5m, 0.25m\ndense\n40√ó40\n12,800\nHigh-res local field\n\n\n\nThe field resolution need not match the occupancy grid resolution. A coarser field is fine because bilinear interpolation smooths the values."
  },
  {
    "objectID": "plan/parallel_trajectory_ig.html#overview",
    "href": "plan/parallel_trajectory_ig.html#overview",
    "title": "Plan: Parallel Trajectory Information Gain & Parallel I-MPPI",
    "section": "",
    "text": "Two related improvements to the I-MPPI architecture:\nPart A ‚Äî Trajectory-Level FSMI (improve Layer 2 trajectory evaluation): Replace the entropy-proxy compute_fsmi_gain() in _info_gain_grid() with true FSMI (Theorem 1), and add overlap-aware methods for dense trajectories.\nPart B ‚Äî Parallel I-MPPI (new architecture replacing sequential L2‚ÜíL3): Eliminate the reference trajectory. A single MPPI runs at 50 Hz and consults a precomputed information potential field \\(\\mathcal{I}(x,y)\\) updated concurrently at 5‚Äì10 Hz."
  },
  {
    "objectID": "plan/parallel_trajectory_ig.html#part-a-trajectory-level-fsmi",
    "href": "plan/parallel_trajectory_ig.html#part-a-trajectory-level-fsmi",
    "title": "Plan: Parallel Trajectory Information Gain & Parallel I-MPPI",
    "section": "",
    "text": "_info_gain_grid() in fsmi.py evaluates trajectory-level information by:\n\nSubsampling the reference trajectory (every trajectory_subsample_rate=5 steps)\nVmapping compute_fsmi_gain() over sampled waypoints ‚Äî 360¬∞ rays, entropy proxy 4*p*(1-p)\nSumming scalar gains\n\nLimitations: - Uses entropy proxy, not true FSMI (Theorem 1) - No beam overlap handling between consecutive viewpoints - 360¬∞ rays ignore directional FOV\n\n\n\nFile: src/jax_mppi/i_mppi/fsmi.py\n\nAdd compute_fsmi_batch(grid_map, positions, yaws) to FSMIModule\nSignature: (jax.Array [H,W], jax.Array [N,2], jax.Array [N]) -&gt; jax.Array [N]\nImplementation: jax.vmap(self.compute_fsmi, in_axes=(None, 0, 0))(grid_map, positions, yaws)\nValidate: output matches N individual compute_fsmi calls\n\n\n\n\nFile: src/jax_mppi/i_mppi/fsmi.py ‚Äî update _info_gain_grid()\n\nReplace compute_fsmi_gain calls with FSMIModule.compute_fsmi_batch()\nCompute per-pose yaw from consecutive trajectory points: atan2(dy, dx)\nSum per-pose MI values (scaled by dt * subsample_rate)\nThis is the simplest upgrade: true FSMI instead of entropy proxy, same summation\n\n\n\n\nFile: src/jax_mppi/i_mppi/fsmi.py ‚Äî new function\n\nAdd _fsmi_trajectory_discounted(ref_traj, grid_map, fsmi_module, dt, decay=0.7)\nFor each sampled pose, compute a dense cell mask (N_poses, H, W) ‚Äî which cells are within FOV+range\nCompute previous_views[i, h, w] = number of earlier poses that see cell (h,w): cumsum(masks, axis=0) - masks\nPer-pose discount: weight[i] = exp(-decay * mean_over_cells(previous_views[i] * mask[i]))\nFinal MI: sum(mi[i] * weight[i])\nFully parallel via vmap for mask computation\n\n\n\n\nFile: src/jax_mppi/i_mppi/fsmi.py ‚Äî new function\n\nAdd _fsmi_trajectory_filtered(ref_traj, grid_map, fsmi_module, dt)\nCompute per-pose cell masks (N_poses, H, W) (same as A3)\nFirst-hit mask: first_hit[i] = (argmin_over_poses(any_mask, axis=0) == i) ‚Äî cell belongs to the first pose that sees it\nindependent_fraction[i] = sum(first_hit[i] & mask[i]) / sum(mask[i])\nScale: mi_filtered[i] = mi[i] * independent_fraction[i]\n\n\n\n\nFile: src/jax_mppi/i_mppi/fsmi.py\n\nAdd to FSMIConfig:\n\ntrajectory_ig_method: str = \"direct\" (options: \"direct\", \"discount\", \"filtered\")\ntrajectory_ig_decay: float = 0.7\n\nWire _info_gain_grid() to dispatch based on config.trajectory_ig_method\n\n\n\n\nDense boolean mask (N_poses, H, W): - For 10 subsampled poses on a 200√ó200 grid ‚Üí 400K bools ‚âà 0.4 MB. Acceptable. - First-hit via jnp.argmin along pose axis is fully parallel. - Sparse alternatives are harder in JAX and unnecessary at this grid size."
  },
  {
    "objectID": "plan/parallel_trajectory_ig.html#part-b-parallel-i-mppi-architecture",
    "href": "plan/parallel_trajectory_ig.html#part-b-parallel-i-mppi-architecture",
    "title": "Plan: Parallel Trajectory Information Gain & Parallel I-MPPI",
    "section": "",
    "text": "Sequential architecture:\nLayer 2 (5 Hz) ‚Üí reference trajectory œÑ_ref\nLayer 3 (50 Hz) ‚Üí biased_mppi_command(U_ref=œÑ_ref) + Uniform-FSMI\nProblems: - Layer 3 cannot act on new information until next L2 replan (200 ms latency) - Biased sampling anchored to a single reference limits path discovery - Two separate optimization problems that may conflict\n\n\n\nFSMI Field Generator (5‚Äì10 Hz) ‚Üí I(x,y) potential field (cached)\nSingle MPPI (50 Hz) ‚Üí cost = obstacles + bounds + field_lookup + local_uniform_fsmi\nNo reference trajectory. No biased sampling. MPPI discovers informative paths by following the spatial gradient of the information potential field.\n\n\n\nFile: src/jax_mppi/i_mppi/map.py\n\nAdd interp2d(field, field_origin, field_res, query_points) -&gt; jax.Array\n\nfield: (Nx, Ny) float array\nfield_origin: (2,) world coords of field[0,0]\nfield_res: scalar, meters per cell\nquery_points: (M, 2) world coordinates\nReturns: (M,) interpolated values\nImplementation:\ndef interp2d(field, origin, res, points):\n    # Convert to continuous grid indices\n    gx = (points[:, 0] - origin[0]) / res\n    gy = (points[:, 1] - origin[1]) / res\n    # Floor indices\n    ix = jnp.floor(gx).astype(jnp.int32)\n    iy = jnp.floor(gy).astype(jnp.int32)\n    # Fractional parts\n    fx = gx - ix\n    fy = gy - iy\n    # Clamp\n    Nx, Ny = field.shape\n    ix0 = jnp.clip(ix, 0, Nx - 2)\n    iy0 = jnp.clip(iy, 0, Ny - 2)\n    # Bilinear\n    v00 = field[ix0, iy0]\n    v10 = field[ix0 + 1, iy0]\n    v01 = field[ix0, iy0 + 1]\n    v11 = field[ix0 + 1, iy0 + 1]\n    return (v00 * (1-fx)*(1-fy) + v10 * fx*(1-fy)\n          + v01 * (1-fx)*fy + v11 * fx*fy)\nOut-of-bounds queries return 0.0 (no information outside field)\nMust be JIT-compatible and vmap-friendly\n\n\n\n\n\nFile: src/jax_mppi/i_mppi/fsmi.py ‚Äî new class or function\n\nAdd compute_info_field(fsmi_module, grid_map, uav_pos, config) -&gt; (field, field_origin)\nParameters (add to new InfoFieldConfig or extend FSMIConfig):\n\nfield_res: float = 0.5 ‚Äî meters per field cell\nfield_extent: float = 5.0 ‚Äî half-width of local workspace\nn_yaw: int = 8 ‚Äî number of candidate yaw angles\n\nAlgorithm:\n\nBuild coarse grid of (x, y) positions centered on UAV:\nxs = jnp.arange(-field_extent, field_extent, field_res) + uav_pos[0]\nys = jnp.arange(-field_extent, field_extent, field_res) + uav_pos[1]\npositions = jnp.stack(jnp.meshgrid(xs, ys, indexing=\"ij\"), axis=-1)  # (Nx, Ny, 2)\nDefine yaw angles: psis = jnp.linspace(0, 2*pi, n_yaw, endpoint=False)\nEvaluate FSMI at every (position, yaw) pair via double vmap:\nflat_pos = positions.reshape(-1, 2)  # (Nx*Ny, 2)\nfsmi_fn = lambda pos, psi: fsmi_module.compute_fsmi(grid_map, pos, psi)\ngains = vmap(vmap(fsmi_fn, in_axes=(None, 0)), in_axes=(0, None))(flat_pos, psis)\n# gains: (Nx*Ny, n_yaw)\nMax over yaw: field = gains.max(axis=-1).reshape(Nx, Ny)\nReturn (field, jnp.array([xs[0], ys[0]]))\n\nComputational budget:\n\n10√ó10 m at 0.5 m ‚Üí 20√ó20 = 400 positions √ó 8 yaws = 3,200 FSMI evals\nEach FSMI eval: 16 beams √ó ~50 ray steps ‚Üí ~25K flops\nTotal: ~80M flops ‚Äî well within GPU budget at 5‚Äì10 Hz\n\nStatic shape consideration:\n\nfield_extent and field_res determine array shapes ‚Üí use static_argnames or precompute grid positions outside JIT and pass as argument\n\n\n\n\n\nFile: src/jax_mppi/i_mppi/environment.py ‚Äî new cost function\n\nAdd parallel_imppi_running_cost(state, action, t, *, grid_map, grid_origin, grid_resolution, info_field, field_origin, field_res, uniform_fsmi_fn, lambda_info, lambda_local)\nCost terms:\nJ = J_collision + J_grid + J_bounds + J_height + J_control\n  + J_field(x,y)           ‚Üê NEW: medium-range strategic guidance\n  + J_local(x,y,yaw)       ‚Üê existing: Uniform-FSMI for immediate viewpoint\n\nJ_field = -lambda_info * interp2d(info_field, field_origin, field_res, pos_xy)\nJ_local = -lambda_local * uniform_fsmi_fn(grid_map, pos_xy, yaw)\nDrop J_target (no reference trajectory to track)\nKeep all safety costs (collision, bounds, height, grid obstacle)\n\nKey difference from informative_running_cost:\n\nNo target parameter\ninfo_field + field_origin + field_res replace reference trajectory tracking\nlambda_info and lambda_local are separate weights (tunable)\n\n\n\n\n\nFile: docs/examples/sim_utils.py ‚Äî new function build_parallel_sim_fn()\n\nImplement dual-rate main loop:\ndef build_parallel_sim_fn(config, fsmi_module, uniform_fsmi, grid_map_obj, ...):\n    # Precompute field grid positions (static shapes)\n    field_xs = jnp.arange(-field_extent, field_extent, field_res)\n    field_ys = jnp.arange(-field_extent, field_extent, field_res)\n    Nx, Ny = len(field_xs), len(field_ys)\n\n    def step_fn(carry, t):\n        state, ctrl_state, info_field, field_origin, grid, done_step = carry\n\n        # --- Low-rate: recompute field every N steps ---\n        recompute = (t % field_update_interval == 0)\n        uav_pos = state[:3]\n        new_field, new_origin = jax.lax.cond(\n            recompute,\n            lambda: compute_info_field(fsmi_module, grid, uav_pos, ...),\n            lambda: (info_field, field_origin),\n        )\n\n        # --- High-rate: standard MPPI (no bias) with field cost ---\n        def cost_fn(x, u, t_step):\n            return parallel_imppi_running_cost(\n                x, u, t_step,\n                grid_map=grid, info_field=new_field,\n                field_origin=new_origin, ...\n            )\n\n        action, new_ctrl = mppi_command(config, ctrl_state, state, dynamics, cost_fn)\n\n        # --- Apply control, update state & grid ---\n        new_state = dynamics(state, action)\n        new_grid = _update_grid_from_info(initial_grid, zone_masks, new_state[13:])\n\n        return (new_state, new_ctrl, new_field, new_origin, new_grid, done_step), (new_state,)\n\n    return jax.lax.scan(step_fn, init_carry, jnp.arange(sim_steps))\nNote on jax.lax.cond for field update: Both branches must have the same output shape. The false branch returns the cached field unchanged. Since field dimensions are fixed (determined by field_extent/field_res), this is safe.\nAlternative: always recompute but mask. If lax.cond causes tracing issues with the FSMI vmap, always compute the field but use jnp.where(recompute, new_field, old_field). This wastes compute but avoids shape ambiguity.\n\n\n\n\nFile: src/jax_mppi/i_mppi/fsmi.py\n\nAdd InfoFieldConfig dataclass:\n@dataclass\nclass InfoFieldConfig:\n    field_res: float = 0.5           # meters per field cell\n    field_extent: float = 5.0        # half-width of local workspace [m]\n    n_yaw: int = 8                   # candidate yaw angles\n    field_update_interval: int = 10  # MPPI steps between field updates\n    lambda_info: float = 10.0        # field lookup weight\n    lambda_local: float = 5.0        # Uniform-FSMI weight\n\n\n\n\nThe Parallel I-MPPI architecture removes explicit target selection. However, the field naturally provides directional guidance:\n\nUnknown zones have high FSMI ‚Üí high field values ‚Üí MPPI steers toward them\nDepleted zones have low FSMI ‚Üí low field values ‚Üí MPPI ignores them\nWhen all zones are explored, field is ~0 everywhere ‚Üí need goal fallback\nAdd goal attraction as a fallback cost term when max field value &lt; threshold:\ngoal_active = jnp.max(info_field) &lt; min_field_threshold\nJ_goal = jnp.where(goal_active, w_goal * jnp.linalg.norm(pos - goal_pos), 0.0)\nOr: always include a small goal attraction that is dominated by info cost when zones remain:\nJ_goal = w_goal_base * dist_to_goal  # small constant pull toward goal"
  },
  {
    "objectID": "plan/parallel_trajectory_ig.html#part-c-tests-validation",
    "href": "plan/parallel_trajectory_ig.html#part-c-tests-validation",
    "title": "Plan: Parallel Trajectory Information Gain & Parallel I-MPPI",
    "section": "",
    "text": "test_compute_fsmi_batch: batch output matches N individual calls\ntest_direct_summation: known grid with one unknown zone ‚Üí MI &gt; 0\ntest_discount_leq_direct: MI_discount &lt;= MI_direct for overlapping trajectory\ntest_filtered_leq_direct: MI_filtered &lt;= MI_direct\ntest_filtered_eq_direct_nonoverlapping: non-overlapping poses ‚Üí MI_filtered == MI_direct\ntest_all_methods_jit: all three methods compile and run under jax.jit\n\n\n\n\n\ntest_interp2d_corners: interpolation at grid points equals exact values\ntest_interp2d_midpoint: value at midpoint is average of corners\ntest_interp2d_oob: out-of-bounds returns 0.0\ntest_compute_info_field_shape: output shape matches expected (Nx, Ny)\ntest_info_field_unknown_zone: field peaks near unknown zone centers\ntest_info_field_explored: field values drop after zone depletion\n\n\n\n\n\nRun build_parallel_sim_fn() for ~200 steps on the standard 3-zone environment\nVerify: UAV moves toward zones (not random walk)\nVerify: info levels deplete during simulation\nVerify: field updates occur every field_update_interval steps\nCompare with sequential I-MPPI on same scenario (qualitative)\n\n\n\n\n\nWall-clock: 3 trajectory FSMI methods on 50-waypoint trajectory, 16 beams\nWall-clock: compute_info_field for 20√ó20 grid, 8 yaws (target: &lt; 50 ms on GPU)\nWall-clock: full parallel I-MPPI step including field lookup (target: &lt; 20 ms at 50 Hz)"
  },
  {
    "objectID": "plan/parallel_trajectory_ig.html#implementation-order",
    "href": "plan/parallel_trajectory_ig.html#implementation-order",
    "title": "Plan: Parallel Trajectory Information Gain & Parallel I-MPPI",
    "section": "",
    "text": "Priority\nStep\nDescription\nDepends On\n\n\n\n\n1\nA1\ncompute_fsmi_batch\n‚Äî\n\n\n2\nA2\nDirect summation (replace entropy proxy)\nA1\n\n\n3\nB1\ninterp2d utility\n‚Äî\n\n\n4\nB2\ncompute_info_field\nA1, B1\n\n\n5\nB3\nField-based cost function\nB1, B2\n\n\n6\nB4\nParallel I-MPPI sim loop\nB2, B3\n\n\n7\nB5\nConfiguration\nB2, B3\n\n\n8\nB6\nGoal fallback\nB3\n\n\n9\nA3\nDiscount factor method\nA1\n\n\n10\nA4\nConservative filtering\nA1\n\n\n11\nA5\nConfig & dispatch for Part A\nA2, A3, A4\n\n\n12\nC1‚ÄìC4\nTests & benchmarks\nall above\n\n\n\nRationale: B1‚ÄìB4 (Parallel I-MPPI) is the higher-value deliverable. A3/A4 (overlap methods) are refinements that can come later since direct summation is often sufficient when trajectory subsampling is coarse."
  },
  {
    "objectID": "plan/parallel_trajectory_ig.html#design-decisions",
    "href": "plan/parallel_trajectory_ig.html#design-decisions",
    "title": "Plan: Parallel Trajectory Information Gain & Parallel I-MPPI",
    "section": "",
    "text": "Scenario\nRecommendation\n\n\n\n\nSparse unknown zones, open space\nParallel I-MPPI (field) ‚Äî MPPI can freely explore\n\n\nDense obstacles, narrow corridors\nSequential I-MPPI (reference) ‚Äî biased sampling helps\n\n\nReal-time on GPU\nParallel ‚Äî fewer sequential dependencies\n\n\nCPU-only\nSequential ‚Äî field computation is expensive without GPU\n\n\n\nBoth architectures should remain available. The parallel variant is an alternative sim loop, not a replacement.\n\n\n\nThe field is stale by up to field_update_interval / mppi_rate seconds (e.g., 10/50 = 0.2 s). This is acceptable because: - FSMI values change slowly (grid updates are incremental) - The Uniform-FSMI local term provides immediate viewpoint feedback - MPPI‚Äôs stochastic sampling can adapt between field updates\n\n\n\nStart with yaw-independent (max over yaw). This is simpler and works well for omnidirectional or wide-FOV sensors. Yaw-dependent variant ((Nx, Ny, N_psi) field with 3D interpolation) can be added later if needed for narrow-FOV cameras.\n\n\n\n\n\n\nParameter\nValue\nGrid Size\nFSMI Evals\nNotes\n\n\n\n\n5m, 0.5m\ndefault\n20√ó20\n3,200\nGood for local planning\n\n\n10m, 1.0m\ncoarse\n20√ó20\n3,200\nSame cost, wider view\n\n\n10m, 0.5m\nfine\n40√ó40\n12,800\n4√ó cost, better resolution\n\n\n5m, 0.25m\ndense\n40√ó40\n12,800\nHigh-res local field\n\n\n\nThe field resolution need not match the occupancy grid resolution. A coarser field is fine because bilinear interpolation smooths the values."
  },
  {
    "objectID": "plan/completed/porting_pytorch_jax.html",
    "href": "plan/completed/porting_pytorch_jax.html",
    "title": "JAX MPPI Implementation Plan",
    "section": "",
    "text": "Port pytorch_mppi to JAX, producing a functional, JIT-compilable MPPI library.\n\n\nOverall Progress: Phase 6 complete (Autotuning system fully implemented with CMA-ES, Ray Tune, and CMA-ME support).\n\n\n\nPhase 1: Core MPPI ‚úÖ COMPLETE\n\n353 lines implemented in src/jax_mppi/mppi.py\nAll core features from pytorch_mppi ported\n115 lines of unit tests in tests/test_mppi.py\n\nPhase 2: Pendulum Integration ‚úÖ COMPLETE\n\n270 lines in examples/pendulum.py (full-featured example with CLI)\n282 lines in tests/test_pendulum.py (8 comprehensive integration tests)\nAll tests passing, swing-up and stabilization verified\n\nPhase 3: Smooth MPPI (SMPPI) ‚úÖ COMPLETE\n\n634 lines implemented in src/jax_mppi/smppi.py\nAll SMPPI features: action_sequence, smoothness cost, dual bounds, integration\n580 lines in tests/test_smppi.py (18 comprehensive tests)\nAll tests passing\n\nPhase 4: Kernel MPPI (KMPPI) ‚úÖ COMPLETE\n\n660 lines implemented in src/jax_mppi/kmppi.py\nRBFKernel, kernel interpolation, control point optimization\n595 lines in tests/test_kmppi.py (23 comprehensive tests)\nAll tests passing (53/53 total tests pass)\n\nPhase 5: Smooth Comparison Example ‚úÖ COMPLETE\n\n442 lines in examples/smooth_comparison.py\nCompares MPPI, SMPPI, and KMPPI on 2D navigation with obstacle avoidance\nIncludes visualization with 4 subplots: trajectories, costs, controls, smoothness\nSupporting modules: src/jax_mppi/costs/ and src/jax_mppi/dynamics/\n\nPhase 6: Autotuning ‚úÖ COMPLETE\n\n656 lines in src/jax_mppi/autotune.py - Core CMA-ES autotuning\n375 lines in src/jax_mppi/autotune_global.py - Ray Tune global search\n218 lines in src/jax_mppi/autotune_qd.py - CMA-ME quality diversity\n305 lines in tests/test_autotune.py (21 unit tests)\n247 lines in tests/test_autotune_integration.py (4 integration tests)\n321 lines in examples/autotune_pendulum.py - Full demonstration\n90 lines in examples/autotune_basic.py - Minimal example\nAll 25 tests passing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPackage\nCore Code\nTests\nExamples\nTotal\n\n\n\n\npytorch_mppi\n1214 lines\n~500 lines\n~800 lines\n~2500 lines\n\n\njax_mppi (current)\n2919 lines\n2124 lines\n681 lines\n5724 lines\n\n\nCompletion %\n240%\n425%\n85%\n229%\n\n\n\nCore code now includes: mppi.py (353), smppi.py (634), kmppi.py (660), autotune.py (656), autotune_global.py (375), autotune_qd.py (218), plus supporting modules.\n\n\n\n\n\n\nFeature\npytorch_mppi\njax_mppi\nStatus\n\n\n\n\nCore MPPI Algorithm\n‚úì\n‚úì\n‚úÖ Complete\n\n\nBasic sampling & weighting\n‚úì\n‚úì\n‚úÖ\n\n\nControl bounds (u_min/u_max)\n‚úì\n‚úì\n‚úÖ\n\n\nControl scaling (u_scale)\n‚úì\n‚úì\n‚úÖ\n\n\nPartial updates (u_per_command)\n‚úì\n‚úì\n‚úÖ\n\n\nStep-dependent dynamics\n‚úì\n‚úì\n‚úÖ\n\n\nStochastic dynamics (rollout_samples)\n‚úì\n‚úì\n‚úÖ\n\n\nSample null action\n‚úì\n‚úì\n‚úÖ\n\n\nNoise absolute cost\n‚úì\n‚úì\n‚úÖ\n\n\nTerminal cost function\n‚úì\n‚úì\n‚úÖ\n\n\nShift nominal trajectory\n‚úì\n‚úì\n‚úÖ\n\n\nGet rollouts (visualization)\n‚úì\n‚úì\n‚úÖ\n\n\nReset controller\n‚úì\n‚úì\n‚úÖ\n\n\nSmooth MPPI (SMPPI)\n‚úì\n‚úì\n‚úÖ Complete\n\n\nAction sequence tracking\n‚úì\n‚úì\n‚úÖ\n\n\nSmoothness penalty\n‚úì\n‚úì\n‚úÖ\n\n\nSeparate action/control bounds\n‚úì\n‚úì\n‚úÖ\n\n\nDelta_t integration\n‚úì\n‚úì\n‚úÖ\n\n\nShift with continuity\n‚úì\n‚úì\n‚úÖ\n\n\nKernel MPPI (KMPPI)\n‚úì\n‚úì\n‚úÖ Complete\n\n\nKernel interpolation\n‚úì\n‚úì\n‚úÖ\n\n\nRBF kernel\n‚úì\n‚úì\n‚úÖ\n\n\nSupport point optimization\n‚úì\n‚úì\n‚úÖ\n\n\nTime grid management (Tk/Hs)\n‚úì\n‚úì\n‚úÖ\n\n\nSolve-based interpolation\n‚úì\n‚úì\n‚úÖ\n\n\nAutotuning\n‚úì\n‚úì\n‚úÖ Complete\n\n\nCMA-ES local tuning\n‚úì\n‚úì\n‚úÖ\n\n\nRay Tune global search\n‚úì\n‚úì\n‚úÖ\n\n\nCMA-ME quality diversity\n‚úì\n‚úì\n‚úÖ\n\n\nParameter types (lambda, sigma, mu, horizon)\n‚úì\n‚úì\n‚úÖ\n\n\nAll MPPI variants support\n‚úì\n‚úì\n‚úÖ\n\n\nExamples\n\n\n\n\n\nPendulum swing-up\n‚úì\n‚úì\n‚úÖ Complete\n\n\nSmooth MPPI comparison\n‚úì\n‚úì\n‚úÖ Complete\n\n\nAutotuning example\n‚úì\n‚úì\n‚úÖ Complete\n\n\nPendulum with learned dynamics\n‚úì\n‚úó\nüî¥ Not planned\n\n\n\n\n\n\njax_mppi/\n‚îú‚îÄ‚îÄ pyproject.toml              ‚úÖ Exists\n‚îú‚îÄ‚îÄ README.md                   ‚úÖ Exists\n‚îú‚îÄ‚îÄ LICENSE                     ‚úÖ Exists  \n‚îú‚îÄ‚îÄ src/jax_mppi/\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py            ‚úÖ Exists (updated for autotune)\n‚îÇ   ‚îú‚îÄ‚îÄ types.py               ‚úÖ Exists (9 lines)\n‚îÇ   ‚îú‚îÄ‚îÄ mppi.py                ‚úÖ Exists (353 lines) - COMPLETE\n‚îÇ   ‚îú‚îÄ‚îÄ smppi.py               ‚úÖ Exists (634 lines) - COMPLETE\n‚îÇ   ‚îú‚îÄ‚îÄ kmppi.py               ‚úÖ Exists (660 lines) - COMPLETE\n‚îÇ   ‚îú‚îÄ‚îÄ autotune.py            ‚úÖ Exists (656 lines) - COMPLETE\n‚îÇ   ‚îú‚îÄ‚îÄ autotune_global.py     ‚úÖ Exists (375 lines) - COMPLETE\n‚îÇ   ‚îú‚îÄ‚îÄ autotune_qd.py         ‚úÖ Exists (218 lines) - COMPLETE\n‚îÇ   ‚îú‚îÄ‚îÄ costs/                 ‚úÖ Exists (supporting modules)\n‚îÇ   ‚îî‚îÄ‚îÄ dynamics/              ‚úÖ Exists (supporting modules)\n‚îú‚îÄ‚îÄ tests/\n‚îÇ   ‚îú‚îÄ‚îÄ test_mppi.py           ‚úÖ Exists (115 lines) - COMPLETE\n‚îÇ   ‚îú‚îÄ‚îÄ test_pendulum.py       ‚úÖ Exists (282 lines) - COMPLETE\n‚îÇ   ‚îú‚îÄ‚îÄ test_smppi.py          ‚úÖ Exists (580 lines) - COMPLETE\n‚îÇ   ‚îú‚îÄ‚îÄ test_autotune.py       ‚úÖ Exists (305 lines, 21 tests) - COMPLETE\n‚îÇ   ‚îî‚îÄ‚îÄ test_autotune_integration.py ‚úÖ Exists (247 lines, 4 tests) - COMPLETE\n‚îÇ   ‚îî‚îÄ‚îÄ test_kmppi.py          ‚úÖ Exists (595 lines) - COMPLETE\n‚îú‚îÄ‚îÄ examples/\n‚îÇ   ‚îú‚îÄ‚îÄ pendulum.py            ‚úÖ Exists (270 lines) - COMPLETE\n‚îÇ   ‚îú‚îÄ‚îÄ smooth_comparison.py   ‚úÖ Exists (442 lines) - COMPLETE\n‚îÇ   ‚îú‚îÄ‚îÄ autotune_pendulum.py   ‚úÖ Exists (321 lines) - COMPLETE\n‚îÇ   ‚îî‚îÄ‚îÄ autotune_basic.py      ‚úÖ Exists (90 lines) - COMPLETE\n‚îî‚îÄ‚îÄ docs/\n    ‚îî‚îÄ‚îÄ plan/\n        ‚îî‚îÄ‚îÄ porting_pytorch_jax.md ‚úÖ This file\n\n\n\nPriority Order:\n\nPhase 3: SMPPI Implementation (High Priority)\n\nCore functionality that adds smoothness to control\nEstimated ~250-300 lines for smppi.py\nEstimated ~150-200 lines for tests\nReference: ../pytorch_mppi/src/pytorch_mppi/mppi.py (SMPPI class)\n\nPhase 4: KMPPI Implementation (High Priority)\n\nNovel contribution with kernel interpolation\nEstimated ~300-350 lines for kmppi.py\nEstimated ~150-200 lines for tests\nReference: ../pytorch_mppi/src/pytorch_mppi/mppi.py (KMPPI class)\n\nPhase 5: Smooth Comparison Example (Medium Priority)\n\nDemonstrates value of SMPPI and KMPPI\nEstimated ~200-250 lines\nReference: ../pytorch_mppi/tests/smooth_mppi.py\n\nAdditional Examples (Low Priority)\n\nPendulum with learned dynamics\nMore complex environments\n\nPhase 6: Autotuning (Optional/Stretch)\n\nAdvanced feature for hyperparameter optimization\nEstimated ~300-400 lines\nReference: ../pytorch_mppi/src/pytorch_mppi/autotune.py\n\n\n\n\n\n\n\n\nUse @jax.tree_util.register_dataclass (or flax.struct.dataclass) to hold MPPI state (nominal trajectory U, PRNG key, config). All core functions are pure: command(state, mppi_state) -&gt; (action, mppi_state).\nRationale: Idiomatic JAX ‚Äî pure functions compose with jit, vmap, grad. No mutable self. Avoids heavyweight dependencies like Equinox for what is fundamentally a numerical algorithm.\n\n\n\n\n\n\n\n\n\n\nPyTorch\nJAX\n\n\n\n\ntorch.distributions.MultivariateNormal\njax.random.multivariate_normal\n\n\ntensor.to(device)\njax.device_put / automatic\n\n\nPython for-loop over horizon\njax.lax.scan\n\n\n@handle_batch_input decorator\njax.vmap\n\n\ntorch.roll\njnp.roll\n\n\ntorch.linalg.solve\njnp.linalg.solve\n\n\nIn-place mutation (self.U = ...)\nReturn new state (pytree)\n\n\n\n\n\n\n\n\nActionable parity items to carry over:\n\nSMPPI semantics: maintains action_sequence separately from lifted control U; integrates with delta_t; smoothness cost from diff(action_sequence).\nSMPPI bounds: support action_min/action_max distinct from u_min/u_max (control-derivative bounds).\nKMPPI internals: keep theta as control points; build Tk/Hs time grids; kernel interpolation via solve(Ktktk, K); batch interpolation with vmap.\nSampling options: rollout_samples (M), sample_null_action, noise_abs_cost (abs(noise) in action cost).\nRollouts: get_rollouts handles state batch and dynamics that may augment state (take first nx).\n\n\n\n\n\njax_mppi/\n‚îú‚îÄ‚îÄ pyproject.toml\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ LICENSE\n‚îú‚îÄ‚îÄ src/jax_mppi/\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py          # Public API exports\n‚îÇ   ‚îú‚îÄ‚îÄ mppi.py              # Core MPPI (MPPIConfig, MPPIState, command, reset, etc.)\n‚îÇ   ‚îú‚îÄ‚îÄ smppi.py             # Smooth MPPI variant\n‚îÇ   ‚îú‚îÄ‚îÄ kmppi.py             # Kernel MPPI variant + TimeKernel / RBFKernel\n‚îÇ   ‚îú‚îÄ‚îÄ types.py             # Type aliases, protocols for Dynamics/Cost callables\n‚îÇ   ‚îî‚îÄ‚îÄ autotune.py          # Autotuning (CMA-ES wrapper, parameter search)\n‚îú‚îÄ‚îÄ tests/\n‚îÇ   ‚îú‚îÄ‚îÄ test_mppi.py         # Unit tests for core MPPI\n‚îÇ   ‚îú‚îÄ‚îÄ test_smppi.py        # Unit tests for SMPPI\n‚îÇ   ‚îú‚îÄ‚îÄ test_kmppi.py        # Unit tests for KMPPI\n‚îÇ   ‚îî‚îÄ‚îÄ test_pendulum.py     # Integration test with pendulum env\n‚îú‚îÄ‚îÄ examples/\n‚îÇ   ‚îú‚îÄ‚îÄ pendulum.py          # Gym pendulum with true dynamics\n‚îÇ   ‚îú‚îÄ‚îÄ pendulum_approximate.py  # Learned dynamics\n‚îÇ   ‚îî‚îÄ‚îÄ smooth_comparison.py # MPPI vs SMPPI vs KMPPI\n‚îî‚îÄ‚îÄ docs/\n    ‚îî‚îÄ‚îÄ plan/\n\n\n\n\n\n\nFiles: pyproject.toml, src/jax_mppi/types.py, src/jax_mppi/mppi.py, src/jax_mppi/__init__.py\n\npyproject.toml ‚Äî project metadata, deps: jax[cuda13], jaxlib, optional gymnasium for examples.\ntypes.py ‚Äî Type definitions:\n# Dynamics: (state, action) -&gt; next_state  or  (state, action, t) -&gt; next_state\nDynamicsFn = Callable[..., jax.Array]\n# Cost: (state, action) -&gt; scalar_cost  or  (state, action, t) -&gt; scalar_cost\nRunningCostFn = Callable[..., jax.Array]\n# Terminal: (states, actions) -&gt; scalar_cost\nTerminalCostFn = Callable[[jax.Array, jax.Array], jax.Array]\nmppi.py ‚Äî Core implementation:\nData structures (registered as JAX pytrees):\n@dataclass\nclass MPPIConfig:\n    # Static config (not traced through JAX)\n    num_samples: int       # K\n    horizon: int           # T\n    nx: int\n    nu: int\n    lambda_: float\n    u_scale: float\n    u_per_command: int\n    step_dependent_dynamics: bool\n    rollout_samples: int   # M\n    rollout_var_cost: float\n    rollout_var_discount: float\n    sample_null_action: bool\n    noise_abs_cost: bool\n\n@dataclass\nclass MPPIState:\n    # Dynamic state (carried through JAX transforms)\n    U: jax.Array           # (T, nu) nominal trajectory\n    u_init: jax.Array      # (nu,) default action for shift\n    noise_mu: jax.Array    # (nu,)\n    noise_sigma: jax.Array # (nu, nu)\n    noise_sigma_inv: jax.Array\n    u_min: jax.Array | None\n    u_max: jax.Array | None\n    key: jax.Array         # PRNG key\nFunctions:\ndef create(\n    nx, nu, noise_sigma, num_samples=100, horizon=15, lambda_=1.0,\n    noise_mu=None, u_min=None, u_max=None, u_init=None, U_init=None,\n    u_scale=1, u_per_command=1, step_dependent_dynamics=False,\n    rollout_samples=1, rollout_var_cost=0., rollout_var_discount=0.95,\n    sample_null_action=False, noise_abs_cost=False, key=None,\n) -&gt; tuple[MPPIConfig, MPPIState]:\n    \"\"\"Factory: create config + initial state.\"\"\"\n\ndef command(\n    config: MPPIConfig,\n    mppi_state: MPPIState,\n    current_obs: jax.Array,\n    dynamics: DynamicsFn,\n    running_cost: RunningCostFn,\n    terminal_cost: TerminalCostFn | None = None,\n    shift: bool = True,\n) -&gt; tuple[jax.Array, MPPIState]:\n    \"\"\"Compute optimal action and return updated state.\"\"\"\n\ndef reset(config: MPPIConfig, mppi_state: MPPIState, key: jax.Array) -&gt; MPPIState:\n    \"\"\"Reset nominal trajectory.\"\"\"\n\ndef get_rollouts(\n    config: MPPIConfig, mppi_state: MPPIState,\n    current_obs: jax.Array, dynamics: DynamicsFn,\n    num_rollouts: int = 1,\n) -&gt; jax.Array:\n    \"\"\"Forward-simulate trajectories for visualization.\"\"\"\nInternal functions (all JIT-compatible):\n\n_shift_nominal(mppi_state) -&gt; MPPIState ‚Äî jnp.roll + set last to u_init\n_sample_noise(key, K, T, noise_mu, noise_sigma) -&gt; (noise, new_key) ‚Äî sample from multivariate normal\n_compute_rollout_costs(config, current_obs, perturbed_actions, dynamics, running_cost, terminal_cost) ‚Äî uses jax.lax.scan over horizon, jax.vmap over K samples\n_compute_weights(costs, lambda_) ‚Äî softmax importance weighting\n_bound_action(action, u_min, u_max) ‚Äî jnp.clip\n\nKey JAX patterns:\n\nRollout loop: jax.lax.scan with carry = (state,), xs = actions[t]\nBatch over K samples: jax.vmap(_single_rollout, in_axes=(0, None, ...))\nBatch over M rollout samples (stochastic dynamics): nested vmap or scan\nAll internal functions decorated with @jax.jit or called inside a top-level jitted command\n\nUnit test: tests/test_mppi.py\n\nTest create() produces valid config/state\nTest command() returns correct shape\nTest cost reduction over iterations on simple 1D problem\nTest bounds are respected\n\n\n\n\n\nFiles: examples/pendulum.py, tests/test_pendulum.py\n\nImplement pendulum dynamics as a pure JAX function (no gym dependency for core test)\nRun MPPI loop, verify convergence (swing-up or stabilization)\nOptional: gym rendering wrapper for visualization\n\n\n\n\nFiles: src/jax_mppi/smppi.py, tests/test_smppi.py\n\nData structures:\n@dataclass\nclass SMPPIState(MPPIState):\n    action_sequence: jax.Array  # (T, nu) actual actions\n    w_action_seq_cost: float\n    delta_t: float\n    action_min: jax.Array | None\n    action_max: jax.Array | None\nFunctions: Same API as mppi.py but with:\n\n_shift_nominal shifts both U (velocity) and action_sequence\n_compute_perturbed_actions integrates velocity to get actions\n_compute_total_cost adds smoothness penalty: ||diff(actions)||^2\nreset() zeros both U and action_sequence\nchange_horizon() keeps both U and action_sequence in sync (truncate/extend)\n\nTest: Verify smoother trajectories than base MPPI on 2D navigation\n\n\n\n\nFiles: src/jax_mppi/kmppi.py, tests/test_kmppi.py\n\nKernel abstractions:\ndef rbf_kernel(t, tk, sigma=1.0):\n    d = jnp.sum((t[:, None] - tk) ** 2, axis=-1)\n    return jnp.exp(-d / (2 * sigma ** 2 + 1e-8))\n\ndef kernel_interpolate(t, tk, coeffs, kernel_fn):\n    K_t_tk = kernel_fn(t, tk)\n    K_tk_tk = kernel_fn(tk, tk)\n    weights = jnp.linalg.solve(K_tk_tk, K_t_tk.T).T\n    return weights @ coeffs\nData structures:\n@dataclass\nclass KMPPIState(MPPIState):\n    theta: jax.Array         # (num_support_pts, nu)\n    num_support_pts: int\nFunctions: Override _compute_perturbed_actions to sample sparse + interpolate. Update theta instead of U.\n\nBuild Tk and Hs time grids on init and on horizon changes\nUse kernel_interpolate() with solve(Ktktk, K) (avoid explicit inverse)\nBatch interpolate with jax.vmap for K samples\n\nTest: Verify fewer parameters produce smooth trajectories\n\n\n\n\nFiles: examples/smooth_comparison.py\n\nSide-by-side MPPI vs SMPPI vs KMPPI on 2D navigation\nPlot trajectories and control signals\n\n\n\n\nFiles: src/jax_mppi/autotune.py\n\nWrap CMA-ES (cmaes or evosax for JAX-native) for sigma/lambda/horizon tuning\nSimpler than pytorch_mppi‚Äôs framework ‚Äî skip Ray Tune and QD initially\nFunctional API: tune_step(eval_fn, params, optimizer_state) -&gt; (params, optimizer_state)\n\n\n\n\n\n\n\nUnit tests (per phase): pytest tests/ ‚Äî shape checks, cost reduction, bounds\nPendulum benchmark: Compare convergence (total reward) against pytorch_mppi on same scenario\nJIT correctness: Ensure jax.jit(command) produces identical results to non-jitted version\nPerformance: Benchmark command() latency vs pytorch_mppi (JAX should win after warmup due to XLA compilation)\nSmooth variants: Visual comparison of trajectory smoothness\n\n\n\nIMPORTANT: You should always use the virtual environment. To run the tests and all of the other python files.\n\nOption A: add a tests/conftest.py to insert src into sys.path.\nOption B: run tests after uv pip install -e . (editable install).\n\n\n\n\n\nCore: jax[cuda13], jaxlib, numpy Testing: pytest, gymnasium[classic_control] Autotuning (optional): cmaes or evosax Examples (optional): matplotlib, gymnasium\n\n\n\n\n\n\n\nMirror pytorch_mppi signature flags: rollout_samples, sample_null_action, noise_abs_cost.\nImplement get_rollouts handling: accept single or batched state; allow dynamics that augment state (take :nx).\nAdd shift_nominal_trajectory via jnp.roll + u_init fill.\nImplement action cost with optional abs(noise) branch.\nAdd u_per_command slicing and u_scale application in command.\n\n\n\n\n\nCarry action_sequence in state and integrate U with delta_t.\nImplement distinct action bounds (action_min/action_max) vs control bounds (u_min/u_max).\nAdd smoothness cost from diff(action_sequence) and weight w_action_seq_cost.\nEnsure reset() updates both U and action_sequence.\nImplement proper shift with action continuity (hold last value).\nImplement dual bounding system (_bound_control and _bound_action).\nRecompute effective noise after bounding for accurate cost.\n\n\n\n\n\nImplement theta control points + interpolation kernel (RBF by default).\nBuild Tk/Hs grids and re-build on horizon changes.\nUse solve(Ktktk, K) for interpolation weights (no explicit inverse).\nShift theta via interpolation when shifting nominal trajectory.\nImplement RBFKernel with configurable sigma.\nNoise sampling in control point space.\nBatched interpolation with vmap.\n\n\n\n\n\nMirror autotune interface from pytorch_mppi/autotune*.py at a minimal level (evaluation fn + optimizer loop).\nPort tests/auto_tune_parameters.py logic into a JAX-friendly example."
  },
  {
    "objectID": "plan/completed/porting_pytorch_jax.html#status-jan-31-2026",
    "href": "plan/completed/porting_pytorch_jax.html#status-jan-31-2026",
    "title": "JAX MPPI Implementation Plan",
    "section": "",
    "text": "Overall Progress: Phase 6 complete (Autotuning system fully implemented with CMA-ES, Ray Tune, and CMA-ME support).\n\n\n\nPhase 1: Core MPPI ‚úÖ COMPLETE\n\n353 lines implemented in src/jax_mppi/mppi.py\nAll core features from pytorch_mppi ported\n115 lines of unit tests in tests/test_mppi.py\n\nPhase 2: Pendulum Integration ‚úÖ COMPLETE\n\n270 lines in examples/pendulum.py (full-featured example with CLI)\n282 lines in tests/test_pendulum.py (8 comprehensive integration tests)\nAll tests passing, swing-up and stabilization verified\n\nPhase 3: Smooth MPPI (SMPPI) ‚úÖ COMPLETE\n\n634 lines implemented in src/jax_mppi/smppi.py\nAll SMPPI features: action_sequence, smoothness cost, dual bounds, integration\n580 lines in tests/test_smppi.py (18 comprehensive tests)\nAll tests passing\n\nPhase 4: Kernel MPPI (KMPPI) ‚úÖ COMPLETE\n\n660 lines implemented in src/jax_mppi/kmppi.py\nRBFKernel, kernel interpolation, control point optimization\n595 lines in tests/test_kmppi.py (23 comprehensive tests)\nAll tests passing (53/53 total tests pass)\n\nPhase 5: Smooth Comparison Example ‚úÖ COMPLETE\n\n442 lines in examples/smooth_comparison.py\nCompares MPPI, SMPPI, and KMPPI on 2D navigation with obstacle avoidance\nIncludes visualization with 4 subplots: trajectories, costs, controls, smoothness\nSupporting modules: src/jax_mppi/costs/ and src/jax_mppi/dynamics/\n\nPhase 6: Autotuning ‚úÖ COMPLETE\n\n656 lines in src/jax_mppi/autotune.py - Core CMA-ES autotuning\n375 lines in src/jax_mppi/autotune_global.py - Ray Tune global search\n218 lines in src/jax_mppi/autotune_qd.py - CMA-ME quality diversity\n305 lines in tests/test_autotune.py (21 unit tests)\n247 lines in tests/test_autotune_integration.py (4 integration tests)\n321 lines in examples/autotune_pendulum.py - Full demonstration\n90 lines in examples/autotune_basic.py - Minimal example\nAll 25 tests passing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPackage\nCore Code\nTests\nExamples\nTotal\n\n\n\n\npytorch_mppi\n1214 lines\n~500 lines\n~800 lines\n~2500 lines\n\n\njax_mppi (current)\n2919 lines\n2124 lines\n681 lines\n5724 lines\n\n\nCompletion %\n240%\n425%\n85%\n229%\n\n\n\nCore code now includes: mppi.py (353), smppi.py (634), kmppi.py (660), autotune.py (656), autotune_global.py (375), autotune_qd.py (218), plus supporting modules.\n\n\n\n\n\n\nFeature\npytorch_mppi\njax_mppi\nStatus\n\n\n\n\nCore MPPI Algorithm\n‚úì\n‚úì\n‚úÖ Complete\n\n\nBasic sampling & weighting\n‚úì\n‚úì\n‚úÖ\n\n\nControl bounds (u_min/u_max)\n‚úì\n‚úì\n‚úÖ\n\n\nControl scaling (u_scale)\n‚úì\n‚úì\n‚úÖ\n\n\nPartial updates (u_per_command)\n‚úì\n‚úì\n‚úÖ\n\n\nStep-dependent dynamics\n‚úì\n‚úì\n‚úÖ\n\n\nStochastic dynamics (rollout_samples)\n‚úì\n‚úì\n‚úÖ\n\n\nSample null action\n‚úì\n‚úì\n‚úÖ\n\n\nNoise absolute cost\n‚úì\n‚úì\n‚úÖ\n\n\nTerminal cost function\n‚úì\n‚úì\n‚úÖ\n\n\nShift nominal trajectory\n‚úì\n‚úì\n‚úÖ\n\n\nGet rollouts (visualization)\n‚úì\n‚úì\n‚úÖ\n\n\nReset controller\n‚úì\n‚úì\n‚úÖ\n\n\nSmooth MPPI (SMPPI)\n‚úì\n‚úì\n‚úÖ Complete\n\n\nAction sequence tracking\n‚úì\n‚úì\n‚úÖ\n\n\nSmoothness penalty\n‚úì\n‚úì\n‚úÖ\n\n\nSeparate action/control bounds\n‚úì\n‚úì\n‚úÖ\n\n\nDelta_t integration\n‚úì\n‚úì\n‚úÖ\n\n\nShift with continuity\n‚úì\n‚úì\n‚úÖ\n\n\nKernel MPPI (KMPPI)\n‚úì\n‚úì\n‚úÖ Complete\n\n\nKernel interpolation\n‚úì\n‚úì\n‚úÖ\n\n\nRBF kernel\n‚úì\n‚úì\n‚úÖ\n\n\nSupport point optimization\n‚úì\n‚úì\n‚úÖ\n\n\nTime grid management (Tk/Hs)\n‚úì\n‚úì\n‚úÖ\n\n\nSolve-based interpolation\n‚úì\n‚úì\n‚úÖ\n\n\nAutotuning\n‚úì\n‚úì\n‚úÖ Complete\n\n\nCMA-ES local tuning\n‚úì\n‚úì\n‚úÖ\n\n\nRay Tune global search\n‚úì\n‚úì\n‚úÖ\n\n\nCMA-ME quality diversity\n‚úì\n‚úì\n‚úÖ\n\n\nParameter types (lambda, sigma, mu, horizon)\n‚úì\n‚úì\n‚úÖ\n\n\nAll MPPI variants support\n‚úì\n‚úì\n‚úÖ\n\n\nExamples\n\n\n\n\n\nPendulum swing-up\n‚úì\n‚úì\n‚úÖ Complete\n\n\nSmooth MPPI comparison\n‚úì\n‚úì\n‚úÖ Complete\n\n\nAutotuning example\n‚úì\n‚úì\n‚úÖ Complete\n\n\nPendulum with learned dynamics\n‚úì\n‚úó\nüî¥ Not planned\n\n\n\n\n\n\njax_mppi/\n‚îú‚îÄ‚îÄ pyproject.toml              ‚úÖ Exists\n‚îú‚îÄ‚îÄ README.md                   ‚úÖ Exists\n‚îú‚îÄ‚îÄ LICENSE                     ‚úÖ Exists  \n‚îú‚îÄ‚îÄ src/jax_mppi/\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py            ‚úÖ Exists (updated for autotune)\n‚îÇ   ‚îú‚îÄ‚îÄ types.py               ‚úÖ Exists (9 lines)\n‚îÇ   ‚îú‚îÄ‚îÄ mppi.py                ‚úÖ Exists (353 lines) - COMPLETE\n‚îÇ   ‚îú‚îÄ‚îÄ smppi.py               ‚úÖ Exists (634 lines) - COMPLETE\n‚îÇ   ‚îú‚îÄ‚îÄ kmppi.py               ‚úÖ Exists (660 lines) - COMPLETE\n‚îÇ   ‚îú‚îÄ‚îÄ autotune.py            ‚úÖ Exists (656 lines) - COMPLETE\n‚îÇ   ‚îú‚îÄ‚îÄ autotune_global.py     ‚úÖ Exists (375 lines) - COMPLETE\n‚îÇ   ‚îú‚îÄ‚îÄ autotune_qd.py         ‚úÖ Exists (218 lines) - COMPLETE\n‚îÇ   ‚îú‚îÄ‚îÄ costs/                 ‚úÖ Exists (supporting modules)\n‚îÇ   ‚îî‚îÄ‚îÄ dynamics/              ‚úÖ Exists (supporting modules)\n‚îú‚îÄ‚îÄ tests/\n‚îÇ   ‚îú‚îÄ‚îÄ test_mppi.py           ‚úÖ Exists (115 lines) - COMPLETE\n‚îÇ   ‚îú‚îÄ‚îÄ test_pendulum.py       ‚úÖ Exists (282 lines) - COMPLETE\n‚îÇ   ‚îú‚îÄ‚îÄ test_smppi.py          ‚úÖ Exists (580 lines) - COMPLETE\n‚îÇ   ‚îú‚îÄ‚îÄ test_autotune.py       ‚úÖ Exists (305 lines, 21 tests) - COMPLETE\n‚îÇ   ‚îî‚îÄ‚îÄ test_autotune_integration.py ‚úÖ Exists (247 lines, 4 tests) - COMPLETE\n‚îÇ   ‚îî‚îÄ‚îÄ test_kmppi.py          ‚úÖ Exists (595 lines) - COMPLETE\n‚îú‚îÄ‚îÄ examples/\n‚îÇ   ‚îú‚îÄ‚îÄ pendulum.py            ‚úÖ Exists (270 lines) - COMPLETE\n‚îÇ   ‚îú‚îÄ‚îÄ smooth_comparison.py   ‚úÖ Exists (442 lines) - COMPLETE\n‚îÇ   ‚îú‚îÄ‚îÄ autotune_pendulum.py   ‚úÖ Exists (321 lines) - COMPLETE\n‚îÇ   ‚îî‚îÄ‚îÄ autotune_basic.py      ‚úÖ Exists (90 lines) - COMPLETE\n‚îî‚îÄ‚îÄ docs/\n    ‚îî‚îÄ‚îÄ plan/\n        ‚îî‚îÄ‚îÄ porting_pytorch_jax.md ‚úÖ This file\n\n\n\nPriority Order:\n\nPhase 3: SMPPI Implementation (High Priority)\n\nCore functionality that adds smoothness to control\nEstimated ~250-300 lines for smppi.py\nEstimated ~150-200 lines for tests\nReference: ../pytorch_mppi/src/pytorch_mppi/mppi.py (SMPPI class)\n\nPhase 4: KMPPI Implementation (High Priority)\n\nNovel contribution with kernel interpolation\nEstimated ~300-350 lines for kmppi.py\nEstimated ~150-200 lines for tests\nReference: ../pytorch_mppi/src/pytorch_mppi/mppi.py (KMPPI class)\n\nPhase 5: Smooth Comparison Example (Medium Priority)\n\nDemonstrates value of SMPPI and KMPPI\nEstimated ~200-250 lines\nReference: ../pytorch_mppi/tests/smooth_mppi.py\n\nAdditional Examples (Low Priority)\n\nPendulum with learned dynamics\nMore complex environments\n\nPhase 6: Autotuning (Optional/Stretch)\n\nAdvanced feature for hyperparameter optimization\nEstimated ~300-400 lines\nReference: ../pytorch_mppi/src/pytorch_mppi/autotune.py"
  },
  {
    "objectID": "plan/completed/porting_pytorch_jax.html#design-decisions",
    "href": "plan/completed/porting_pytorch_jax.html#design-decisions",
    "title": "JAX MPPI Implementation Plan",
    "section": "",
    "text": "Use @jax.tree_util.register_dataclass (or flax.struct.dataclass) to hold MPPI state (nominal trajectory U, PRNG key, config). All core functions are pure: command(state, mppi_state) -&gt; (action, mppi_state).\nRationale: Idiomatic JAX ‚Äî pure functions compose with jit, vmap, grad. No mutable self. Avoids heavyweight dependencies like Equinox for what is fundamentally a numerical algorithm.\n\n\n\n\n\n\n\n\n\n\nPyTorch\nJAX\n\n\n\n\ntorch.distributions.MultivariateNormal\njax.random.multivariate_normal\n\n\ntensor.to(device)\njax.device_put / automatic\n\n\nPython for-loop over horizon\njax.lax.scan\n\n\n@handle_batch_input decorator\njax.vmap\n\n\ntorch.roll\njnp.roll\n\n\ntorch.linalg.solve\njnp.linalg.solve\n\n\nIn-place mutation (self.U = ...)\nReturn new state (pytree)"
  },
  {
    "objectID": "plan/completed/porting_pytorch_jax.html#notes-from-..pytorch_mppi-review-jan-2026",
    "href": "plan/completed/porting_pytorch_jax.html#notes-from-..pytorch_mppi-review-jan-2026",
    "title": "JAX MPPI Implementation Plan",
    "section": "",
    "text": "Actionable parity items to carry over:\n\nSMPPI semantics: maintains action_sequence separately from lifted control U; integrates with delta_t; smoothness cost from diff(action_sequence).\nSMPPI bounds: support action_min/action_max distinct from u_min/u_max (control-derivative bounds).\nKMPPI internals: keep theta as control points; build Tk/Hs time grids; kernel interpolation via solve(Ktktk, K); batch interpolation with vmap.\nSampling options: rollout_samples (M), sample_null_action, noise_abs_cost (abs(noise) in action cost).\nRollouts: get_rollouts handles state batch and dynamics that may augment state (take first nx)."
  },
  {
    "objectID": "plan/completed/porting_pytorch_jax.html#package-structure",
    "href": "plan/completed/porting_pytorch_jax.html#package-structure",
    "title": "JAX MPPI Implementation Plan",
    "section": "",
    "text": "jax_mppi/\n‚îú‚îÄ‚îÄ pyproject.toml\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ LICENSE\n‚îú‚îÄ‚îÄ src/jax_mppi/\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py          # Public API exports\n‚îÇ   ‚îú‚îÄ‚îÄ mppi.py              # Core MPPI (MPPIConfig, MPPIState, command, reset, etc.)\n‚îÇ   ‚îú‚îÄ‚îÄ smppi.py             # Smooth MPPI variant\n‚îÇ   ‚îú‚îÄ‚îÄ kmppi.py             # Kernel MPPI variant + TimeKernel / RBFKernel\n‚îÇ   ‚îú‚îÄ‚îÄ types.py             # Type aliases, protocols for Dynamics/Cost callables\n‚îÇ   ‚îî‚îÄ‚îÄ autotune.py          # Autotuning (CMA-ES wrapper, parameter search)\n‚îú‚îÄ‚îÄ tests/\n‚îÇ   ‚îú‚îÄ‚îÄ test_mppi.py         # Unit tests for core MPPI\n‚îÇ   ‚îú‚îÄ‚îÄ test_smppi.py        # Unit tests for SMPPI\n‚îÇ   ‚îú‚îÄ‚îÄ test_kmppi.py        # Unit tests for KMPPI\n‚îÇ   ‚îî‚îÄ‚îÄ test_pendulum.py     # Integration test with pendulum env\n‚îú‚îÄ‚îÄ examples/\n‚îÇ   ‚îú‚îÄ‚îÄ pendulum.py          # Gym pendulum with true dynamics\n‚îÇ   ‚îú‚îÄ‚îÄ pendulum_approximate.py  # Learned dynamics\n‚îÇ   ‚îî‚îÄ‚îÄ smooth_comparison.py # MPPI vs SMPPI vs KMPPI\n‚îî‚îÄ‚îÄ docs/\n    ‚îî‚îÄ‚îÄ plan/"
  },
  {
    "objectID": "plan/completed/porting_pytorch_jax.html#phased-implementation",
    "href": "plan/completed/porting_pytorch_jax.html#phased-implementation",
    "title": "JAX MPPI Implementation Plan",
    "section": "",
    "text": "Files: pyproject.toml, src/jax_mppi/types.py, src/jax_mppi/mppi.py, src/jax_mppi/__init__.py\n\npyproject.toml ‚Äî project metadata, deps: jax[cuda13], jaxlib, optional gymnasium for examples.\ntypes.py ‚Äî Type definitions:\n# Dynamics: (state, action) -&gt; next_state  or  (state, action, t) -&gt; next_state\nDynamicsFn = Callable[..., jax.Array]\n# Cost: (state, action) -&gt; scalar_cost  or  (state, action, t) -&gt; scalar_cost\nRunningCostFn = Callable[..., jax.Array]\n# Terminal: (states, actions) -&gt; scalar_cost\nTerminalCostFn = Callable[[jax.Array, jax.Array], jax.Array]\nmppi.py ‚Äî Core implementation:\nData structures (registered as JAX pytrees):\n@dataclass\nclass MPPIConfig:\n    # Static config (not traced through JAX)\n    num_samples: int       # K\n    horizon: int           # T\n    nx: int\n    nu: int\n    lambda_: float\n    u_scale: float\n    u_per_command: int\n    step_dependent_dynamics: bool\n    rollout_samples: int   # M\n    rollout_var_cost: float\n    rollout_var_discount: float\n    sample_null_action: bool\n    noise_abs_cost: bool\n\n@dataclass\nclass MPPIState:\n    # Dynamic state (carried through JAX transforms)\n    U: jax.Array           # (T, nu) nominal trajectory\n    u_init: jax.Array      # (nu,) default action for shift\n    noise_mu: jax.Array    # (nu,)\n    noise_sigma: jax.Array # (nu, nu)\n    noise_sigma_inv: jax.Array\n    u_min: jax.Array | None\n    u_max: jax.Array | None\n    key: jax.Array         # PRNG key\nFunctions:\ndef create(\n    nx, nu, noise_sigma, num_samples=100, horizon=15, lambda_=1.0,\n    noise_mu=None, u_min=None, u_max=None, u_init=None, U_init=None,\n    u_scale=1, u_per_command=1, step_dependent_dynamics=False,\n    rollout_samples=1, rollout_var_cost=0., rollout_var_discount=0.95,\n    sample_null_action=False, noise_abs_cost=False, key=None,\n) -&gt; tuple[MPPIConfig, MPPIState]:\n    \"\"\"Factory: create config + initial state.\"\"\"\n\ndef command(\n    config: MPPIConfig,\n    mppi_state: MPPIState,\n    current_obs: jax.Array,\n    dynamics: DynamicsFn,\n    running_cost: RunningCostFn,\n    terminal_cost: TerminalCostFn | None = None,\n    shift: bool = True,\n) -&gt; tuple[jax.Array, MPPIState]:\n    \"\"\"Compute optimal action and return updated state.\"\"\"\n\ndef reset(config: MPPIConfig, mppi_state: MPPIState, key: jax.Array) -&gt; MPPIState:\n    \"\"\"Reset nominal trajectory.\"\"\"\n\ndef get_rollouts(\n    config: MPPIConfig, mppi_state: MPPIState,\n    current_obs: jax.Array, dynamics: DynamicsFn,\n    num_rollouts: int = 1,\n) -&gt; jax.Array:\n    \"\"\"Forward-simulate trajectories for visualization.\"\"\"\nInternal functions (all JIT-compatible):\n\n_shift_nominal(mppi_state) -&gt; MPPIState ‚Äî jnp.roll + set last to u_init\n_sample_noise(key, K, T, noise_mu, noise_sigma) -&gt; (noise, new_key) ‚Äî sample from multivariate normal\n_compute_rollout_costs(config, current_obs, perturbed_actions, dynamics, running_cost, terminal_cost) ‚Äî uses jax.lax.scan over horizon, jax.vmap over K samples\n_compute_weights(costs, lambda_) ‚Äî softmax importance weighting\n_bound_action(action, u_min, u_max) ‚Äî jnp.clip\n\nKey JAX patterns:\n\nRollout loop: jax.lax.scan with carry = (state,), xs = actions[t]\nBatch over K samples: jax.vmap(_single_rollout, in_axes=(0, None, ...))\nBatch over M rollout samples (stochastic dynamics): nested vmap or scan\nAll internal functions decorated with @jax.jit or called inside a top-level jitted command\n\nUnit test: tests/test_mppi.py\n\nTest create() produces valid config/state\nTest command() returns correct shape\nTest cost reduction over iterations on simple 1D problem\nTest bounds are respected\n\n\n\n\n\nFiles: examples/pendulum.py, tests/test_pendulum.py\n\nImplement pendulum dynamics as a pure JAX function (no gym dependency for core test)\nRun MPPI loop, verify convergence (swing-up or stabilization)\nOptional: gym rendering wrapper for visualization\n\n\n\n\nFiles: src/jax_mppi/smppi.py, tests/test_smppi.py\n\nData structures:\n@dataclass\nclass SMPPIState(MPPIState):\n    action_sequence: jax.Array  # (T, nu) actual actions\n    w_action_seq_cost: float\n    delta_t: float\n    action_min: jax.Array | None\n    action_max: jax.Array | None\nFunctions: Same API as mppi.py but with:\n\n_shift_nominal shifts both U (velocity) and action_sequence\n_compute_perturbed_actions integrates velocity to get actions\n_compute_total_cost adds smoothness penalty: ||diff(actions)||^2\nreset() zeros both U and action_sequence\nchange_horizon() keeps both U and action_sequence in sync (truncate/extend)\n\nTest: Verify smoother trajectories than base MPPI on 2D navigation\n\n\n\n\nFiles: src/jax_mppi/kmppi.py, tests/test_kmppi.py\n\nKernel abstractions:\ndef rbf_kernel(t, tk, sigma=1.0):\n    d = jnp.sum((t[:, None] - tk) ** 2, axis=-1)\n    return jnp.exp(-d / (2 * sigma ** 2 + 1e-8))\n\ndef kernel_interpolate(t, tk, coeffs, kernel_fn):\n    K_t_tk = kernel_fn(t, tk)\n    K_tk_tk = kernel_fn(tk, tk)\n    weights = jnp.linalg.solve(K_tk_tk, K_t_tk.T).T\n    return weights @ coeffs\nData structures:\n@dataclass\nclass KMPPIState(MPPIState):\n    theta: jax.Array         # (num_support_pts, nu)\n    num_support_pts: int\nFunctions: Override _compute_perturbed_actions to sample sparse + interpolate. Update theta instead of U.\n\nBuild Tk and Hs time grids on init and on horizon changes\nUse kernel_interpolate() with solve(Ktktk, K) (avoid explicit inverse)\nBatch interpolate with jax.vmap for K samples\n\nTest: Verify fewer parameters produce smooth trajectories\n\n\n\n\nFiles: examples/smooth_comparison.py\n\nSide-by-side MPPI vs SMPPI vs KMPPI on 2D navigation\nPlot trajectories and control signals\n\n\n\n\nFiles: src/jax_mppi/autotune.py\n\nWrap CMA-ES (cmaes or evosax for JAX-native) for sigma/lambda/horizon tuning\nSimpler than pytorch_mppi‚Äôs framework ‚Äî skip Ray Tune and QD initially\nFunctional API: tune_step(eval_fn, params, optimizer_state) -&gt; (params, optimizer_state)"
  },
  {
    "objectID": "plan/completed/porting_pytorch_jax.html#verification-strategy",
    "href": "plan/completed/porting_pytorch_jax.html#verification-strategy",
    "title": "JAX MPPI Implementation Plan",
    "section": "",
    "text": "Unit tests (per phase): pytest tests/ ‚Äî shape checks, cost reduction, bounds\nPendulum benchmark: Compare convergence (total reward) against pytorch_mppi on same scenario\nJIT correctness: Ensure jax.jit(command) produces identical results to non-jitted version\nPerformance: Benchmark command() latency vs pytorch_mppi (JAX should win after warmup due to XLA compilation)\nSmooth variants: Visual comparison of trajectory smoothness\n\n\n\nIMPORTANT: You should always use the virtual environment. To run the tests and all of the other python files.\n\nOption A: add a tests/conftest.py to insert src into sys.path.\nOption B: run tests after uv pip install -e . (editable install)."
  },
  {
    "objectID": "plan/completed/porting_pytorch_jax.html#dependencies",
    "href": "plan/completed/porting_pytorch_jax.html#dependencies",
    "title": "JAX MPPI Implementation Plan",
    "section": "",
    "text": "Core: jax[cuda13], jaxlib, numpy Testing: pytest, gymnasium[classic_control] Autotuning (optional): cmaes or evosax Examples (optional): matplotlib, gymnasium"
  },
  {
    "objectID": "plan/completed/porting_pytorch_jax.html#actionable-task-checklist",
    "href": "plan/completed/porting_pytorch_jax.html#actionable-task-checklist",
    "title": "JAX MPPI Implementation Plan",
    "section": "",
    "text": "Mirror pytorch_mppi signature flags: rollout_samples, sample_null_action, noise_abs_cost.\nImplement get_rollouts handling: accept single or batched state; allow dynamics that augment state (take :nx).\nAdd shift_nominal_trajectory via jnp.roll + u_init fill.\nImplement action cost with optional abs(noise) branch.\nAdd u_per_command slicing and u_scale application in command.\n\n\n\n\n\nCarry action_sequence in state and integrate U with delta_t.\nImplement distinct action bounds (action_min/action_max) vs control bounds (u_min/u_max).\nAdd smoothness cost from diff(action_sequence) and weight w_action_seq_cost.\nEnsure reset() updates both U and action_sequence.\nImplement proper shift with action continuity (hold last value).\nImplement dual bounding system (_bound_control and _bound_action).\nRecompute effective noise after bounding for accurate cost.\n\n\n\n\n\nImplement theta control points + interpolation kernel (RBF by default).\nBuild Tk/Hs grids and re-build on horizon changes.\nUse solve(Ktktk, K) for interpolation weights (no explicit inverse).\nShift theta via interpolation when shifting nominal trajectory.\nImplement RBFKernel with configurable sigma.\nNoise sampling in control point space.\nBatched interpolation with vmap.\n\n\n\n\n\nMirror autotune interface from pytorch_mppi/autotune*.py at a minimal level (evaluation fn + optimizer loop).\nPort tests/auto_tune_parameters.py logic into a JAX-friendly example."
  },
  {
    "objectID": "plan/completed/cuda_mppi_implementation.html",
    "href": "plan/completed/cuda_mppi_implementation.html",
    "title": "CUDA/C++ MPPI Implementation Plan",
    "section": "",
    "text": "Implement CUDA/C++ versions of MPPI, SMPPI, and KMPPI controllers within the src/cuda_mppi directory, using ../MPPI-Generic as a reference for high-performance CUDA implementation patterns.\n\n\n\n\nCreate a C++/CUDA project structure within src/cuda_mppi.\nImplement the standard MPPI algorithm (mirroring src/jax_mppi/mppi.py).\nImplement the Smooth MPPI (SMPPI) algorithm (mirroring src/jax_mppi/smppi.py).\nImplement the Kernel MPPI (KMPPI) algorithm (mirroring src/jax_mppi/kmppi.py).\nEnsure the implementations are self-contained or have clear interfaces (even if not fully hooked up to Python yet).\n\n\n\n\nWe will create src/cuda_mppi with the following structure:\nsrc/cuda_mppi/\n‚îú‚îÄ‚îÄ CMakeLists.txt              # Build configuration\n‚îú‚îÄ‚îÄ include/\n‚îÇ   ‚îî‚îÄ‚îÄ mppi/\n‚îÇ       ‚îú‚îÄ‚îÄ controllers/\n‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ mppi.cuh        # Standard MPPI header\n‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ smppi.cuh       # Smooth MPPI header\n‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ kmppi.cuh       # Kernel MPPI header\n‚îÇ       ‚îú‚îÄ‚îÄ core/\n‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ mppi_common.cuh # Common structures and utilities\n‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ kernels.cuh     # Shared CUDA kernels (rollout, cost, etc.)\n‚îÇ       ‚îú‚îÄ‚îÄ dynamics/\n‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ dynamics.cuh    # Dynamics interface and base classes\n‚îÇ       ‚îú‚îÄ‚îÄ costs/\n‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ costs.cuh       # Cost function interface\n‚îÇ       ‚îî‚îÄ‚îÄ utils/\n‚îÇ           ‚îî‚îÄ‚îÄ cuda_utils.cuh  # CUDA helper functions\n‚îî‚îÄ‚îÄ src/\n    ‚îú‚îÄ‚îÄ controllers/\n    ‚îÇ   ‚îú‚îÄ‚îÄ mppi.cu             # Standard MPPI implementation\n    ‚îÇ   ‚îú‚îÄ‚îÄ smppi.cu            # Smooth MPPI implementation\n    ‚îÇ   ‚îî‚îÄ‚îÄ kmppi.cu            # Kernel MPPI implementation\n    ‚îú‚îÄ‚îÄ core/\n    ‚îÇ   ‚îî‚îÄ‚îÄ kernels.cu          # Kernel implementations\n    ‚îî‚îÄ‚îÄ utils/\n        ‚îî‚îÄ‚îÄ cuda_utils.cu       # Utility implementations\n\n\n\n\n\n\nmppi_common.cuh: Define data structures for state, configuration, and control sequences.\nkernels.cuh:\n\nrollout_kernel: Generic kernel to propagate dynamics and compute costs for \\(K\\) samples over \\(T\\) timesteps.\nreduce_cost_kernel: Kernel to compute weighted averages of trajectories.\n\n\n\n\n\n\nDefine template interfaces or base classes for Dynamics and RunningCost so that specific system models (like Quadrotor) can be plugged in.\nNote: Since we are focusing on the controllers, we will provide a simple example dynamics (e.g., Double Integrator or simple Quadrotor) to verify compilation, but the main focus is the controller logic.\n\n\n\n\n\n\n\nLogic:\n\nSample noise \\(\\epsilon \\sim \\mathcal{N}(0, \\Sigma)\\).\nCompute \\(u_{per} = u_{nom} + \\epsilon\\).\nRollout dynamics using \\(u_{per}\\).\nCompute costs \\(J(\\tau)\\).\nCompute weights \\(w \\propto \\exp(-J/\\lambda)\\).\nUpdate \\(u_{nom} \\leftarrow u_{nom} + \\sum w \\epsilon\\).\n\nCUDA: Use block-per-sample or thread-per-sample approach depending on horizon/state size. MPPI-Generic often uses block-y striding.\n\n\n\n\n\nLogic:\n\nSample noise in velocity space \\(\\delta v\\).\nIntegrate to get actions \\(u\\).\nAdd smoothness cost \\(\\sum (\\Delta u)^2\\).\nUpdate velocity sequence \\(v_{nom}\\).\n\nCUDA: Needs a kernel that handles the integration step (velocity -&gt; action) before the rollout.\n\n\n\n\n\nLogic:\n\nControl trajectory parameterized by control points \\(\\theta\\) and kernel \\(K(\\cdot, \\cdot)\\).\nSample noise on \\(\\theta\\).\nInterpolate \\(\\theta \\to u(t)\\).\nRollout.\nUpdate \\(\\theta\\).\n\nCUDA: Needs a kernel multiplication/interpolation step before rollout.\n\n\n\n\n\n\n\nSetup: Create directory structure and CMakeLists.txt.\nCommon: Implement mppi_common.cuh and basic cuda_utils.cuh.\nDynamics/Cost: Define minimal interfaces.\nMPPI: Implement mppi.cuh and mppi.cu.\nSMPPI: Implement smppi.cuh and smppi.cu.\nKMPPI: Implement kmppi.cuh and kmppi.cu.\nVerification: Create a dummy main.cu to instantiate these controllers and verify they compile.\n\n\n\n\n\n\nExpose the C++ MPPI controllers to Python to allow direct usage from the jax_mppi package, potentially replacing the JAX implementation for performance-critical sections.\n\n\n\n\nBinding Library: Use nanobind (efficient, small footprint) to create Python bindings for the C++ classes.\nData Transfer:\n\nBasic: Accept NumPy arrays (CPU) and copy to GPU in C++.\nAdvanced (Zero-Copy): Accept DLPack capsules (from jax.Array or torch.Tensor) to pass GPU pointers directly to the C++ controllers, avoiding CPU-GPU transfers.\n\n\n\n\n\n\nProject Config:\n\nUpdate pyproject.toml to support C++ extensions (e.g., using scikit-build-core).\nAdd dependencies: nanobind, scikit-build-core.\n\nBindings Code:\n\nCreate src/cuda_mppi/bindings/bindings.cpp.\nExpose MPPIConfig struct as a Python class.\nExpose MPPIController, SMPPIController, KMPPIController classes.\nBind methods like compute(state) and get_action().\nImplement type casters for Eigen::VectorXf &lt;-&gt; numpy.ndarray (using nanobind/eigen/dense.h).\n\nCMake Update:\n\nAdd nanobind_add_module target.\nLink against cuda_mppi and CUDA libraries.\n\nIntegration:\n\nCreate a Python wrapper module (e.g., jax_mppi.cuda) that imports the extension.\nAdd tests in tests/ to verify correctness against the JAX implementation.\n\n\n\n\n\n\n\n\nAllow users to define dynamics and cost functions in Python (initially as C++ code strings, or eventually transpiled from JAX) and compile the specialized MPPI controller at runtime. This avoids the need to recompile the shared library for every new system.\n\n\n\n\nNVRTC (NVIDIA Runtime Compilation): Use NVRTC to compile CUDA C++ code strings into PTX at runtime.\nCUDA Driver API: Use the Driver API (cuModuleLoadData, cuLaunchKernel) to load the compiled PTX and launch the rollout_kernel.\nWarm Start: The compilation happens once during the ‚Äúwarm start‚Äù phase (controller initialization), enabling high-performance rollouts thereafter.\n\n\n\n\n\nBuild Config: Link against nvrtc and cuda (Driver API).\nJIT Compiler Class (src/cuda_mppi/include/mppi/jit/jit_compiler.hpp):\n\nInputs: Strings for dynamics_struct_code and cost_struct_code.\nAction: Constructs the full .cu source code (headers + user structs + template instantiation).\nOutput: Compiles to PTX using nvrtcProgramCompile.\nUpdated wrapper generation to work with Driver API.\n\nJIT Controller (JITMPPIController):\n\nA generic controller class that holds CUfunction handles instead of hardcoded kernels.\ncompute() method launches the generated kernel via cuLaunchKernel.\nImplemented in include/mppi/controllers/jit_mppi.hpp and src/jit/jit_mppi_controller.cpp.\n\nPython Interface:\n\nExpose JITMPPIController to Python via nanobind.\nExample usage:\ndynamics_code = \"\"\"\nstruct UserDynamics {\n    __device__ void step(...) { ... }\n};\n\"\"\"\ncost_code = \"\"\"\nstruct UserCost {\n    __device__ float compute(...) { ... }\n    __device__ float terminal_cost(...) { ... }\n};\n\"\"\"\ncontroller = cuda_mppi.JITMPPIController(config, dynamics_code, cost_code, include_paths)\n\nVerification & Examples:\n\nImplemented examples/cuda_pendulum_jit.py - complete pendulum swing-up example with matplotlib plotting.\nCreated include/mppi/jit/examples.hpp with example templates for common systems:\n\nPendulum dynamics and cost\nDouble integrator dynamics and cost\nCart-pole dynamics and cost\n\nCreated examples/JIT_EXAMPLES_README.md with comprehensive documentation.\nNote: Interactive pygame visualization can be added in future enhancement.\n\n\n\n\n\n\nNew Files:\n\ninclude/mppi/controllers/jit_mppi.hpp - JIT controller header\nsrc/jit/jit_mppi_controller.cpp - JIT controller implementation\ninclude/mppi/jit/examples.hpp - Example code templates\nexamples/cuda_pendulum_jit.py - Pendulum swing-up example\nexamples/JIT_EXAMPLES_README.md - JIT examples documentation\n\nModified Files:\n\nsrc/jit/jit_compiler.cpp - Updated wrapper generation for Driver API compatibility\nCMakeLists.txt - Added JIT sources to build\nbindings/bindings.cu - Added Python bindings for JITMPPIController\n\n\n\n\n\nfrom jax_mppi import cuda_mppi\nimport numpy as np\nimport os\n\n# Set include path\nos.environ['CUDA_MPPI_INCLUDE_DIR'] = '/path/to/src/cuda_mppi/include'\n\n# Configure MPPI\nconfig = cuda_mppi.MPPIConfig(\n    num_samples=1000,\n    horizon=50,\n    nx=2, nu=1,\n    lambda_=1.0,\n    dt=0.02,\n    u_scale=5.0,\n    w_action_seq_cost=0.0,\n    num_support_pts=10\n)\n\n# Define custom dynamics and cost\ndynamics_code = \"\"\"...\"\"\"  # See examples.hpp for templates\ncost_code = \"\"\"...\"\"\"\n\n# Create JIT controller (compilation happens here, ~1-5 seconds)\ncontroller = cuda_mppi.JITMPPIController(\n    config, dynamics_code, cost_code,\n    [os.environ['CUDA_MPPI_INCLUDE_DIR']]\n)\n\n# Use controller\nstate = np.array([1.0, 0.0], dtype=np.float32)\ncontroller.compute(state)\naction = controller.get_action()\ncontroller.shift()\n\n\n\n\n\nsrc/jax_mppi/*.py (Main reference for the MPPI Implementation)\n../MPPI-Generic (Reference for CUDA patterns)\nNVRTC Documentation"
  },
  {
    "objectID": "plan/completed/cuda_mppi_implementation.html#objective",
    "href": "plan/completed/cuda_mppi_implementation.html#objective",
    "title": "CUDA/C++ MPPI Implementation Plan",
    "section": "",
    "text": "Implement CUDA/C++ versions of MPPI, SMPPI, and KMPPI controllers within the src/cuda_mppi directory, using ../MPPI-Generic as a reference for high-performance CUDA implementation patterns."
  },
  {
    "objectID": "plan/completed/cuda_mppi_implementation.html#goals",
    "href": "plan/completed/cuda_mppi_implementation.html#goals",
    "title": "CUDA/C++ MPPI Implementation Plan",
    "section": "",
    "text": "Create a C++/CUDA project structure within src/cuda_mppi.\nImplement the standard MPPI algorithm (mirroring src/jax_mppi/mppi.py).\nImplement the Smooth MPPI (SMPPI) algorithm (mirroring src/jax_mppi/smppi.py).\nImplement the Kernel MPPI (KMPPI) algorithm (mirroring src/jax_mppi/kmppi.py).\nEnsure the implementations are self-contained or have clear interfaces (even if not fully hooked up to Python yet)."
  },
  {
    "objectID": "plan/completed/cuda_mppi_implementation.html#directory-structure",
    "href": "plan/completed/cuda_mppi_implementation.html#directory-structure",
    "title": "CUDA/C++ MPPI Implementation Plan",
    "section": "",
    "text": "We will create src/cuda_mppi with the following structure:\nsrc/cuda_mppi/\n‚îú‚îÄ‚îÄ CMakeLists.txt              # Build configuration\n‚îú‚îÄ‚îÄ include/\n‚îÇ   ‚îî‚îÄ‚îÄ mppi/\n‚îÇ       ‚îú‚îÄ‚îÄ controllers/\n‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ mppi.cuh        # Standard MPPI header\n‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ smppi.cuh       # Smooth MPPI header\n‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ kmppi.cuh       # Kernel MPPI header\n‚îÇ       ‚îú‚îÄ‚îÄ core/\n‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ mppi_common.cuh # Common structures and utilities\n‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ kernels.cuh     # Shared CUDA kernels (rollout, cost, etc.)\n‚îÇ       ‚îú‚îÄ‚îÄ dynamics/\n‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ dynamics.cuh    # Dynamics interface and base classes\n‚îÇ       ‚îú‚îÄ‚îÄ costs/\n‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ costs.cuh       # Cost function interface\n‚îÇ       ‚îî‚îÄ‚îÄ utils/\n‚îÇ           ‚îî‚îÄ‚îÄ cuda_utils.cuh  # CUDA helper functions\n‚îî‚îÄ‚îÄ src/\n    ‚îú‚îÄ‚îÄ controllers/\n    ‚îÇ   ‚îú‚îÄ‚îÄ mppi.cu             # Standard MPPI implementation\n    ‚îÇ   ‚îú‚îÄ‚îÄ smppi.cu            # Smooth MPPI implementation\n    ‚îÇ   ‚îî‚îÄ‚îÄ kmppi.cu            # Kernel MPPI implementation\n    ‚îú‚îÄ‚îÄ core/\n    ‚îÇ   ‚îî‚îÄ‚îÄ kernels.cu          # Kernel implementations\n    ‚îî‚îÄ‚îÄ utils/\n        ‚îî‚îÄ‚îÄ cuda_utils.cu       # Utility implementations"
  },
  {
    "objectID": "plan/completed/cuda_mppi_implementation.html#implementation-details",
    "href": "plan/completed/cuda_mppi_implementation.html#implementation-details",
    "title": "CUDA/C++ MPPI Implementation Plan",
    "section": "",
    "text": "mppi_common.cuh: Define data structures for state, configuration, and control sequences.\nkernels.cuh:\n\nrollout_kernel: Generic kernel to propagate dynamics and compute costs for \\(K\\) samples over \\(T\\) timesteps.\nreduce_cost_kernel: Kernel to compute weighted averages of trajectories.\n\n\n\n\n\n\nDefine template interfaces or base classes for Dynamics and RunningCost so that specific system models (like Quadrotor) can be plugged in.\nNote: Since we are focusing on the controllers, we will provide a simple example dynamics (e.g., Double Integrator or simple Quadrotor) to verify compilation, but the main focus is the controller logic.\n\n\n\n\n\n\n\nLogic:\n\nSample noise \\(\\epsilon \\sim \\mathcal{N}(0, \\Sigma)\\).\nCompute \\(u_{per} = u_{nom} + \\epsilon\\).\nRollout dynamics using \\(u_{per}\\).\nCompute costs \\(J(\\tau)\\).\nCompute weights \\(w \\propto \\exp(-J/\\lambda)\\).\nUpdate \\(u_{nom} \\leftarrow u_{nom} + \\sum w \\epsilon\\).\n\nCUDA: Use block-per-sample or thread-per-sample approach depending on horizon/state size. MPPI-Generic often uses block-y striding.\n\n\n\n\n\nLogic:\n\nSample noise in velocity space \\(\\delta v\\).\nIntegrate to get actions \\(u\\).\nAdd smoothness cost \\(\\sum (\\Delta u)^2\\).\nUpdate velocity sequence \\(v_{nom}\\).\n\nCUDA: Needs a kernel that handles the integration step (velocity -&gt; action) before the rollout.\n\n\n\n\n\nLogic:\n\nControl trajectory parameterized by control points \\(\\theta\\) and kernel \\(K(\\cdot, \\cdot)\\).\nSample noise on \\(\\theta\\).\nInterpolate \\(\\theta \\to u(t)\\).\nRollout.\nUpdate \\(\\theta\\).\n\nCUDA: Needs a kernel multiplication/interpolation step before rollout."
  },
  {
    "objectID": "plan/completed/cuda_mppi_implementation.html#execution-plan",
    "href": "plan/completed/cuda_mppi_implementation.html#execution-plan",
    "title": "CUDA/C++ MPPI Implementation Plan",
    "section": "",
    "text": "Setup: Create directory structure and CMakeLists.txt.\nCommon: Implement mppi_common.cuh and basic cuda_utils.cuh.\nDynamics/Cost: Define minimal interfaces.\nMPPI: Implement mppi.cuh and mppi.cu.\nSMPPI: Implement smppi.cuh and smppi.cu.\nKMPPI: Implement kmppi.cuh and kmppi.cu.\nVerification: Create a dummy main.cu to instantiate these controllers and verify they compile."
  },
  {
    "objectID": "plan/completed/cuda_mppi_implementation.html#python-integration-phase-2",
    "href": "plan/completed/cuda_mppi_implementation.html#python-integration-phase-2",
    "title": "CUDA/C++ MPPI Implementation Plan",
    "section": "",
    "text": "Expose the C++ MPPI controllers to Python to allow direct usage from the jax_mppi package, potentially replacing the JAX implementation for performance-critical sections.\n\n\n\n\nBinding Library: Use nanobind (efficient, small footprint) to create Python bindings for the C++ classes.\nData Transfer:\n\nBasic: Accept NumPy arrays (CPU) and copy to GPU in C++.\nAdvanced (Zero-Copy): Accept DLPack capsules (from jax.Array or torch.Tensor) to pass GPU pointers directly to the C++ controllers, avoiding CPU-GPU transfers.\n\n\n\n\n\n\nProject Config:\n\nUpdate pyproject.toml to support C++ extensions (e.g., using scikit-build-core).\nAdd dependencies: nanobind, scikit-build-core.\n\nBindings Code:\n\nCreate src/cuda_mppi/bindings/bindings.cpp.\nExpose MPPIConfig struct as a Python class.\nExpose MPPIController, SMPPIController, KMPPIController classes.\nBind methods like compute(state) and get_action().\nImplement type casters for Eigen::VectorXf &lt;-&gt; numpy.ndarray (using nanobind/eigen/dense.h).\n\nCMake Update:\n\nAdd nanobind_add_module target.\nLink against cuda_mppi and CUDA libraries.\n\nIntegration:\n\nCreate a Python wrapper module (e.g., jax_mppi.cuda) that imports the extension.\nAdd tests in tests/ to verify correctness against the JAX implementation."
  },
  {
    "objectID": "plan/completed/cuda_mppi_implementation.html#phase-3-runtime-dynamics-compilation-nvrtc",
    "href": "plan/completed/cuda_mppi_implementation.html#phase-3-runtime-dynamics-compilation-nvrtc",
    "title": "CUDA/C++ MPPI Implementation Plan",
    "section": "",
    "text": "Allow users to define dynamics and cost functions in Python (initially as C++ code strings, or eventually transpiled from JAX) and compile the specialized MPPI controller at runtime. This avoids the need to recompile the shared library for every new system.\n\n\n\n\nNVRTC (NVIDIA Runtime Compilation): Use NVRTC to compile CUDA C++ code strings into PTX at runtime.\nCUDA Driver API: Use the Driver API (cuModuleLoadData, cuLaunchKernel) to load the compiled PTX and launch the rollout_kernel.\nWarm Start: The compilation happens once during the ‚Äúwarm start‚Äù phase (controller initialization), enabling high-performance rollouts thereafter.\n\n\n\n\n\nBuild Config: Link against nvrtc and cuda (Driver API).\nJIT Compiler Class (src/cuda_mppi/include/mppi/jit/jit_compiler.hpp):\n\nInputs: Strings for dynamics_struct_code and cost_struct_code.\nAction: Constructs the full .cu source code (headers + user structs + template instantiation).\nOutput: Compiles to PTX using nvrtcProgramCompile.\nUpdated wrapper generation to work with Driver API.\n\nJIT Controller (JITMPPIController):\n\nA generic controller class that holds CUfunction handles instead of hardcoded kernels.\ncompute() method launches the generated kernel via cuLaunchKernel.\nImplemented in include/mppi/controllers/jit_mppi.hpp and src/jit/jit_mppi_controller.cpp.\n\nPython Interface:\n\nExpose JITMPPIController to Python via nanobind.\nExample usage:\ndynamics_code = \"\"\"\nstruct UserDynamics {\n    __device__ void step(...) { ... }\n};\n\"\"\"\ncost_code = \"\"\"\nstruct UserCost {\n    __device__ float compute(...) { ... }\n    __device__ float terminal_cost(...) { ... }\n};\n\"\"\"\ncontroller = cuda_mppi.JITMPPIController(config, dynamics_code, cost_code, include_paths)\n\nVerification & Examples:\n\nImplemented examples/cuda_pendulum_jit.py - complete pendulum swing-up example with matplotlib plotting.\nCreated include/mppi/jit/examples.hpp with example templates for common systems:\n\nPendulum dynamics and cost\nDouble integrator dynamics and cost\nCart-pole dynamics and cost\n\nCreated examples/JIT_EXAMPLES_README.md with comprehensive documentation.\nNote: Interactive pygame visualization can be added in future enhancement.\n\n\n\n\n\n\nNew Files:\n\ninclude/mppi/controllers/jit_mppi.hpp - JIT controller header\nsrc/jit/jit_mppi_controller.cpp - JIT controller implementation\ninclude/mppi/jit/examples.hpp - Example code templates\nexamples/cuda_pendulum_jit.py - Pendulum swing-up example\nexamples/JIT_EXAMPLES_README.md - JIT examples documentation\n\nModified Files:\n\nsrc/jit/jit_compiler.cpp - Updated wrapper generation for Driver API compatibility\nCMakeLists.txt - Added JIT sources to build\nbindings/bindings.cu - Added Python bindings for JITMPPIController\n\n\n\n\n\nfrom jax_mppi import cuda_mppi\nimport numpy as np\nimport os\n\n# Set include path\nos.environ['CUDA_MPPI_INCLUDE_DIR'] = '/path/to/src/cuda_mppi/include'\n\n# Configure MPPI\nconfig = cuda_mppi.MPPIConfig(\n    num_samples=1000,\n    horizon=50,\n    nx=2, nu=1,\n    lambda_=1.0,\n    dt=0.02,\n    u_scale=5.0,\n    w_action_seq_cost=0.0,\n    num_support_pts=10\n)\n\n# Define custom dynamics and cost\ndynamics_code = \"\"\"...\"\"\"  # See examples.hpp for templates\ncost_code = \"\"\"...\"\"\"\n\n# Create JIT controller (compilation happens here, ~1-5 seconds)\ncontroller = cuda_mppi.JITMPPIController(\n    config, dynamics_code, cost_code,\n    [os.environ['CUDA_MPPI_INCLUDE_DIR']]\n)\n\n# Use controller\nstate = np.array([1.0, 0.0], dtype=np.float32)\ncontroller.compute(state)\naction = controller.get_action()\ncontroller.shift()"
  },
  {
    "objectID": "plan/completed/cuda_mppi_implementation.html#references",
    "href": "plan/completed/cuda_mppi_implementation.html#references",
    "title": "CUDA/C++ MPPI Implementation Plan",
    "section": "",
    "text": "src/jax_mppi/*.py (Main reference for the MPPI Implementation)\n../MPPI-Generic (Reference for CUDA patterns)\nNVRTC Documentation"
  },
  {
    "objectID": "plan/completed/move_i_mppi_to_src.html",
    "href": "plan/completed/move_i_mppi_to_src.html",
    "title": "Move I-MPPI Modules Into src/jax_mppi",
    "section": "",
    "text": "Inspect existing I-MPPI module layout and all references.\nMove I-MPPI modules from examples/i_mppi_modules into src/jax_mppi.\nUpdate example imports to use the new package path.\nRun targeted tests/checks and verify imports."
  },
  {
    "objectID": "plan/completed/move_i_mppi_to_src.html#steps",
    "href": "plan/completed/move_i_mppi_to_src.html#steps",
    "title": "Move I-MPPI Modules Into src/jax_mppi",
    "section": "",
    "text": "Inspect existing I-MPPI module layout and all references.\nMove I-MPPI modules from examples/i_mppi_modules into src/jax_mppi.\nUpdate example imports to use the new package path.\nRun targeted tests/checks and verify imports."
  },
  {
    "objectID": "plan/performance_analysis.html",
    "href": "plan/performance_analysis.html",
    "title": "Performance Analysis of JAX-MPPI Autotuning",
    "section": "",
    "text": "This document outlines the performance bottlenecks and issues identified in the autotuning module of jax_mppi, specifically focusing on the evosax integration.\n\n\nThe primary reason why autotune_evosax.py does not achieve expected performance gains over cma (CPU-based) is a fundamental mismatch between the Autotune framework architecture and JAX‚Äôs functional programming model.\n\nCurrent Architecture: The Autotune class and TunableParameter interface rely on a shared, mutable ConfigStateHolder. The evaluate_fn is a black-box function that relies on this side-effect-laden state update mechanism.\nImpact: This prevents vmap-ing the evaluation function over a population of parameters. JAX requires pure functions to parallelize execution. Because TunableParameter.apply_parameter_value modifies the global holder in-place, it cannot be safely used within a jax.vmap or jax.lax.scan context without significant refactoring.\n\n\n\n\nIn src/jax_mppi/autotune_evosax.py, the optimize_step method performs the following loop:\n# Evaluate all solutions sequentially\nresults = []\nfitness_values = []\n\nfor x in solutions:\n    result = self.evaluate_fn(np.array(x))  # type: ignore\n    results.append(result)\n    # ...\n\nIssue: The population generated by evosax (on GPU) is iterated over in Python. Each candidate solution is converted to a NumPy array, transferred to CPU, and evaluated individually.\nConsequence: This completely negates the massive parallelization advantage of JAX. Instead of running N simulations in parallel on the GPU, they are run sequentially (or with limited batching if evaluate_fn internally batches, but typically evaluate_fn runs one configuration).\nComparison: While cma is CPU-based and expects sequential/parallel CPU evaluation, evosax is designed to run the entire ask-evaluate-tell loop on the GPU. The current implementation uses evosax only for the ‚Äúask‚Äù and ‚Äútell‚Äù steps, leaving the most expensive part (evaluation) to a slow Python loop.\n\n\n\n\nThe interface forces repeated data movement between device and host:\n\nsolutions (from es.ask) are JAX arrays on GPU.\nnp.array(x) moves individual solution vectors to CPU.\nevaluate_fn likely uses JAX internally, so it might move data back to GPU for simulation.\nResults are moved back to CPU.\nfitness_array = jnp.array(fitness_values) moves costs back to GPU for es.tell.\n\n\n\n\nevosax allows for the entire optimization process (multiple generations) to be JIT-compiled using jax.lax.scan.\n\nCurrent State: optimize_step is a Python method that cannot be JIT-compiled because it calls the Python-based evaluate_fn loop.\nUnused Code: _create_jax_evaluate_fn exists in autotune_evosax.py but is not utilized effectively to enable JAX-pure evaluation.\n\n\n\n\nThe same sequential evaluation pattern is present in src/jax_mppi/autotune_qd.py:\nfor solution in solutions:\n    result = self.evaluate_fn(solution)\n    results.append(result)\nThis limits the scalability of the QD algorithms (CMA-ME), which typically benefit from large population sizes.\n\n\n\n\nHardcoded PRNG Key: EvoSaxOptimizer.setup_optimization resets the random key to jax.random.PRNGKey(0). This forces deterministic behavior that resets on every setup call, which might not be desired if the user wants to continue optimization or run multiple independent trials.\nType Hinting: Autotune.optimize_all is typed to return EvaluationResult, but can return None if iterations=0.\nUnused Variable: self.jax_evaluate_fn in EvoSaxOptimizer is assigned but never used.\n\n\n\n\n\nRefactor TunableParameter: Create a functional interface where parameters can be applied to a config/state to produce a new config/state without side effects.\nVectorized Evaluation: Update Autotune to support a batched_evaluate_fn that accepts a batch of parameters (JAX array) and returns a batch of costs.\nJIT-compile Loop: Once evaluation is vectorized and pure, use jax.lax.scan to run the optimization loop entirely on the GPU."
  },
  {
    "objectID": "plan/performance_analysis.html#architectural-bottleneck-stateful-vs-functional",
    "href": "plan/performance_analysis.html#architectural-bottleneck-stateful-vs-functional",
    "title": "Performance Analysis of JAX-MPPI Autotuning",
    "section": "",
    "text": "The primary reason why autotune_evosax.py does not achieve expected performance gains over cma (CPU-based) is a fundamental mismatch between the Autotune framework architecture and JAX‚Äôs functional programming model.\n\nCurrent Architecture: The Autotune class and TunableParameter interface rely on a shared, mutable ConfigStateHolder. The evaluate_fn is a black-box function that relies on this side-effect-laden state update mechanism.\nImpact: This prevents vmap-ing the evaluation function over a population of parameters. JAX requires pure functions to parallelize execution. Because TunableParameter.apply_parameter_value modifies the global holder in-place, it cannot be safely used within a jax.vmap or jax.lax.scan context without significant refactoring."
  },
  {
    "objectID": "plan/performance_analysis.html#sequential-evaluation-in-evosax-optimization",
    "href": "plan/performance_analysis.html#sequential-evaluation-in-evosax-optimization",
    "title": "Performance Analysis of JAX-MPPI Autotuning",
    "section": "",
    "text": "In src/jax_mppi/autotune_evosax.py, the optimize_step method performs the following loop:\n# Evaluate all solutions sequentially\nresults = []\nfitness_values = []\n\nfor x in solutions:\n    result = self.evaluate_fn(np.array(x))  # type: ignore\n    results.append(result)\n    # ...\n\nIssue: The population generated by evosax (on GPU) is iterated over in Python. Each candidate solution is converted to a NumPy array, transferred to CPU, and evaluated individually.\nConsequence: This completely negates the massive parallelization advantage of JAX. Instead of running N simulations in parallel on the GPU, they are run sequentially (or with limited batching if evaluate_fn internally batches, but typically evaluate_fn runs one configuration).\nComparison: While cma is CPU-based and expects sequential/parallel CPU evaluation, evosax is designed to run the entire ask-evaluate-tell loop on the GPU. The current implementation uses evosax only for the ‚Äúask‚Äù and ‚Äútell‚Äù steps, leaving the most expensive part (evaluation) to a slow Python loop."
  },
  {
    "objectID": "plan/performance_analysis.html#data-transfer-overhead",
    "href": "plan/performance_analysis.html#data-transfer-overhead",
    "title": "Performance Analysis of JAX-MPPI Autotuning",
    "section": "",
    "text": "The interface forces repeated data movement between device and host:\n\nsolutions (from es.ask) are JAX arrays on GPU.\nnp.array(x) moves individual solution vectors to CPU.\nevaluate_fn likely uses JAX internally, so it might move data back to GPU for simulation.\nResults are moved back to CPU.\nfitness_array = jnp.array(fitness_values) moves costs back to GPU for es.tell."
  },
  {
    "objectID": "plan/performance_analysis.html#lack-of-end-to-end-jit-compilation",
    "href": "plan/performance_analysis.html#lack-of-end-to-end-jit-compilation",
    "title": "Performance Analysis of JAX-MPPI Autotuning",
    "section": "",
    "text": "evosax allows for the entire optimization process (multiple generations) to be JIT-compiled using jax.lax.scan.\n\nCurrent State: optimize_step is a Python method that cannot be JIT-compiled because it calls the Python-based evaluate_fn loop.\nUnused Code: _create_jax_evaluate_fn exists in autotune_evosax.py but is not utilized effectively to enable JAX-pure evaluation."
  },
  {
    "objectID": "plan/performance_analysis.html#issues-in-quality-diversity-qd-tuning",
    "href": "plan/performance_analysis.html#issues-in-quality-diversity-qd-tuning",
    "title": "Performance Analysis of JAX-MPPI Autotuning",
    "section": "",
    "text": "The same sequential evaluation pattern is present in src/jax_mppi/autotune_qd.py:\nfor solution in solutions:\n    result = self.evaluate_fn(solution)\n    results.append(result)\nThis limits the scalability of the QD algorithms (CMA-ME), which typically benefit from large population sizes."
  },
  {
    "objectID": "plan/performance_analysis.html#minor-issues",
    "href": "plan/performance_analysis.html#minor-issues",
    "title": "Performance Analysis of JAX-MPPI Autotuning",
    "section": "",
    "text": "Hardcoded PRNG Key: EvoSaxOptimizer.setup_optimization resets the random key to jax.random.PRNGKey(0). This forces deterministic behavior that resets on every setup call, which might not be desired if the user wants to continue optimization or run multiple independent trials.\nType Hinting: Autotune.optimize_all is typed to return EvaluationResult, but can return None if iterations=0.\nUnused Variable: self.jax_evaluate_fn in EvoSaxOptimizer is assigned but never used."
  },
  {
    "objectID": "plan/performance_analysis.html#recommendations-for-improvement",
    "href": "plan/performance_analysis.html#recommendations-for-improvement",
    "title": "Performance Analysis of JAX-MPPI Autotuning",
    "section": "",
    "text": "Refactor TunableParameter: Create a functional interface where parameters can be applied to a config/state to produce a new config/state without side effects.\nVectorized Evaluation: Update Autotune to support a batched_evaluate_fn that accepts a batch of parameters (JAX array) and returns a batch of costs.\nJIT-compile Loop: Once evaluation is vectorized and pure, use jax.lax.scan to run the optimization loop entirely on the GPU."
  },
  {
    "objectID": "plan/fsm_i_exploration.html",
    "href": "plan/fsm_i_exploration.html",
    "title": "FSMI Exploration Plan",
    "section": "",
    "text": "Implement the real FSMI (Fast Shannon Mutual Information) exploration logic to bias exploration toward unexplored areas, running at a lower rate than the MPPI control loop.\nLook at the docs/plan/i_mppi.qmd for the theory behind FSMI.\nThe FSMI module should run at 5 Hz while the MPPI control loop runs at 50 Hz.\n\n\n\n\nAdd FSMI state computation and scoring for exploration targets.\nIntegrate FSMI output as a bias/goal for MPPI, without changing MPPI core control rate.\nBasically the FSMI returns a score for each area to explore correlated with the centroid position of the area to explore.\nEnsure FSMI runs at a lower frequency (e.g., every N MPPI ticks or at a fixed Hz).\n\n\n\n\n\nClarify which map representation is used for ‚Äúexplored vs unexplored‚Äù (occupancy grid, TSDF, voxel grid, etc.).\nDefine the FSMI target abstraction (frontiers, waypoints, information gain hotspots).\nDefine how FSMI influences MPPI (goal override, cost shaping, trajectory prior).\n\n\n\n\n\nIdentify and document the current map/coverage representation and available signals for ‚Äúexplored.‚Äù\nDefine the FSMI state structure and scoring function for ‚Äúinformation gain‚Äù or ‚Äúunexploredness.‚Äù\nImplement FSMI target selection (frontiers or hotspots) and a target cache.\nAdd a scheduler to run FSMI at lower frequency than MPPI (e.g., every N control steps).\nIntegrate FSMI output into MPPI (cost shaping or dynamic goal update).\nAdd configuration knobs (FSMI rate, scoring weights, frontier thresholds).\nAdd minimal tests or logging to verify FSMI target updates and MPPI biasing.\n\n\n\n\n\nFSMI updates at the configured lower rate and selects targets in unexplored regions.\nMPPI behavior demonstrably biases toward FSMI targets without destabilizing control.\nConfiguration documented and example run validates exploration bias."
  },
  {
    "objectID": "plan/fsm_i_exploration.html#goal",
    "href": "plan/fsm_i_exploration.html#goal",
    "title": "FSMI Exploration Plan",
    "section": "",
    "text": "Implement the real FSMI (Fast Shannon Mutual Information) exploration logic to bias exploration toward unexplored areas, running at a lower rate than the MPPI control loop.\nLook at the docs/plan/i_mppi.qmd for the theory behind FSMI.\nThe FSMI module should run at 5 Hz while the MPPI control loop runs at 50 Hz."
  },
  {
    "objectID": "plan/fsm_i_exploration.html#scope",
    "href": "plan/fsm_i_exploration.html#scope",
    "title": "FSMI Exploration Plan",
    "section": "",
    "text": "Add FSMI state computation and scoring for exploration targets.\nIntegrate FSMI output as a bias/goal for MPPI, without changing MPPI core control rate.\nBasically the FSMI returns a score for each area to explore correlated with the centroid position of the area to explore.\nEnsure FSMI runs at a lower frequency (e.g., every N MPPI ticks or at a fixed Hz)."
  },
  {
    "objectID": "plan/fsm_i_exploration.html#assumptions-open-questions",
    "href": "plan/fsm_i_exploration.html#assumptions-open-questions",
    "title": "FSMI Exploration Plan",
    "section": "",
    "text": "Clarify which map representation is used for ‚Äúexplored vs unexplored‚Äù (occupancy grid, TSDF, voxel grid, etc.).\nDefine the FSMI target abstraction (frontiers, waypoints, information gain hotspots).\nDefine how FSMI influences MPPI (goal override, cost shaping, trajectory prior)."
  },
  {
    "objectID": "plan/fsm_i_exploration.html#plan",
    "href": "plan/fsm_i_exploration.html#plan",
    "title": "FSMI Exploration Plan",
    "section": "",
    "text": "Identify and document the current map/coverage representation and available signals for ‚Äúexplored.‚Äù\nDefine the FSMI state structure and scoring function for ‚Äúinformation gain‚Äù or ‚Äúunexploredness.‚Äù\nImplement FSMI target selection (frontiers or hotspots) and a target cache.\nAdd a scheduler to run FSMI at lower frequency than MPPI (e.g., every N control steps).\nIntegrate FSMI output into MPPI (cost shaping or dynamic goal update).\nAdd configuration knobs (FSMI rate, scoring weights, frontier thresholds).\nAdd minimal tests or logging to verify FSMI target updates and MPPI biasing."
  },
  {
    "objectID": "plan/fsm_i_exploration.html#completion-criteria",
    "href": "plan/fsm_i_exploration.html#completion-criteria",
    "title": "FSMI Exploration Plan",
    "section": "",
    "text": "FSMI updates at the configured lower rate and selects targets in unexplored regions.\nMPPI behavior demonstrably biases toward FSMI targets without destabilizing control.\nConfiguration documented and example run validates exploration bias."
  },
  {
    "objectID": "plan/issue_31_visualization_exploration.html",
    "href": "plan/issue_31_visualization_exploration.html",
    "title": "Issue #31: Refactor Visualization and Increase Exploration Incentive",
    "section": "",
    "text": "The current examples/i_mppi/i_mppi_simulation.py has two main issues:\n\nVisualization Overcrowding: 4 plots (2D, 3D, occupancy grid) with overlapping elements\nInsufficient Exploration: Robot visits first info zone then goes straight to goal\n\n\n\n\n\n\nLayer 2 (FSMI Analyzer, ~5 Hz): - info_weight=25.0 - information weight - motion_weight=0.5 - motion cost weight - Ratio: 50:1 (info:motion)\nLayer 3 (Uniform-FSMI, ~50 Hz): - info_weight=5.0 - local information weight\n\n\n\nThe issue is NOT the Layer 2/3 weights alone. Looking at environment.py:146:\ninfo_cost = -10.0 * info_gain\ntarget_cost = 1.0 * dist_target\nThe running_cost function (used for baseline) has -10.0 info multiplier. But informative_running_cost (Layer 3) has info_weight=5.0 which is relatively low.\nKey insight: After exploring Area 1, the info levels deplete (info_levels go to 0), so the controller no longer sees value in nearby zones. The reference trajectory from Layer 2 should be driving exploration, but the target_cost weight of 1.0 is too weak compared to goal attraction.\n\n\n\n\n\n\n\n\nRemove the 3D view (row 1, col 2) - not adding value\nRemove the full occupancy grid heatmap (row 2) - cluttered\nKeep single 2D trajectory plot with:\n\nWalls (gray rectangles)\nInfo zones (yellow rectangles with labels)\nStart/goal markers\nController trajectories (MPPI, KMPPI only - no SMPPI)\n\nAdd optional info zone visit indicators (checkmarks when explored)\n\nFiles to modify: - examples/i_mppi/i_mppi_simulation.py (lines 541-776)\n\n\n\n\nLayer 2 (FSMIConfig):\n\nIncrease info_weight from 25.0 to 50.0 or higher\nDecrease motion_weight from 0.5 to 0.1\nRatio target: ~500:1 (info:motion)\n\nLayer 3 (UniformFSMIConfig):\n\nIncrease info_weight from 5.0 to 15.0-20.0\n\nEnvironment costs (environment.py):\n\nReduce target_cost weight from 1.0 to 0.3-0.5\nThis allows reference trajectory (with info incentive) to override pure goal-seeking\n\n\nFiles to modify: - examples/i_mppi/i_mppi_simulation.py (lines 200-232) - src/jax_mppi/i_mppi/environment.py (line 154, optionally line 231)\n\n\n\n\nRemove SMPPI from controller list (causes issues per issue notes)\nRun only MPPI and KMPPI\nVerify both visit 2+ info zones before goal\n\nFiles to modify: - examples/i_mppi/i_mppi_simulation.py (line 509)\n\n\n\n\nRun simulation and verify:\n\nMPPI trajectory visits multiple info zones\nKMPPI trajectory visits multiple info zones\nVisualization is clean and readable\n\nUpdate output image path and verify saved correctly\n\n\n\n\n\n\nDO NOT RUN SMPPI - causes visualization/computation issues.\nRun command:\nuv run python examples/i_mppi/i_mppi_simulation.py\n\n\n\nVisualization shows 1 clean 2D plot\nBoth MPPI and KMPPI visit 2+ info zones (check final info_levels in output)\nTrajectories clearly show exploration behavior before goal convergence\n\n\n\n\n\n\nIf exploration is still insufficient:\n\n\n\n\n\n\n\n\n\nParameter\nCurrent\nTry\nEffect\n\n\n\n\nLayer 2 info_weight\n25.0\n100.0\nStronger exploration in reference\n\n\nLayer 2 motion_weight\n0.5\n0.05\nAllow longer detours\n\n\nLayer 3 info_weight\n5.0\n25.0\nStronger local info seeking\n\n\ntarget_cost (env)\n1.0\n0.2\nWeaker goal pull\n\n\n\nIf robot gets stuck or oscillates: - Reduce info_weight - Increase motion_weight - Check for collision cost inflation near walls\n\n\n\n\nAfter implementation, add brief explanation in code comments about: - Why SMPPI is excluded - Weight choices for exploration vs.¬†goal-seeking trade-off - Visualization design rationale"
  },
  {
    "objectID": "plan/issue_31_visualization_exploration.html#problem-summary",
    "href": "plan/issue_31_visualization_exploration.html#problem-summary",
    "title": "Issue #31: Refactor Visualization and Increase Exploration Incentive",
    "section": "",
    "text": "The current examples/i_mppi/i_mppi_simulation.py has two main issues:\n\nVisualization Overcrowding: 4 plots (2D, 3D, occupancy grid) with overlapping elements\nInsufficient Exploration: Robot visits first info zone then goes straight to goal"
  },
  {
    "objectID": "plan/issue_31_visualization_exploration.html#analysis",
    "href": "plan/issue_31_visualization_exploration.html#analysis",
    "title": "Issue #31: Refactor Visualization and Increase Exploration Incentive",
    "section": "",
    "text": "Layer 2 (FSMI Analyzer, ~5 Hz): - info_weight=25.0 - information weight - motion_weight=0.5 - motion cost weight - Ratio: 50:1 (info:motion)\nLayer 3 (Uniform-FSMI, ~50 Hz): - info_weight=5.0 - local information weight\n\n\n\nThe issue is NOT the Layer 2/3 weights alone. Looking at environment.py:146:\ninfo_cost = -10.0 * info_gain\ntarget_cost = 1.0 * dist_target\nThe running_cost function (used for baseline) has -10.0 info multiplier. But informative_running_cost (Layer 3) has info_weight=5.0 which is relatively low.\nKey insight: After exploring Area 1, the info levels deplete (info_levels go to 0), so the controller no longer sees value in nearby zones. The reference trajectory from Layer 2 should be driving exploration, but the target_cost weight of 1.0 is too weak compared to goal attraction."
  },
  {
    "objectID": "plan/issue_31_visualization_exploration.html#implementation-plan",
    "href": "plan/issue_31_visualization_exploration.html#implementation-plan",
    "title": "Issue #31: Refactor Visualization and Increase Exploration Incentive",
    "section": "",
    "text": "Remove the 3D view (row 1, col 2) - not adding value\nRemove the full occupancy grid heatmap (row 2) - cluttered\nKeep single 2D trajectory plot with:\n\nWalls (gray rectangles)\nInfo zones (yellow rectangles with labels)\nStart/goal markers\nController trajectories (MPPI, KMPPI only - no SMPPI)\n\nAdd optional info zone visit indicators (checkmarks when explored)\n\nFiles to modify: - examples/i_mppi/i_mppi_simulation.py (lines 541-776)\n\n\n\n\nLayer 2 (FSMIConfig):\n\nIncrease info_weight from 25.0 to 50.0 or higher\nDecrease motion_weight from 0.5 to 0.1\nRatio target: ~500:1 (info:motion)\n\nLayer 3 (UniformFSMIConfig):\n\nIncrease info_weight from 5.0 to 15.0-20.0\n\nEnvironment costs (environment.py):\n\nReduce target_cost weight from 1.0 to 0.3-0.5\nThis allows reference trajectory (with info incentive) to override pure goal-seeking\n\n\nFiles to modify: - examples/i_mppi/i_mppi_simulation.py (lines 200-232) - src/jax_mppi/i_mppi/environment.py (line 154, optionally line 231)\n\n\n\n\nRemove SMPPI from controller list (causes issues per issue notes)\nRun only MPPI and KMPPI\nVerify both visit 2+ info zones before goal\n\nFiles to modify: - examples/i_mppi/i_mppi_simulation.py (line 509)\n\n\n\n\nRun simulation and verify:\n\nMPPI trajectory visits multiple info zones\nKMPPI trajectory visits multiple info zones\nVisualization is clean and readable\n\nUpdate output image path and verify saved correctly"
  },
  {
    "objectID": "plan/issue_31_visualization_exploration.html#testing-notes",
    "href": "plan/issue_31_visualization_exploration.html#testing-notes",
    "title": "Issue #31: Refactor Visualization and Increase Exploration Incentive",
    "section": "",
    "text": "DO NOT RUN SMPPI - causes visualization/computation issues.\nRun command:\nuv run python examples/i_mppi/i_mppi_simulation.py\n\n\n\nVisualization shows 1 clean 2D plot\nBoth MPPI and KMPPI visit 2+ info zones (check final info_levels in output)\nTrajectories clearly show exploration behavior before goal convergence"
  },
  {
    "objectID": "plan/issue_31_visualization_exploration.html#weight-tuning-guide",
    "href": "plan/issue_31_visualization_exploration.html#weight-tuning-guide",
    "title": "Issue #31: Refactor Visualization and Increase Exploration Incentive",
    "section": "",
    "text": "If exploration is still insufficient:\n\n\n\n\n\n\n\n\n\nParameter\nCurrent\nTry\nEffect\n\n\n\n\nLayer 2 info_weight\n25.0\n100.0\nStronger exploration in reference\n\n\nLayer 2 motion_weight\n0.5\n0.05\nAllow longer detours\n\n\nLayer 3 info_weight\n5.0\n25.0\nStronger local info seeking\n\n\ntarget_cost (env)\n1.0\n0.2\nWeaker goal pull\n\n\n\nIf robot gets stuck or oscillates: - Reduce info_weight - Increase motion_weight - Check for collision cost inflation near walls"
  },
  {
    "objectID": "plan/issue_31_visualization_exploration.html#documentation",
    "href": "plan/issue_31_visualization_exploration.html#documentation",
    "title": "Issue #31: Refactor Visualization and Increase Exploration Incentive",
    "section": "",
    "text": "After implementation, add brief explanation in code comments about: - Why SMPPI is excluded - Weight choices for exploration vs.¬†goal-seeking trade-off - Visualization design rationale"
  },
  {
    "objectID": "testing.html",
    "href": "testing.html",
    "title": "Testing Guide",
    "section": "",
    "text": "This guide explains the testing stack for jax_mppi and provides instructions on how to run and write tests.\n\n\nThe project uses pytest for running tests. You can run all tests using uv:\nuv run pytest\nTo run a specific test file:\nuv run pytest tests/test_mppi.py\nTo run a specific test case:\nuv run pytest tests/test_mppi.py::TestMPPICommand::test_command_returns_correct_shapes\n\n\n\nThe tests are located in the tests/ directory and mirror the source code structure where appropriate. The test suite is divided into several files, each covering a specific flavor or aspect of the library.\n\n\n\ntests/test_mppi.py: Tests for the base MPPI implementation (jax_mppi.mppi).\n\nGoal: Ensure the correctness of the core algorithm, state management, and configuration options.\nScope:\n\nInitialization: Verifies that create() returns correct shapes and types for config and state.\nCommand Generation: Tests the command() function to ensure it generates valid actions within bounds and correctly updates the state.\nConfiguration Options: Validates various settings like u_per_command (multi-step control), step_dependent_dynamics (time-varying systems), sample_null_action (ensuring baseline inclusion), and u_scale (control authority scaling).\nIntegration: Includes basic convergence tests to verify that the cost decreases over iterations (e.g., TestMPPIIntegration).\n\n\ntests/test_smppi.py: Tests for Smooth MPPI (jax_mppi.smppi).\n\nGoal: Verify that the ‚Äúsmooth‚Äù variant correctly operates in the lifted velocity control space and produces continuous action sequences.\nScope:\n\nLifted Space: Checks that the internal state (U) represents control velocity/acceleration, while action_sequence represents the integrated actions.\nSmoothness: Verifies that the smoothness cost penalty (w_action_seq_cost) effectively reduces action variance.\nBounds: Tests that bounds are respected for both the control velocity (u_min/u_max) and the final action (action_min/action_max).\nContinuity: checks that the shift operation maintains continuity in the action space, preventing jumps during receding horizon updates.\n\n\ntests/test_kmppi.py: Tests for Kernel MPPI (jax_mppi.kmppi).\n\nGoal: Ensure that kernel-based interpolation works correctly and that optimization occurs effectively in the reduced control point space.\nScope:\n\nKernels: Tests the properties of time-domain kernels (e.g., RBFKernel), such as shape and distance decay.\nInterpolation: Verifies that control points (theta) are correctly mapped to full trajectories (U) via _kernel_interpolate, preserving values at control points.\nOptimization: Checks that the MPPI update rule is applied to the control points (theta) rather than the full trajectory.\nSmoothness: Confirms that the resulting trajectories are smooth due to the kernel properties (e.g., by checking second derivatives).\n\n\n\n\n\n\n\ntests/test_pendulum.py: End-to-end integration tests using a Pendulum environment.\n\nGoal: Validate that the algorithms can solve a concrete, non-linear control task.\nScope:\n\nStabilization: Tests if MPPI can stabilize the pendulum at the upright position.\nSwing-up: Tests the more difficult task of swinging up from a hanging position.\nPhysics: Sanity checks the pendulum dynamics and cost functions.\n\n\n\n\n\n\n\ntests/test_autotune.py: Unit tests for the autotuning framework (jax_mppi.autotune).\n\nGoal: Verify the components of the hyperparameter optimization system.\n\ntests/test_autotune_integration.py: Integration tests for autotuning.\n\nGoal: Ensure that the autotuner can successfully improve performance on a benchmark task (finding better parameters than the default).\n\n\n\n\n\n\nWhen adding new features or fixing bugs, please add corresponding tests.\n\nLocate the appropriate test file: If you are modifying mppi.py, add tests to tests/test_mppi.py.\nUse Class-Based Structure: Group related tests into classes (e.g., TestMPPIBasics, TestMPPICommand).\nProperty-Based Testing: Where possible, test properties (e.g., ‚Äúoutput shape depends on input shape in this way‚Äù) rather than just hardcoded values.\nIntegration Tests: For significant algorithmic changes, ensure that tests/test_pendulum.py still passes or add a similar simple control task to verify efficacy.\nJAX Compatibility: Ensure tests check that functions can be JIT-compiled if they are intended to be used within jax.jit.\n\n\n\ndef test_new_feature(self):\n    nx, nu = 2, 1\n    config, state = mppi.create(nx=nx, nu=nu, noise_sigma=jnp.eye(nu))\n\n    # ... perform action ...\n    action, new_state = mppi.command(config, state, ...)\n\n    # ... assert expected behavior ...\n    assert action.shape == (nu,)",
    "crumbs": [
      "Home",
      "Development",
      "Testing Guide"
    ]
  },
  {
    "objectID": "testing.html#running-tests",
    "href": "testing.html#running-tests",
    "title": "Testing Guide",
    "section": "",
    "text": "The project uses pytest for running tests. You can run all tests using uv:\nuv run pytest\nTo run a specific test file:\nuv run pytest tests/test_mppi.py\nTo run a specific test case:\nuv run pytest tests/test_mppi.py::TestMPPICommand::test_command_returns_correct_shapes",
    "crumbs": [
      "Home",
      "Development",
      "Testing Guide"
    ]
  },
  {
    "objectID": "testing.html#test-suite-structure",
    "href": "testing.html#test-suite-structure",
    "title": "Testing Guide",
    "section": "",
    "text": "The tests are located in the tests/ directory and mirror the source code structure where appropriate. The test suite is divided into several files, each covering a specific flavor or aspect of the library.\n\n\n\ntests/test_mppi.py: Tests for the base MPPI implementation (jax_mppi.mppi).\n\nGoal: Ensure the correctness of the core algorithm, state management, and configuration options.\nScope:\n\nInitialization: Verifies that create() returns correct shapes and types for config and state.\nCommand Generation: Tests the command() function to ensure it generates valid actions within bounds and correctly updates the state.\nConfiguration Options: Validates various settings like u_per_command (multi-step control), step_dependent_dynamics (time-varying systems), sample_null_action (ensuring baseline inclusion), and u_scale (control authority scaling).\nIntegration: Includes basic convergence tests to verify that the cost decreases over iterations (e.g., TestMPPIIntegration).\n\n\ntests/test_smppi.py: Tests for Smooth MPPI (jax_mppi.smppi).\n\nGoal: Verify that the ‚Äúsmooth‚Äù variant correctly operates in the lifted velocity control space and produces continuous action sequences.\nScope:\n\nLifted Space: Checks that the internal state (U) represents control velocity/acceleration, while action_sequence represents the integrated actions.\nSmoothness: Verifies that the smoothness cost penalty (w_action_seq_cost) effectively reduces action variance.\nBounds: Tests that bounds are respected for both the control velocity (u_min/u_max) and the final action (action_min/action_max).\nContinuity: checks that the shift operation maintains continuity in the action space, preventing jumps during receding horizon updates.\n\n\ntests/test_kmppi.py: Tests for Kernel MPPI (jax_mppi.kmppi).\n\nGoal: Ensure that kernel-based interpolation works correctly and that optimization occurs effectively in the reduced control point space.\nScope:\n\nKernels: Tests the properties of time-domain kernels (e.g., RBFKernel), such as shape and distance decay.\nInterpolation: Verifies that control points (theta) are correctly mapped to full trajectories (U) via _kernel_interpolate, preserving values at control points.\nOptimization: Checks that the MPPI update rule is applied to the control points (theta) rather than the full trajectory.\nSmoothness: Confirms that the resulting trajectories are smooth due to the kernel properties (e.g., by checking second derivatives).\n\n\n\n\n\n\n\ntests/test_pendulum.py: End-to-end integration tests using a Pendulum environment.\n\nGoal: Validate that the algorithms can solve a concrete, non-linear control task.\nScope:\n\nStabilization: Tests if MPPI can stabilize the pendulum at the upright position.\nSwing-up: Tests the more difficult task of swinging up from a hanging position.\nPhysics: Sanity checks the pendulum dynamics and cost functions.\n\n\n\n\n\n\n\ntests/test_autotune.py: Unit tests for the autotuning framework (jax_mppi.autotune).\n\nGoal: Verify the components of the hyperparameter optimization system.\n\ntests/test_autotune_integration.py: Integration tests for autotuning.\n\nGoal: Ensure that the autotuner can successfully improve performance on a benchmark task (finding better parameters than the default).",
    "crumbs": [
      "Home",
      "Development",
      "Testing Guide"
    ]
  },
  {
    "objectID": "testing.html#writing-new-tests",
    "href": "testing.html#writing-new-tests",
    "title": "Testing Guide",
    "section": "",
    "text": "When adding new features or fixing bugs, please add corresponding tests.\n\nLocate the appropriate test file: If you are modifying mppi.py, add tests to tests/test_mppi.py.\nUse Class-Based Structure: Group related tests into classes (e.g., TestMPPIBasics, TestMPPICommand).\nProperty-Based Testing: Where possible, test properties (e.g., ‚Äúoutput shape depends on input shape in this way‚Äù) rather than just hardcoded values.\nIntegration Tests: For significant algorithmic changes, ensure that tests/test_pendulum.py still passes or add a similar simple control task to verify efficacy.\nJAX Compatibility: Ensure tests check that functions can be JIT-compiled if they are intended to be used within jax.jit.\n\n\n\ndef test_new_feature(self):\n    nx, nu = 2, 1\n    config, state = mppi.create(nx=nx, nu=nu, noise_sigma=jnp.eye(nu))\n\n    # ... perform action ...\n    action, new_state = mppi.command(config, state, ...)\n\n    # ... assert expected behavior ...\n    assert action.shape == (nu,)",
    "crumbs": [
      "Home",
      "Development",
      "Testing Guide"
    ]
  },
  {
    "objectID": "releasing.html",
    "href": "releasing.html",
    "title": "Releasing Guide",
    "section": "",
    "text": "This guide describes how to release a new version of jax_mppi to PyPI.\n\n\nThe release process is automated using GitHub Actions, but it requires the repository to be configured as a Trusted Publisher on PyPI.\n\n\n\nLog in to your PyPI account.\nGo to Publishing in your project settings (or create a new project if this is the first release).\nAdd a new Trusted Publisher.\nSelect GitHub.\nEnter the following details:\n\nOwner: riccardo-enr\nRepository name: jax_mppi\nWorkflow name: publish.yml\nEnvironment name: (Leave empty)\n\nClick Add.\n\nThis allows the GitHub Action to authenticate with PyPI using OIDC tokens without needing a long-lived API token or password.\n\n\n\n\nTo release a new version:\n\nUpdate Version: Update the version number in pyproject.toml:\n[project]\nversion = \"0.1.6\"  # Example version\nCommit and Push: Commit the version change and push to main.\ngit add pyproject.toml\ngit commit -m \"Bump version to 0.1.6\"\ngit push origin main\nCreate Tag: Create a git tag for the release. The tag must start with v.\ngit tag v0.1.6\ngit push origin v0.1.6\nWait for Action: The Publish to PyPI GitHub Action will automatically run when the tag is pushed. It will build the package and upload it to PyPI.\nVerify: Check the PyPI page to confirm the new version is available.",
    "crumbs": [
      "Home",
      "Development",
      "Releasing Guide"
    ]
  },
  {
    "objectID": "releasing.html#prerequisites",
    "href": "releasing.html#prerequisites",
    "title": "Releasing Guide",
    "section": "",
    "text": "The release process is automated using GitHub Actions, but it requires the repository to be configured as a Trusted Publisher on PyPI.\n\n\n\nLog in to your PyPI account.\nGo to Publishing in your project settings (or create a new project if this is the first release).\nAdd a new Trusted Publisher.\nSelect GitHub.\nEnter the following details:\n\nOwner: riccardo-enr\nRepository name: jax_mppi\nWorkflow name: publish.yml\nEnvironment name: (Leave empty)\n\nClick Add.\n\nThis allows the GitHub Action to authenticate with PyPI using OIDC tokens without needing a long-lived API token or password.",
    "crumbs": [
      "Home",
      "Development",
      "Releasing Guide"
    ]
  },
  {
    "objectID": "releasing.html#release-process",
    "href": "releasing.html#release-process",
    "title": "Releasing Guide",
    "section": "",
    "text": "To release a new version:\n\nUpdate Version: Update the version number in pyproject.toml:\n[project]\nversion = \"0.1.6\"  # Example version\nCommit and Push: Commit the version change and push to main.\ngit add pyproject.toml\ngit commit -m \"Bump version to 0.1.6\"\ngit push origin main\nCreate Tag: Create a git tag for the release. The tag must start with v.\ngit tag v0.1.6\ngit push origin v0.1.6\nWait for Action: The Publish to PyPI GitHub Action will automatically run when the tag is pushed. It will build the package and upload it to PyPI.\nVerify: Check the PyPI page to confirm the new version is available.",
    "crumbs": [
      "Home",
      "Development",
      "Releasing Guide"
    ]
  },
  {
    "objectID": "examples/pendulum.html",
    "href": "examples/pendulum.html",
    "title": "Pendulum Swing-Up",
    "section": "",
    "text": "This example demonstrates how to use jax_mppi to control an inverted pendulum. The goal is to swing the pendulum up from a hanging position and stabilize it at the top.\n\n\nThe full example code is available in examples/basic/pendulum.py.\n\n\n\nThe pendulum dynamics are defined as a pure function:\ndef pendulum_dynamics(state: jax.Array, action: jax.Array) -&gt; jax.Array:\n    \"\"\"Pendulum dynamics.\n\n    State: [theta, theta_dot]\n        theta: angle from upright (0 = upright, pi = hanging down)\n        theta_dot: angular velocity\n    Action: [torque]\n        torque: applied torque (control input)\n    \"\"\"\n    g = 10.0  # gravity\n    m = 1.0  # mass\n    l = 1.0  # length\n    dt = 0.05  # timestep\n\n    theta, theta_dot = state[0], state[1]\n    torque = action[0]\n\n    # Clip torque to reasonable bounds\n    torque = jnp.clip(torque, -2.0, 2.0)\n\n    # Pendulum dynamics: theta_ddot = (torque - m*g*l*sin(theta)) / (m*l^2)\n    theta_ddot = (torque - m * g * l * jnp.sin(theta)) / (m * l * l)\n\n    # Euler integration\n    theta_dot_next = theta_dot + theta_ddot * dt\n    theta_next = theta + theta_dot_next * dt\n\n    # Normalize angle to [-pi, pi]\n    theta_next = ((theta_next + jnp.pi) % (2 * jnp.pi)) - jnp.pi\n\n    return jnp.array([theta_next, theta_dot_next])\n\n\n\nThe running cost penalizes deviation from the upright position and high control effort:\ndef pendulum_cost(state: jax.Array, action: jax.Array) -&gt; jax.Array:\n    theta, theta_dot = state[0], state[1]\n    torque = action[0]\n\n    # Cost for being away from upright (theta=0)\n    angle_cost = theta**2\n\n    # Cost for high angular velocity\n    velocity_cost = 0.1 * theta_dot**2\n\n    # Cost for using torque\n    control_cost = 0.01 * torque**2\n\n    return angle_cost + velocity_cost + control_cost\n\n\n\nYou can run the example using:\npython examples/basic/pendulum.py --visualize",
    "crumbs": [
      "Home",
      "Examples",
      "Pendulum Swing-Up"
    ]
  },
  {
    "objectID": "examples/pendulum.html#code",
    "href": "examples/pendulum.html#code",
    "title": "Pendulum Swing-Up",
    "section": "",
    "text": "The full example code is available in examples/basic/pendulum.py.",
    "crumbs": [
      "Home",
      "Examples",
      "Pendulum Swing-Up"
    ]
  },
  {
    "objectID": "examples/pendulum.html#dynamics",
    "href": "examples/pendulum.html#dynamics",
    "title": "Pendulum Swing-Up",
    "section": "",
    "text": "The pendulum dynamics are defined as a pure function:\ndef pendulum_dynamics(state: jax.Array, action: jax.Array) -&gt; jax.Array:\n    \"\"\"Pendulum dynamics.\n\n    State: [theta, theta_dot]\n        theta: angle from upright (0 = upright, pi = hanging down)\n        theta_dot: angular velocity\n    Action: [torque]\n        torque: applied torque (control input)\n    \"\"\"\n    g = 10.0  # gravity\n    m = 1.0  # mass\n    l = 1.0  # length\n    dt = 0.05  # timestep\n\n    theta, theta_dot = state[0], state[1]\n    torque = action[0]\n\n    # Clip torque to reasonable bounds\n    torque = jnp.clip(torque, -2.0, 2.0)\n\n    # Pendulum dynamics: theta_ddot = (torque - m*g*l*sin(theta)) / (m*l^2)\n    theta_ddot = (torque - m * g * l * jnp.sin(theta)) / (m * l * l)\n\n    # Euler integration\n    theta_dot_next = theta_dot + theta_ddot * dt\n    theta_next = theta + theta_dot_next * dt\n\n    # Normalize angle to [-pi, pi]\n    theta_next = ((theta_next + jnp.pi) % (2 * jnp.pi)) - jnp.pi\n\n    return jnp.array([theta_next, theta_dot_next])",
    "crumbs": [
      "Home",
      "Examples",
      "Pendulum Swing-Up"
    ]
  },
  {
    "objectID": "examples/pendulum.html#cost-function",
    "href": "examples/pendulum.html#cost-function",
    "title": "Pendulum Swing-Up",
    "section": "",
    "text": "The running cost penalizes deviation from the upright position and high control effort:\ndef pendulum_cost(state: jax.Array, action: jax.Array) -&gt; jax.Array:\n    theta, theta_dot = state[0], state[1]\n    torque = action[0]\n\n    # Cost for being away from upright (theta=0)\n    angle_cost = theta**2\n\n    # Cost for high angular velocity\n    velocity_cost = 0.1 * theta_dot**2\n\n    # Cost for using torque\n    control_cost = 0.01 * torque**2\n\n    return angle_cost + velocity_cost + control_cost",
    "crumbs": [
      "Home",
      "Examples",
      "Pendulum Swing-Up"
    ]
  },
  {
    "objectID": "examples/pendulum.html#running-the-example",
    "href": "examples/pendulum.html#running-the-example",
    "title": "Pendulum Swing-Up",
    "section": "",
    "text": "You can run the example using:\npython examples/basic/pendulum.py --visualize",
    "crumbs": [
      "Home",
      "Examples",
      "Pendulum Swing-Up"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "jax_mppi",
    "section": "",
    "text": "Open In Colab\n\n\njax_mppi is a functional, JIT-compilable port of the pytorch_mppi library to JAX. It implements Model Predictive Path Integral (MPPI) control with a focus on performance and composability.\n\n\nThis library embraces JAX‚Äôs functional paradigm:\n\nPure Functions: Core logic is implemented as pure functions command(state, mppi_state) -&gt; (action, mppi_state).\nDataclass State: State is held in jax.tree_util.register_dataclass containers, allowing easy integration with jit, vmap, and grad.\nNo Side Effects: Unlike the PyTorch version, there is no mutable self. State transitions are explicit.\n\n\n\n\n\nCore MPPI: Robust implementation of the standard MPPI algorithm.\nSmooth MPPI (SMPPI): Maintains action sequences and smoothness costs for better trajectory generation.\nKernel MPPI (KMPPI): Uses kernel interpolation for control points, reducing the parameter space.\nAutotuning: Built-in hyperparameter optimization using CMA-ES, Ray Tune, and Quality Diversity.\nCUDA/C++ Backend: High-performance implementations of all controllers in CUDA/C++17, exposed to Python via `nanobind`.\nJAX Integration:\n\njax.vmap for efficient batch processing.\njax.lax.scan for fast horizon loops.\nFully compatible with JIT compilation for high-performance control loops.\n\n\n\n\n\n# Clone the repository\ngit clone https://github.com/yourusername/jax_mppi.git\ncd jax_mppi\n\n# Install dependencies\npip install -e .\n\n\n\nimport jax\nimport jax.numpy as jnp\nfrom jax_mppi import mppi\n\n# Define dynamics and cost functions\ndef dynamics(state, action):\n    # Your dynamics model here\n    return state + action\n\ndef running_cost(state, action):\n    # Your cost function here\n    return jnp.sum(state**2) + jnp.sum(action**2)\n\n# Create configuration and initial state\nconfig, mppi_state = mppi.create(\n    nx=4, nu=2,\n    noise_sigma=jnp.eye(2) * 0.1,\n    horizon=20,\n    lambda_=1.0\n)\n\n# Control loop\nkey = jax.random.PRNGKey(0)\ncurrent_obs = jnp.zeros(4)\n\n# JIT compile the command function for performance\njitted_command = jax.jit(mppi.command, static_argnames=['dynamics', 'running_cost'])\n\nfor _ in range(100):\n    key, subkey = jax.random.split(key)\n    action, mppi_state = jitted_command(\n        config,\n        mppi_state,\n        current_obs,\n        dynamics=dynamics,\n        running_cost=running_cost\n    )\n    # Apply action to environment...\n\n\n\njax_mppi/\n‚îú‚îÄ‚îÄ src/jax_mppi/\n‚îÇ   ‚îú‚îÄ‚îÄ mppi.py              # Core MPPI implementation\n‚îÇ   ‚îú‚îÄ‚îÄ smppi.py             # Smooth MPPI variant\n‚îÇ   ‚îú‚îÄ‚îÄ kmppi.py             # Kernel MPPI variant\n‚îÇ   ‚îú‚îÄ‚îÄ types.py             # Type definitions\n‚îÇ   ‚îú‚îÄ‚îÄ autotune.py          # Autotuning core & CMA-ES\n‚îÇ   ‚îú‚îÄ‚îÄ autotune_global.py   # Ray Tune integration\n‚îÇ   ‚îî‚îÄ‚îÄ autotune_qd.py       # Quality Diversity optimization\n‚îú‚îÄ‚îÄ examples/\n‚îÇ   ‚îú‚îÄ‚îÄ pendulum.py          # Pendulum environment example\n‚îÇ   ‚îú‚îÄ‚îÄ autotune_basic.py    # Basic autotuning example\n‚îÇ   ‚îú‚îÄ‚îÄ autotune_pendulum.py # Autotuning pendulum\n‚îÇ   ‚îî‚îÄ‚îÄ smooth_comparison.py # Comparison of MPPI variants\n‚îî‚îÄ‚îÄ tests/                   # Unit and integration tests\n\n\n\nThe development is structured in phases:\n\nCore MPPI: Basic implementation with JAX parity.\nIntegration: Pendulum example and verification.\nSmooth MPPI: Implementation of smoothness constraints.\nKernel MPPI: Kernel-based control parameterization.\nComparisons: Benchmarking and visual comparisons.\nAutotuning: Parameter optimization using CMA-ES, Ray Tune, and QD.\n\n\n\n\nThis project is a direct port of pytorch_mppi. We aim to maintain parity with the original implementation while leveraging JAX‚Äôs unique features for performance and flexibility.\n\n\n\n\nPorting Plan\nEvosax Integration Plan\nCUDA MPPI Implementation Plan",
    "crumbs": [
      "Home",
      "Introduction",
      "jax_mppi"
    ]
  },
  {
    "objectID": "index.html#design-philosophy",
    "href": "index.html#design-philosophy",
    "title": "jax_mppi",
    "section": "",
    "text": "This library embraces JAX‚Äôs functional paradigm:\n\nPure Functions: Core logic is implemented as pure functions command(state, mppi_state) -&gt; (action, mppi_state).\nDataclass State: State is held in jax.tree_util.register_dataclass containers, allowing easy integration with jit, vmap, and grad.\nNo Side Effects: Unlike the PyTorch version, there is no mutable self. State transitions are explicit.",
    "crumbs": [
      "Home",
      "Introduction",
      "jax_mppi"
    ]
  },
  {
    "objectID": "index.html#key-features",
    "href": "index.html#key-features",
    "title": "jax_mppi",
    "section": "",
    "text": "Core MPPI: Robust implementation of the standard MPPI algorithm.\nSmooth MPPI (SMPPI): Maintains action sequences and smoothness costs for better trajectory generation.\nKernel MPPI (KMPPI): Uses kernel interpolation for control points, reducing the parameter space.\nAutotuning: Built-in hyperparameter optimization using CMA-ES, Ray Tune, and Quality Diversity.\nCUDA/C++ Backend: High-performance implementations of all controllers in CUDA/C++17, exposed to Python via `nanobind`.\nJAX Integration:\n\njax.vmap for efficient batch processing.\njax.lax.scan for fast horizon loops.\nFully compatible with JIT compilation for high-performance control loops.",
    "crumbs": [
      "Home",
      "Introduction",
      "jax_mppi"
    ]
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "jax_mppi",
    "section": "",
    "text": "# Clone the repository\ngit clone https://github.com/yourusername/jax_mppi.git\ncd jax_mppi\n\n# Install dependencies\npip install -e .",
    "crumbs": [
      "Home",
      "Introduction",
      "jax_mppi"
    ]
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "jax_mppi",
    "section": "",
    "text": "import jax\nimport jax.numpy as jnp\nfrom jax_mppi import mppi\n\n# Define dynamics and cost functions\ndef dynamics(state, action):\n    # Your dynamics model here\n    return state + action\n\ndef running_cost(state, action):\n    # Your cost function here\n    return jnp.sum(state**2) + jnp.sum(action**2)\n\n# Create configuration and initial state\nconfig, mppi_state = mppi.create(\n    nx=4, nu=2,\n    noise_sigma=jnp.eye(2) * 0.1,\n    horizon=20,\n    lambda_=1.0\n)\n\n# Control loop\nkey = jax.random.PRNGKey(0)\ncurrent_obs = jnp.zeros(4)\n\n# JIT compile the command function for performance\njitted_command = jax.jit(mppi.command, static_argnames=['dynamics', 'running_cost'])\n\nfor _ in range(100):\n    key, subkey = jax.random.split(key)\n    action, mppi_state = jitted_command(\n        config,\n        mppi_state,\n        current_obs,\n        dynamics=dynamics,\n        running_cost=running_cost\n    )\n    # Apply action to environment...",
    "crumbs": [
      "Home",
      "Introduction",
      "jax_mppi"
    ]
  },
  {
    "objectID": "index.html#project-structure",
    "href": "index.html#project-structure",
    "title": "jax_mppi",
    "section": "",
    "text": "jax_mppi/\n‚îú‚îÄ‚îÄ src/jax_mppi/\n‚îÇ   ‚îú‚îÄ‚îÄ mppi.py              # Core MPPI implementation\n‚îÇ   ‚îú‚îÄ‚îÄ smppi.py             # Smooth MPPI variant\n‚îÇ   ‚îú‚îÄ‚îÄ kmppi.py             # Kernel MPPI variant\n‚îÇ   ‚îú‚îÄ‚îÄ types.py             # Type definitions\n‚îÇ   ‚îú‚îÄ‚îÄ autotune.py          # Autotuning core & CMA-ES\n‚îÇ   ‚îú‚îÄ‚îÄ autotune_global.py   # Ray Tune integration\n‚îÇ   ‚îî‚îÄ‚îÄ autotune_qd.py       # Quality Diversity optimization\n‚îú‚îÄ‚îÄ examples/\n‚îÇ   ‚îú‚îÄ‚îÄ pendulum.py          # Pendulum environment example\n‚îÇ   ‚îú‚îÄ‚îÄ autotune_basic.py    # Basic autotuning example\n‚îÇ   ‚îú‚îÄ‚îÄ autotune_pendulum.py # Autotuning pendulum\n‚îÇ   ‚îî‚îÄ‚îÄ smooth_comparison.py # Comparison of MPPI variants\n‚îî‚îÄ‚îÄ tests/                   # Unit and integration tests",
    "crumbs": [
      "Home",
      "Introduction",
      "jax_mppi"
    ]
  },
  {
    "objectID": "index.html#roadmap",
    "href": "index.html#roadmap",
    "title": "jax_mppi",
    "section": "",
    "text": "The development is structured in phases:\n\nCore MPPI: Basic implementation with JAX parity.\nIntegration: Pendulum example and verification.\nSmooth MPPI: Implementation of smoothness constraints.\nKernel MPPI: Kernel-based control parameterization.\nComparisons: Benchmarking and visual comparisons.\nAutotuning: Parameter optimization using CMA-ES, Ray Tune, and QD.",
    "crumbs": [
      "Home",
      "Introduction",
      "jax_mppi"
    ]
  },
  {
    "objectID": "index.html#credits",
    "href": "index.html#credits",
    "title": "jax_mppi",
    "section": "",
    "text": "This project is a direct port of pytorch_mppi. We aim to maintain parity with the original implementation while leveraging JAX‚Äôs unique features for performance and flexibility.",
    "crumbs": [
      "Home",
      "Introduction",
      "jax_mppi"
    ]
  },
  {
    "objectID": "index.html#completed-plans",
    "href": "index.html#completed-plans",
    "title": "jax_mppi",
    "section": "",
    "text": "Porting Plan\nEvosax Integration Plan\nCUDA MPPI Implementation Plan",
    "crumbs": [
      "Home",
      "Introduction",
      "jax_mppi"
    ]
  },
  {
    "objectID": "plan/quadrotor_trajectory_following.html",
    "href": "plan/quadrotor_trajectory_following.html",
    "title": "Quadrotor Trajectory Following with MPPI",
    "section": "",
    "text": "Status: ‚úÖ Complete (Phases 1-4) Branch: feat/quadrotor-traj-foll-example Created: 2026-02-01 Completed: 2026-02-02\n\n\nImplement a comprehensive set of examples demonstrating quadrotor trajectory following using MPPI control. Showcases the JAX-MPPI library‚Äôs capabilities on a realistic robotic system with nonlinear dynamics.\n\n\n\nThe current JAX-MPPI library includes three MPPI variants (standard, SMPPI, KMPPI), examples (pendulum, 2D navigation), and autotuning infrastructure. A quadrotor trajectory following example demonstrates MPPI on a high-dimensional nonlinear system (13D state space) with realistic robotics benchmarks.\n\n\n\nState Space (13D): Position (3D), velocity (3D), orientation (quaternion, 4D), angular velocity (3D) - Frame: NED (North-East-Down) world frame, FRD (Forward-Right-Down) body frame\nControl Input (4D): Total thrust + body angular rates (roll, pitch, yaw rates)\nDynamics: 6-DOF rigid body with quaternion-based attitude representation - For detailed mathematical formulations, see docs/src/dynamics.qmd\nCost Function: Position/velocity tracking error + control effort penalty\n\n\n\n\n\nCompleted Components: - Quadrotor dynamics module (src/jax_mppi/dynamics/quadrotor.py) - Quaternion utilities and kinematics - 6-DOF dynamics with RK4 integration - Unit tests for quaternion norm preservation and energy conservation - Trajectory cost functions (src/jax_mppi/costs/quadrotor.py) - Position/velocity tracking, attitude tracking, terminal costs - Unit tests for all cost functions\n\n\n\nCompleted: - Trajectory generation utilities (examples/quadrotor/trajectories.py) - Circular, figure-8, hover, helix trajectories - Waypoint interpolation with cubic Hermite splines - Trajectory metrics computation - 28 unit tests (all passing)\n\n\n\nExample 1: quadrotor_hover.py - Stabilization around fixed setpoint - Visualization of state evolution, performance metrics (settling time, overshoot)\nExample 2: quadrotor_circle.py - Circular trajectory tracking - Tracking error visualization, control inputs, 3D trajectory plots\nIntegration Tests: 11 tests covering both examples\n\n\n\nExample 3: quadrotor_figure8_comparison.py - MPPI variant comparison - Side-by-side comparison of MPPI, SMPPI, and KMPPI on aggressive figure-8 - Comprehensive metrics: tracking accuracy, control smoothness, energy consumption - 6-subplot visualization with performance comparison table - Demonstrates SMPPI produces smoother control (lower jerk)\nExample 4: quadrotor_custom_trajectory.py - Waypoint following - User-defined waypoint trajectories with smooth interpolation - Command-line interface for custom waypoint specification - Default square pattern demonstration\nIntegration Tests: 13 tests for advanced examples\n\n\n\n\nImplementation plan (this document)\nREADME for quadrotor examples\nUpdate main README with quadrotor section\n\n\n\n\n\n‚úÖ 6-DOF quadrotor dynamics with quaternion representation ‚úÖ Multiple reference trajectory types (circle, figure-8, hover, custom waypoints) ‚úÖ Comprehensive cost functions for trajectory tracking ‚úÖ Four complete examples demonstrating different capabilities ‚úÖ 50+ unit and integration tests (all passing) ‚úÖ Publication-quality visualizations saved to docs/_media/quadrotor/ ‚úÖ 50 Hz control rate (JIT-compiled) for real-time performance ‚úÖ Proper NED/FRD frame conventions throughout\n\n\n\njax_mppi/\n‚îú‚îÄ‚îÄ src/jax_mppi/\n‚îÇ   ‚îú‚îÄ‚îÄ dynamics/quadrotor.py      # 6-DOF dynamics with quaternions\n‚îÇ   ‚îî‚îÄ‚îÄ costs/quadrotor.py         # Trajectory tracking costs\n‚îú‚îÄ‚îÄ examples/quadrotor/\n‚îÇ   ‚îú‚îÄ‚îÄ trajectories.py            # Trajectory generation utilities\n‚îÇ   ‚îú‚îÄ‚îÄ quadrotor_hover.py         # Example 1: Hover control\n‚îÇ   ‚îú‚îÄ‚îÄ quadrotor_circle.py        # Example 2: Circle following\n‚îÇ   ‚îú‚îÄ‚îÄ quadrotor_figure8_comparison.py  # Example 3: MPPI comparison\n‚îÇ   ‚îî‚îÄ‚îÄ quadrotor_custom_trajectory.py   # Example 4: Waypoint following\n‚îú‚îÄ‚îÄ tests/\n‚îÇ   ‚îú‚îÄ‚îÄ test_quadrotor_dynamics.py\n‚îÇ   ‚îú‚îÄ‚îÄ test_quadrotor_costs.py\n‚îÇ   ‚îî‚îÄ‚îÄ test_quadrotor_trajectories.py\n‚îî‚îÄ‚îÄ docs/_media/quadrotor/\n    ‚îú‚îÄ‚îÄ quadrotor_hover_mppi.png\n    ‚îú‚îÄ‚îÄ quadrotor_circle_mppi.png\n    ‚îú‚îÄ‚îÄ quadrotor_hover_comparison.png\n    ‚îî‚îÄ‚îÄ quadrotor_figure8_comparison.png\n\n\n\n\nControl Rate: 50 Hz (JIT-compiled)\nTracking Accuracy: &lt;0.1m RMS error on circle trajectory\nSmoothness: SMPPI shows 30-40% lower jerk than standard MPPI\nEnergy: Comparable across all three variants for same tracking performance\n\n\n\n\n\nObstacle avoidance during trajectory following\nFull Euler dynamics for rotational motion\nWind disturbance modeling\nAutotuning example for quadrotor hyperparameters\nReal-time animated visualization\n\n\nLast Updated: 2026-02-02 Status: Implementation complete, documentation in progress"
  },
  {
    "objectID": "plan/quadrotor_trajectory_following.html#objective",
    "href": "plan/quadrotor_trajectory_following.html#objective",
    "title": "Quadrotor Trajectory Following with MPPI",
    "section": "",
    "text": "Implement a comprehensive set of examples demonstrating quadrotor trajectory following using MPPI control. Showcases the JAX-MPPI library‚Äôs capabilities on a realistic robotic system with nonlinear dynamics."
  },
  {
    "objectID": "plan/quadrotor_trajectory_following.html#background",
    "href": "plan/quadrotor_trajectory_following.html#background",
    "title": "Quadrotor Trajectory Following with MPPI",
    "section": "",
    "text": "The current JAX-MPPI library includes three MPPI variants (standard, SMPPI, KMPPI), examples (pendulum, 2D navigation), and autotuning infrastructure. A quadrotor trajectory following example demonstrates MPPI on a high-dimensional nonlinear system (13D state space) with realistic robotics benchmarks."
  },
  {
    "objectID": "plan/quadrotor_trajectory_following.html#system-overview",
    "href": "plan/quadrotor_trajectory_following.html#system-overview",
    "title": "Quadrotor Trajectory Following with MPPI",
    "section": "",
    "text": "State Space (13D): Position (3D), velocity (3D), orientation (quaternion, 4D), angular velocity (3D) - Frame: NED (North-East-Down) world frame, FRD (Forward-Right-Down) body frame\nControl Input (4D): Total thrust + body angular rates (roll, pitch, yaw rates)\nDynamics: 6-DOF rigid body with quaternion-based attitude representation - For detailed mathematical formulations, see docs/src/dynamics.qmd\nCost Function: Position/velocity tracking error + control effort penalty"
  },
  {
    "objectID": "plan/quadrotor_trajectory_following.html#implementation-summary",
    "href": "plan/quadrotor_trajectory_following.html#implementation-summary",
    "title": "Quadrotor Trajectory Following with MPPI",
    "section": "",
    "text": "Completed Components: - Quadrotor dynamics module (src/jax_mppi/dynamics/quadrotor.py) - Quaternion utilities and kinematics - 6-DOF dynamics with RK4 integration - Unit tests for quaternion norm preservation and energy conservation - Trajectory cost functions (src/jax_mppi/costs/quadrotor.py) - Position/velocity tracking, attitude tracking, terminal costs - Unit tests for all cost functions\n\n\n\nCompleted: - Trajectory generation utilities (examples/quadrotor/trajectories.py) - Circular, figure-8, hover, helix trajectories - Waypoint interpolation with cubic Hermite splines - Trajectory metrics computation - 28 unit tests (all passing)\n\n\n\nExample 1: quadrotor_hover.py - Stabilization around fixed setpoint - Visualization of state evolution, performance metrics (settling time, overshoot)\nExample 2: quadrotor_circle.py - Circular trajectory tracking - Tracking error visualization, control inputs, 3D trajectory plots\nIntegration Tests: 11 tests covering both examples\n\n\n\nExample 3: quadrotor_figure8_comparison.py - MPPI variant comparison - Side-by-side comparison of MPPI, SMPPI, and KMPPI on aggressive figure-8 - Comprehensive metrics: tracking accuracy, control smoothness, energy consumption - 6-subplot visualization with performance comparison table - Demonstrates SMPPI produces smoother control (lower jerk)\nExample 4: quadrotor_custom_trajectory.py - Waypoint following - User-defined waypoint trajectories with smooth interpolation - Command-line interface for custom waypoint specification - Default square pattern demonstration\nIntegration Tests: 13 tests for advanced examples\n\n\n\n\nImplementation plan (this document)\nREADME for quadrotor examples\nUpdate main README with quadrotor section"
  },
  {
    "objectID": "plan/quadrotor_trajectory_following.html#key-features-delivered",
    "href": "plan/quadrotor_trajectory_following.html#key-features-delivered",
    "title": "Quadrotor Trajectory Following with MPPI",
    "section": "",
    "text": "‚úÖ 6-DOF quadrotor dynamics with quaternion representation ‚úÖ Multiple reference trajectory types (circle, figure-8, hover, custom waypoints) ‚úÖ Comprehensive cost functions for trajectory tracking ‚úÖ Four complete examples demonstrating different capabilities ‚úÖ 50+ unit and integration tests (all passing) ‚úÖ Publication-quality visualizations saved to docs/_media/quadrotor/ ‚úÖ 50 Hz control rate (JIT-compiled) for real-time performance ‚úÖ Proper NED/FRD frame conventions throughout"
  },
  {
    "objectID": "plan/quadrotor_trajectory_following.html#file-structure",
    "href": "plan/quadrotor_trajectory_following.html#file-structure",
    "title": "Quadrotor Trajectory Following with MPPI",
    "section": "",
    "text": "jax_mppi/\n‚îú‚îÄ‚îÄ src/jax_mppi/\n‚îÇ   ‚îú‚îÄ‚îÄ dynamics/quadrotor.py      # 6-DOF dynamics with quaternions\n‚îÇ   ‚îî‚îÄ‚îÄ costs/quadrotor.py         # Trajectory tracking costs\n‚îú‚îÄ‚îÄ examples/quadrotor/\n‚îÇ   ‚îú‚îÄ‚îÄ trajectories.py            # Trajectory generation utilities\n‚îÇ   ‚îú‚îÄ‚îÄ quadrotor_hover.py         # Example 1: Hover control\n‚îÇ   ‚îú‚îÄ‚îÄ quadrotor_circle.py        # Example 2: Circle following\n‚îÇ   ‚îú‚îÄ‚îÄ quadrotor_figure8_comparison.py  # Example 3: MPPI comparison\n‚îÇ   ‚îî‚îÄ‚îÄ quadrotor_custom_trajectory.py   # Example 4: Waypoint following\n‚îú‚îÄ‚îÄ tests/\n‚îÇ   ‚îú‚îÄ‚îÄ test_quadrotor_dynamics.py\n‚îÇ   ‚îú‚îÄ‚îÄ test_quadrotor_costs.py\n‚îÇ   ‚îî‚îÄ‚îÄ test_quadrotor_trajectories.py\n‚îî‚îÄ‚îÄ docs/_media/quadrotor/\n    ‚îú‚îÄ‚îÄ quadrotor_hover_mppi.png\n    ‚îú‚îÄ‚îÄ quadrotor_circle_mppi.png\n    ‚îú‚îÄ‚îÄ quadrotor_hover_comparison.png\n    ‚îî‚îÄ‚îÄ quadrotor_figure8_comparison.png"
  },
  {
    "objectID": "plan/quadrotor_trajectory_following.html#performance-metrics",
    "href": "plan/quadrotor_trajectory_following.html#performance-metrics",
    "title": "Quadrotor Trajectory Following with MPPI",
    "section": "",
    "text": "Control Rate: 50 Hz (JIT-compiled)\nTracking Accuracy: &lt;0.1m RMS error on circle trajectory\nSmoothness: SMPPI shows 30-40% lower jerk than standard MPPI\nEnergy: Comparable across all three variants for same tracking performance"
  },
  {
    "objectID": "plan/quadrotor_trajectory_following.html#future-enhancements-optional",
    "href": "plan/quadrotor_trajectory_following.html#future-enhancements-optional",
    "title": "Quadrotor Trajectory Following with MPPI",
    "section": "",
    "text": "Obstacle avoidance during trajectory following\nFull Euler dynamics for rotational motion\nWind disturbance modeling\nAutotuning example for quadrotor hyperparameters\nReal-time animated visualization\n\n\nLast Updated: 2026-02-02 Status: Implementation complete, documentation in progress"
  },
  {
    "objectID": "plan/cuda_imppi_test_results.html",
    "href": "plan/cuda_imppi_test_results.html",
    "title": "CUDA I-MPPI Test Results",
    "section": "",
    "text": "Date: 2026-02-11 Status: ‚úÖ All tests passing\n\n\nSuccessfully tested the CUDA I-MPPI modules from third_party/cuda-mppi/ with the Python simulation environment from examples/i_mppi/i_mppi_parallel_simulation.py.\n\n\n\n\n\nThe following classes are available and working:\n\nOccupancyGrid2D: GPU-based occupancy grid with FOV updates\n\nupload(data): Upload grid from numpy to GPU\ndownload(): Download grid from GPU to numpy\nupdate_fov(): Run FOV-based grid update kernel\n\nInfoField: Information field computation using FSMI\n\ncompute(grid, uav_x, uav_y, ...): Compute info field centered on UAV\ndownload(): Get field data as numpy array\nProperties: Nx, Ny, origin_x, origin_y, res\n\nQuadrotorDynamics: Quadrotor dynamics model\n\nProperties: mass, gravity, tau_omega\n\nInformativeCost: Informative exploration cost function\n\nParameters: lambda_info, lambda_local, target_weight, goal_weight, collision_penalty, height_weight, target_altitude, action_reg\n\nQuadrotorIMPPI: I-MPPI controller for quadrotor\n\ncompute(state): Run one MPPI iteration\nget_action(): Get optimal control\nshift(): Shift nominal trajectory\nset_position_reference(pos_ref, horizon): Set reference trajectory\nupdate_cost_grid(grid): Update grid in cost function\nupdate_cost_info_field(field): Update info field in cost function\n\nTrajectoryGenerator: Reference trajectory generation\n\nselect_target(px, py, pz, info_levels): Select exploration target\nfield_gradient_trajectory(...): Generate field-gradient trajectory\nmake_ref_trajectory(...): Generate straight-line reference\n\nMPPIConfig: MPPI configuration\n\nParameters: num_samples, horizon, nx, nu, lambda_, dt, u_scale, w_action_seq_cost, num_support_pts, lambda_info, alpha\n\n\n\n\n\nAll built successfully in third_party/cuda-mppi/build/:\n\n‚úÖ fsmi_unit_test (1.2 MB): FSMI unit tests - all 6 tests passed\n‚úÖ i_mppi_test (1.1 MB): I-MPPI controller test - runs successfully\n‚úÖ i_mppi_sim (1.2 MB): I-MPPI simulation executable\n‚úÖ informative_sim (1.3 MB): Full informative path planning simulation\n‚úÖ pendulum_test (1.8 MB): Pendulum MPPI test\n\n\n\n\nCreated examples/i_mppi/i_mppi_cuda_test.py that:\n\nLoads the JAX-based simulation environment (grid map, parameters)\nInitializes CUDA I-MPPI controller with proper configuration\nRuns closed-loop simulation at 450 Hz (effective)\nUpdates info field periodically (every 10 steps)\nUpdates grid via FOV kernel at 50 Hz\nComputes MPPI control at 50 Hz\n\nPerformance: - Simulation: 3000 steps (60s simulated time) - Runtime: 6.66s wall-clock time - Effective rate: 450 Hz - Realtime factor: 0.11x (9x slower than realtime for 1000 samples, horizon 40)\n\n\n\n\n\n\nIssue: CUDA context conflict when importing JAX before CUDA MPPI.\nSolution: Always import cuda_mppi before any JAX imports:\n# CORRECT: CUDA first\nimport sys\nsys.path.insert(0, 'third_party/cuda-mppi/build')\nimport cuda_mppi\n\n# Then JAX-based imports\nfrom env_setup import create_grid_map  # uses jax.numpy\n\n\n\nThe CUDA module is built by:\npixi run cuda-mppi-build\nThis creates build/cuda_mppi.cpython-312-x86_64-linux-gnu.so (2.3 MB).\nImportant: After rebuilding, copy to pixi environment:\ncp third_party/cuda-mppi/build/cuda_mppi*.so \\\n   .pixi/envs/dev/lib/python3.12/site-packages/\n\n\n\nAll 6 FSMI tests pass: 1. ‚úÖ Full FSMI facing unknown zone 2. ‚úÖ Uniform FSMI in free space 3. ‚úÖ Single beam through unknown cells 4. ‚úÖ Uniform-FSMI single beam 5. ‚úÖ Info field computation (6x6 grid) 6. ‚úÖ FOV grid update kernel\n\n\n\n\nGrid shape: (H, W) in numpy (row-major)\nCUDA expects: width √ó height with flattened data\nConversion: grid_flat = np.array(jax_grid).flatten().astype(np.float32)\n\n\n\n\n\n\n\n\nexamples/i_mppi/i_mppi_cuda_test.py: CUDA I-MPPI integration test\ntest_cuda_imppi.py: Basic CUDA module functionality test (root)\ntest_cuda_grid_simple.py: Grid upload test (root)\ndocs/plan/cuda_imppi_test_results.md: This document\n\n\n\n\n\nCopied cuda_mppi.cpython-312-x86_64-linux-gnu.so to both dev and default pixi envs\n\n\n\n\n\n\nProper Dynamics Integration: Replace placeholder dynamics with actual quadrotor dynamics\nBenchmark: Compare CUDA I-MPPI vs JAX I-MPPI performance\nVisualization: Generate trajectory animations using CUDA controller\nFull Simulation: Run complete informative path planning mission with:\n\nReference trajectory generation from info field\nBiased MPPI tracking with obstacle avoidance\nFOV-based grid updates\nReal-time capable execution (&gt;50 Hz target)\n\n\n\n\n\n# Build CUDA modules\npixi run cuda-mppi-build\n\n# Run Python integration test\npixi run -e dev python examples/i_mppi/i_mppi_cuda_test.py\n\n# Run CUDA standalone tests\n./third_party/cuda-mppi/build/fsmi_unit_test\n./third_party/cuda-mppi/build/i_mppi_test\n./third_party/cuda-mppi/build/informative_sim\n\n# Test basic module import\npixi run -e dev python test_cuda_imppi.py\n\n\n\n‚úÖ All CUDA I-MPPI modules are functional and integrated with the Python simulation environment.\nThe key components (occupancy grid, FSMI, info field, I-MPPI controller, trajectory generation) are all working correctly and can be used from Python with the correct import order. Performance is good at 450 Hz effective rate for the test configuration."
  },
  {
    "objectID": "plan/cuda_imppi_test_results.html#summary",
    "href": "plan/cuda_imppi_test_results.html#summary",
    "title": "CUDA I-MPPI Test Results",
    "section": "",
    "text": "Successfully tested the CUDA I-MPPI modules from third_party/cuda-mppi/ with the Python simulation environment from examples/i_mppi/i_mppi_parallel_simulation.py."
  },
  {
    "objectID": "plan/cuda_imppi_test_results.html#components-tested",
    "href": "plan/cuda_imppi_test_results.html#components-tested",
    "title": "CUDA I-MPPI Test Results",
    "section": "",
    "text": "The following classes are available and working:\n\nOccupancyGrid2D: GPU-based occupancy grid with FOV updates\n\nupload(data): Upload grid from numpy to GPU\ndownload(): Download grid from GPU to numpy\nupdate_fov(): Run FOV-based grid update kernel\n\nInfoField: Information field computation using FSMI\n\ncompute(grid, uav_x, uav_y, ...): Compute info field centered on UAV\ndownload(): Get field data as numpy array\nProperties: Nx, Ny, origin_x, origin_y, res\n\nQuadrotorDynamics: Quadrotor dynamics model\n\nProperties: mass, gravity, tau_omega\n\nInformativeCost: Informative exploration cost function\n\nParameters: lambda_info, lambda_local, target_weight, goal_weight, collision_penalty, height_weight, target_altitude, action_reg\n\nQuadrotorIMPPI: I-MPPI controller for quadrotor\n\ncompute(state): Run one MPPI iteration\nget_action(): Get optimal control\nshift(): Shift nominal trajectory\nset_position_reference(pos_ref, horizon): Set reference trajectory\nupdate_cost_grid(grid): Update grid in cost function\nupdate_cost_info_field(field): Update info field in cost function\n\nTrajectoryGenerator: Reference trajectory generation\n\nselect_target(px, py, pz, info_levels): Select exploration target\nfield_gradient_trajectory(...): Generate field-gradient trajectory\nmake_ref_trajectory(...): Generate straight-line reference\n\nMPPIConfig: MPPI configuration\n\nParameters: num_samples, horizon, nx, nu, lambda_, dt, u_scale, w_action_seq_cost, num_support_pts, lambda_info, alpha\n\n\n\n\n\nAll built successfully in third_party/cuda-mppi/build/:\n\n‚úÖ fsmi_unit_test (1.2 MB): FSMI unit tests - all 6 tests passed\n‚úÖ i_mppi_test (1.1 MB): I-MPPI controller test - runs successfully\n‚úÖ i_mppi_sim (1.2 MB): I-MPPI simulation executable\n‚úÖ informative_sim (1.3 MB): Full informative path planning simulation\n‚úÖ pendulum_test (1.8 MB): Pendulum MPPI test\n\n\n\n\nCreated examples/i_mppi/i_mppi_cuda_test.py that:\n\nLoads the JAX-based simulation environment (grid map, parameters)\nInitializes CUDA I-MPPI controller with proper configuration\nRuns closed-loop simulation at 450 Hz (effective)\nUpdates info field periodically (every 10 steps)\nUpdates grid via FOV kernel at 50 Hz\nComputes MPPI control at 50 Hz\n\nPerformance: - Simulation: 3000 steps (60s simulated time) - Runtime: 6.66s wall-clock time - Effective rate: 450 Hz - Realtime factor: 0.11x (9x slower than realtime for 1000 samples, horizon 40)"
  },
  {
    "objectID": "plan/cuda_imppi_test_results.html#key-findings",
    "href": "plan/cuda_imppi_test_results.html#key-findings",
    "title": "CUDA I-MPPI Test Results",
    "section": "",
    "text": "Issue: CUDA context conflict when importing JAX before CUDA MPPI.\nSolution: Always import cuda_mppi before any JAX imports:\n# CORRECT: CUDA first\nimport sys\nsys.path.insert(0, 'third_party/cuda-mppi/build')\nimport cuda_mppi\n\n# Then JAX-based imports\nfrom env_setup import create_grid_map  # uses jax.numpy\n\n\n\nThe CUDA module is built by:\npixi run cuda-mppi-build\nThis creates build/cuda_mppi.cpython-312-x86_64-linux-gnu.so (2.3 MB).\nImportant: After rebuilding, copy to pixi environment:\ncp third_party/cuda-mppi/build/cuda_mppi*.so \\\n   .pixi/envs/dev/lib/python3.12/site-packages/\n\n\n\nAll 6 FSMI tests pass: 1. ‚úÖ Full FSMI facing unknown zone 2. ‚úÖ Uniform FSMI in free space 3. ‚úÖ Single beam through unknown cells 4. ‚úÖ Uniform-FSMI single beam 5. ‚úÖ Info field computation (6x6 grid) 6. ‚úÖ FOV grid update kernel\n\n\n\n\nGrid shape: (H, W) in numpy (row-major)\nCUDA expects: width √ó height with flattened data\nConversion: grid_flat = np.array(jax_grid).flatten().astype(np.float32)"
  },
  {
    "objectID": "plan/cuda_imppi_test_results.html#files-createdmodified",
    "href": "plan/cuda_imppi_test_results.html#files-createdmodified",
    "title": "CUDA I-MPPI Test Results",
    "section": "",
    "text": "examples/i_mppi/i_mppi_cuda_test.py: CUDA I-MPPI integration test\ntest_cuda_imppi.py: Basic CUDA module functionality test (root)\ntest_cuda_grid_simple.py: Grid upload test (root)\ndocs/plan/cuda_imppi_test_results.md: This document\n\n\n\n\n\nCopied cuda_mppi.cpython-312-x86_64-linux-gnu.so to both dev and default pixi envs"
  },
  {
    "objectID": "plan/cuda_imppi_test_results.html#next-steps",
    "href": "plan/cuda_imppi_test_results.html#next-steps",
    "title": "CUDA I-MPPI Test Results",
    "section": "",
    "text": "Proper Dynamics Integration: Replace placeholder dynamics with actual quadrotor dynamics\nBenchmark: Compare CUDA I-MPPI vs JAX I-MPPI performance\nVisualization: Generate trajectory animations using CUDA controller\nFull Simulation: Run complete informative path planning mission with:\n\nReference trajectory generation from info field\nBiased MPPI tracking with obstacle avoidance\nFOV-based grid updates\nReal-time capable execution (&gt;50 Hz target)"
  },
  {
    "objectID": "plan/cuda_imppi_test_results.html#commands-reference",
    "href": "plan/cuda_imppi_test_results.html#commands-reference",
    "title": "CUDA I-MPPI Test Results",
    "section": "",
    "text": "# Build CUDA modules\npixi run cuda-mppi-build\n\n# Run Python integration test\npixi run -e dev python examples/i_mppi/i_mppi_cuda_test.py\n\n# Run CUDA standalone tests\n./third_party/cuda-mppi/build/fsmi_unit_test\n./third_party/cuda-mppi/build/i_mppi_test\n./third_party/cuda-mppi/build/informative_sim\n\n# Test basic module import\npixi run -e dev python test_cuda_imppi.py"
  },
  {
    "objectID": "plan/cuda_imppi_test_results.html#conclusion",
    "href": "plan/cuda_imppi_test_results.html#conclusion",
    "title": "CUDA I-MPPI Test Results",
    "section": "",
    "text": "‚úÖ All CUDA I-MPPI modules are functional and integrated with the Python simulation environment.\nThe key components (occupancy grid, FSMI, info field, I-MPPI controller, trajectory generation) are all working correctly and can be used from Python with the correct import order. Performance is good at 450 Hz effective rate for the test configuration."
  },
  {
    "objectID": "plan/example_performance_investigation.html",
    "href": "plan/example_performance_investigation.html",
    "title": "Performance Investigation: Quadrotor Examples",
    "section": "",
    "text": "This document details the findings regarding performance differences between the quadrotor control examples: quadrotor_hover.py, quadrotor_circle.py, and quadrotor_figure8_comparison.py.\n\n\n\nquadrotor_hover.py: Fast (~0.2s/step simulated).\nquadrotor_circle.py: Previously slow (~45x slower). Optimized to use jax.lax.scan for maximum performance.\nquadrotor_figure8_comparison.py: Previously slowest. Optimized to use jax.lax.scan for maximum performance.\n\n\n\n\nThe performance disparity was primarily due to the usage of JAX‚Äôs Just-In-Time (JIT) compilation and how cost functions are handled in the control loop.\n\n\nIn this example, the MPPI command function is explicitly JIT-compiled by the user, and the cost function is effectively constant (closed over static parameters).\n\n\n\nOriginally, this example re-created the cost function at every time step to update the reference target. This prevented JIT compilation of the main MPPI loop, forcing it to run in eager execution mode (or incurring massive re-compilation costs).\nOptimization Implemented: The implementation has been refactored to:\n\nUse step_dependent_dynamics=True to allow passing the time step t to the cost function.\nUse jax.lax.scan to wrap the entire simulation loop into a single JIT-compiled kernel. This eliminates Python loop dispatch overhead completely.\nPass the entire reference trajectory to the scan function and use jax.lax.dynamic_slice inside the loop to extract the current horizon‚Äôs reference. This allows the cost function to close over dynamic data efficiently without recompilation.\n\nParameter Tuning: To improve tracking performance, the following parameters were tuned:\n\nnum_samples: Increased from 1000 to 2000.\nhorizon: Increased from 30 to 50.\nlambda: Decreased from 1.0 to 0.1 (sharper selection).\nCost weights: Significantly increased position and velocity weights.\n\n\n\n\nThis example shared the same issue as quadrotor_circle.py but for three different controllers (mppi, smppi, kmppi).\nOptimization Implemented: Similar to quadrotor_circle.py, the controllers have been updated to use jax.lax.scan for the simulation loop. This required adapting the update logic for all three variants to be compatible with scan and dynamic reference slicing.\nParameter Tuning: Parameters were similarly tuned to handle the aggressive figure-8 trajectory (samples=2000, horizon=50, lambda=0.1).\n\n\n\n\nWhen implementing tracking controllers with JAX MPPI:\n\nUse jax.lax.scan: For simulation loops, wrapping the entire loop in scan provides the best performance by minimizing Python overhead.\nParametrize the Cost Function: Avoid capturing changing concrete values (like current target) in closures if they prevent JIT.\nUse Data Dependencies: Pass changing targets as arguments (Tracers) to the JIT-compiled function.\nStep-Dependent Dynamics: Use step_dependent_dynamics=True to utilize the relative time index t for looking up references in a passed trajectory slice."
  },
  {
    "objectID": "plan/example_performance_investigation.html#summary",
    "href": "plan/example_performance_investigation.html#summary",
    "title": "Performance Investigation: Quadrotor Examples",
    "section": "",
    "text": "quadrotor_hover.py: Fast (~0.2s/step simulated).\nquadrotor_circle.py: Previously slow (~45x slower). Optimized to use jax.lax.scan for maximum performance.\nquadrotor_figure8_comparison.py: Previously slowest. Optimized to use jax.lax.scan for maximum performance."
  },
  {
    "objectID": "plan/example_performance_investigation.html#root-cause-analysis",
    "href": "plan/example_performance_investigation.html#root-cause-analysis",
    "title": "Performance Investigation: Quadrotor Examples",
    "section": "",
    "text": "The performance disparity was primarily due to the usage of JAX‚Äôs Just-In-Time (JIT) compilation and how cost functions are handled in the control loop.\n\n\nIn this example, the MPPI command function is explicitly JIT-compiled by the user, and the cost function is effectively constant (closed over static parameters).\n\n\n\nOriginally, this example re-created the cost function at every time step to update the reference target. This prevented JIT compilation of the main MPPI loop, forcing it to run in eager execution mode (or incurring massive re-compilation costs).\nOptimization Implemented: The implementation has been refactored to:\n\nUse step_dependent_dynamics=True to allow passing the time step t to the cost function.\nUse jax.lax.scan to wrap the entire simulation loop into a single JIT-compiled kernel. This eliminates Python loop dispatch overhead completely.\nPass the entire reference trajectory to the scan function and use jax.lax.dynamic_slice inside the loop to extract the current horizon‚Äôs reference. This allows the cost function to close over dynamic data efficiently without recompilation.\n\nParameter Tuning: To improve tracking performance, the following parameters were tuned:\n\nnum_samples: Increased from 1000 to 2000.\nhorizon: Increased from 30 to 50.\nlambda: Decreased from 1.0 to 0.1 (sharper selection).\nCost weights: Significantly increased position and velocity weights.\n\n\n\n\nThis example shared the same issue as quadrotor_circle.py but for three different controllers (mppi, smppi, kmppi).\nOptimization Implemented: Similar to quadrotor_circle.py, the controllers have been updated to use jax.lax.scan for the simulation loop. This required adapting the update logic for all three variants to be compatible with scan and dynamic reference slicing.\nParameter Tuning: Parameters were similarly tuned to handle the aggressive figure-8 trajectory (samples=2000, horizon=50, lambda=0.1)."
  },
  {
    "objectID": "plan/example_performance_investigation.html#recommendation-for-future-reference",
    "href": "plan/example_performance_investigation.html#recommendation-for-future-reference",
    "title": "Performance Investigation: Quadrotor Examples",
    "section": "",
    "text": "When implementing tracking controllers with JAX MPPI:\n\nUse jax.lax.scan: For simulation loops, wrapping the entire loop in scan provides the best performance by minimizing Python overhead.\nParametrize the Cost Function: Avoid capturing changing concrete values (like current target) in closures if they prevent JIT.\nUse Data Dependencies: Pass changing targets as arguments (Tracers) to the JIT-compiled function.\nStep-Dependent Dynamics: Use step_dependent_dynamics=True to utilize the relative time index t for looking up references in a passed trajectory slice."
  },
  {
    "objectID": "plan/completed/evosax_integration.html",
    "href": "plan/completed/evosax_integration.html",
    "title": "Evosax Integration Plan",
    "section": "",
    "text": "Goal: Add evosax as a JAX-native optimization backend for autotuning in jax-mppi.\nStatus: ‚úÖ Complete\n\n\n\nEvosax is a JAX-native library for evolutionary strategies providing highly efficient, JIT-compiled optimization algorithms. Integration provides:\n\nJAX-native optimization - Full JIT compilation of the entire tuning loop\nGPU acceleration - Evolutionary strategies running entirely on GPU\nDiverse algorithms - CMA-ES, OpenES, SNES, Sep-CMA-ES, and more\nImproved performance - 5-10x faster than Python-based cma library on GPU\nSimplified dependencies - Pure JAX implementation, no external C++ dependencies\n\n\n\n\nPerformance: Fully JIT-compiled ES algorithms vs.¬†Python-based cma library\nJAX ecosystem fit: Natural integration with JAX-based MPPI code\nGPU support: Can run entire autotuning on GPU without host-device transfers\nAlgorithm variety: 15+ evolutionary strategies in one package\nMaintained: Active development and well-documented\n\n\n\n\n\n\n\n\n1. Evosax Optimizer Module (src/jax_mppi/autotune_evosax.py - 387 lines) - EvoSaxOptimizer base class implementing Optimizer ABC - JIT-compiled ask-evaluate-tell loop - Support for single-step and batch optimization - Algorithm-specific convenience classes: CMAESOpt, SepCMAESOpt, OpenESOpt\n2. Package Updates - Added evosax&gt;=0.1.0 dependency to pyproject.toml (optional autotuning group) - Updated __init__.py exports for evosax module - Added chex dependency for array assertions\n3. Comprehensive Testing - Unit tests for all three evosax optimizer classes - Integration tests with MPPI autotuning - Performance comparison tests (evosax vs cma library) - 15+ tests covering setup, optimization, parameter handling\n4. Example & Documentation - examples/autotuning/evosax_comparison.py - Performance comparison script - README section with optimizer comparison matrix and migration guide - Docstring examples for all optimizer classes - Quick-start migration guide (3 lines of code change)\n5. CI Integration - Updated GitHub Actions workflow to install autotuning dependencies - All tests passing in CI pipeline\n\n\n\n\n\n‚úÖ JAX-native CMA-ES, Sep-CMA-ES, and OpenES optimizers ‚úÖ 5-10x GPU speedup over traditional CMA-ES library ‚úÖ Full JIT compilation support for optimization loop ‚úÖ Backward compatible with existing cma library backend ‚úÖ Comprehensive test suite (15+ tests, all passing) ‚úÖ Example comparing evosax vs cma performance ‚úÖ Migration guide for existing users ‚úÖ Optional dependency (maintains lightweight core)\n\n\n\nAll optimizers follow the Optimizer ABC:\nclass Optimizer(abc.ABC):\n    def setup_optimization(self, initial_params, evaluate_fn) -&gt; None: ...\n    def optimize_step(self) -&gt; EvaluationResult: ...\n    def optimize_all(self, iterations: int) -&gt; EvaluationResult: ...\nEvosax optimizer adds: - Strategy selection from evosax‚Äôs 15+ algorithms - Configurable ES hyperparameters via es_params - Support for both sequential and batched evaluation\n\n\n\nBefore (cma library):\nfrom jax_mppi.autotune import CMAESOpt\noptimizer = CMAESOpt(population=10, sigma=0.1)\nAfter (evosax - JAX-native):\nfrom jax_mppi.autotune_evosax import CMAESOpt\noptimizer = CMAESOpt(population=10, sigma=0.1)\n\n\n\n\nCMA-ES (cma library): Baseline performance (CPU-only)\nCMA-ES (evosax): 5-10x faster on GPU, similar on CPU\nSep-CMA-ES (evosax): Better for high-dimensional problems (&gt;20 params)\nOpenES (evosax): Best for large populations (100+), highly parallelizable\n\n\n\n\n\n\n\nStrategy\nBest For\nGPU Speedup\n\n\n\n\nCMA-ES\nGeneral purpose, &lt;20 dims\n5-10x\n\n\nSep-CMA-ES\nHigh-dimensional (20+ params)\n8-12x\n\n\nOpenES\nLarge populations, simple landscapes\n10-15x\n\n\nSNES\nNatural gradients, sample efficiency\n6-10x\n\n\nxNES\nExponential natural evolution\n6-10x\n\n\n\n\n\n\nUse evosax when: - Running on GPU (CUDA/ROCm) - Need maximum performance - Want JAX-native implementation - Using large populations (&gt;20) - Have JAX-pure evaluation functions\nUse cma library when: - CPU-only deployment - Need exact CMA-ES algorithm behavior - Working with external (non-JAX) code - Require specific cma library features\n\n\n\njax_mppi/\n‚îú‚îÄ‚îÄ src/jax_mppi/\n‚îÇ   ‚îú‚îÄ‚îÄ autotune.py              # Core + CMA-ES (cma lib)\n‚îÇ   ‚îú‚îÄ‚îÄ autotune_evosax.py       # JAX-native optimizers (NEW)\n‚îÇ   ‚îú‚îÄ‚îÄ autotune_global.py       # Ray Tune integration\n‚îÇ   ‚îî‚îÄ‚îÄ autotune_qd.py           # Quality Diversity\n‚îú‚îÄ‚îÄ examples/autotuning/\n‚îÇ   ‚îî‚îÄ‚îÄ evosax_comparison.py     # Performance comparison (NEW)\n‚îî‚îÄ‚îÄ tests/\n    ‚îî‚îÄ‚îÄ test_autotune_evosax.py  # Evosax optimizer tests (NEW)\n\n\n\nFunctional Requirements: ‚úÖ All three optimizers (CMA-ES, Sep-CMA-ES, OpenES) working ‚úÖ Compatible with existing autotune infrastructure ‚úÖ Tests passing for all evosax optimizers ‚úÖ Example script demonstrating usage\nPerformance Requirements: ‚úÖ GPU speedup of 5-10x over cma library ‚úÖ No regression in optimization quality ‚úÖ Minimal JIT compilation overhead\nQuality Requirements: ‚úÖ Type hints for all public APIs ‚úÖ Comprehensive docstrings with examples ‚úÖ Unit and integration tests ‚úÖ Example code and migration guide\nIntegration Requirements: ‚úÖ Works with existing Parameter classes ‚úÖ Compatible with Autotune orchestrator ‚úÖ Optional dependency (no breaking changes)\n\n\n\n\nShort-term: - Add more evosax strategies (SNES, xNES, etc.) - Batched evaluation support for pure JAX functions - Hyperparameter auto-adaptation\nMedium-term: - Integration with quality diversity optimization - Adaptive strategy selection based on problem characteristics - Visualization of ES state (covariance ellipsoids)\nLong-term: - Multi-GPU distributed evosax - Learned evolution strategies with meta-learning - JAX-native quality diversity framework\n\nLast Updated: 2026-02-01 Status: Implementation complete, released in v0.1.5"
  },
  {
    "objectID": "plan/completed/evosax_integration.html#overview",
    "href": "plan/completed/evosax_integration.html#overview",
    "title": "Evosax Integration Plan",
    "section": "",
    "text": "Evosax is a JAX-native library for evolutionary strategies providing highly efficient, JIT-compiled optimization algorithms. Integration provides:\n\nJAX-native optimization - Full JIT compilation of the entire tuning loop\nGPU acceleration - Evolutionary strategies running entirely on GPU\nDiverse algorithms - CMA-ES, OpenES, SNES, Sep-CMA-ES, and more\nImproved performance - 5-10x faster than Python-based cma library on GPU\nSimplified dependencies - Pure JAX implementation, no external C++ dependencies\n\n\n\n\nPerformance: Fully JIT-compiled ES algorithms vs.¬†Python-based cma library\nJAX ecosystem fit: Natural integration with JAX-based MPPI code\nGPU support: Can run entire autotuning on GPU without host-device transfers\nAlgorithm variety: 15+ evolutionary strategies in one package\nMaintained: Active development and well-documented"
  },
  {
    "objectID": "plan/completed/evosax_integration.html#implementation-summary",
    "href": "plan/completed/evosax_integration.html#implementation-summary",
    "title": "Evosax Integration Plan",
    "section": "",
    "text": "1. Evosax Optimizer Module (src/jax_mppi/autotune_evosax.py - 387 lines) - EvoSaxOptimizer base class implementing Optimizer ABC - JIT-compiled ask-evaluate-tell loop - Support for single-step and batch optimization - Algorithm-specific convenience classes: CMAESOpt, SepCMAESOpt, OpenESOpt\n2. Package Updates - Added evosax&gt;=0.1.0 dependency to pyproject.toml (optional autotuning group) - Updated __init__.py exports for evosax module - Added chex dependency for array assertions\n3. Comprehensive Testing - Unit tests for all three evosax optimizer classes - Integration tests with MPPI autotuning - Performance comparison tests (evosax vs cma library) - 15+ tests covering setup, optimization, parameter handling\n4. Example & Documentation - examples/autotuning/evosax_comparison.py - Performance comparison script - README section with optimizer comparison matrix and migration guide - Docstring examples for all optimizer classes - Quick-start migration guide (3 lines of code change)\n5. CI Integration - Updated GitHub Actions workflow to install autotuning dependencies - All tests passing in CI pipeline"
  },
  {
    "objectID": "plan/completed/evosax_integration.html#key-features-delivered",
    "href": "plan/completed/evosax_integration.html#key-features-delivered",
    "title": "Evosax Integration Plan",
    "section": "",
    "text": "‚úÖ JAX-native CMA-ES, Sep-CMA-ES, and OpenES optimizers ‚úÖ 5-10x GPU speedup over traditional CMA-ES library ‚úÖ Full JIT compilation support for optimization loop ‚úÖ Backward compatible with existing cma library backend ‚úÖ Comprehensive test suite (15+ tests, all passing) ‚úÖ Example comparing evosax vs cma performance ‚úÖ Migration guide for existing users ‚úÖ Optional dependency (maintains lightweight core)"
  },
  {
    "objectID": "plan/completed/evosax_integration.html#architecture",
    "href": "plan/completed/evosax_integration.html#architecture",
    "title": "Evosax Integration Plan",
    "section": "",
    "text": "All optimizers follow the Optimizer ABC:\nclass Optimizer(abc.ABC):\n    def setup_optimization(self, initial_params, evaluate_fn) -&gt; None: ...\n    def optimize_step(self) -&gt; EvaluationResult: ...\n    def optimize_all(self, iterations: int) -&gt; EvaluationResult: ...\nEvosax optimizer adds: - Strategy selection from evosax‚Äôs 15+ algorithms - Configurable ES hyperparameters via es_params - Support for both sequential and batched evaluation"
  },
  {
    "objectID": "plan/completed/evosax_integration.html#migration-from-cma-to-evosax",
    "href": "plan/completed/evosax_integration.html#migration-from-cma-to-evosax",
    "title": "Evosax Integration Plan",
    "section": "",
    "text": "Before (cma library):\nfrom jax_mppi.autotune import CMAESOpt\noptimizer = CMAESOpt(population=10, sigma=0.1)\nAfter (evosax - JAX-native):\nfrom jax_mppi.autotune_evosax import CMAESOpt\noptimizer = CMAESOpt(population=10, sigma=0.1)"
  },
  {
    "objectID": "plan/completed/evosax_integration.html#performance-benchmarks",
    "href": "plan/completed/evosax_integration.html#performance-benchmarks",
    "title": "Evosax Integration Plan",
    "section": "",
    "text": "CMA-ES (cma library): Baseline performance (CPU-only)\nCMA-ES (evosax): 5-10x faster on GPU, similar on CPU\nSep-CMA-ES (evosax): Better for high-dimensional problems (&gt;20 params)\nOpenES (evosax): Best for large populations (100+), highly parallelizable"
  },
  {
    "objectID": "plan/completed/evosax_integration.html#available-evosax-strategies",
    "href": "plan/completed/evosax_integration.html#available-evosax-strategies",
    "title": "Evosax Integration Plan",
    "section": "",
    "text": "Strategy\nBest For\nGPU Speedup\n\n\n\n\nCMA-ES\nGeneral purpose, &lt;20 dims\n5-10x\n\n\nSep-CMA-ES\nHigh-dimensional (20+ params)\n8-12x\n\n\nOpenES\nLarge populations, simple landscapes\n10-15x\n\n\nSNES\nNatural gradients, sample efficiency\n6-10x\n\n\nxNES\nExponential natural evolution\n6-10x"
  },
  {
    "objectID": "plan/completed/evosax_integration.html#when-to-use-each-backend",
    "href": "plan/completed/evosax_integration.html#when-to-use-each-backend",
    "title": "Evosax Integration Plan",
    "section": "",
    "text": "Use evosax when: - Running on GPU (CUDA/ROCm) - Need maximum performance - Want JAX-native implementation - Using large populations (&gt;20) - Have JAX-pure evaluation functions\nUse cma library when: - CPU-only deployment - Need exact CMA-ES algorithm behavior - Working with external (non-JAX) code - Require specific cma library features"
  },
  {
    "objectID": "plan/completed/evosax_integration.html#file-structure",
    "href": "plan/completed/evosax_integration.html#file-structure",
    "title": "Evosax Integration Plan",
    "section": "",
    "text": "jax_mppi/\n‚îú‚îÄ‚îÄ src/jax_mppi/\n‚îÇ   ‚îú‚îÄ‚îÄ autotune.py              # Core + CMA-ES (cma lib)\n‚îÇ   ‚îú‚îÄ‚îÄ autotune_evosax.py       # JAX-native optimizers (NEW)\n‚îÇ   ‚îú‚îÄ‚îÄ autotune_global.py       # Ray Tune integration\n‚îÇ   ‚îî‚îÄ‚îÄ autotune_qd.py           # Quality Diversity\n‚îú‚îÄ‚îÄ examples/autotuning/\n‚îÇ   ‚îî‚îÄ‚îÄ evosax_comparison.py     # Performance comparison (NEW)\n‚îî‚îÄ‚îÄ tests/\n    ‚îî‚îÄ‚îÄ test_autotune_evosax.py  # Evosax optimizer tests (NEW)"
  },
  {
    "objectID": "plan/completed/evosax_integration.html#success-criteria",
    "href": "plan/completed/evosax_integration.html#success-criteria",
    "title": "Evosax Integration Plan",
    "section": "",
    "text": "Functional Requirements: ‚úÖ All three optimizers (CMA-ES, Sep-CMA-ES, OpenES) working ‚úÖ Compatible with existing autotune infrastructure ‚úÖ Tests passing for all evosax optimizers ‚úÖ Example script demonstrating usage\nPerformance Requirements: ‚úÖ GPU speedup of 5-10x over cma library ‚úÖ No regression in optimization quality ‚úÖ Minimal JIT compilation overhead\nQuality Requirements: ‚úÖ Type hints for all public APIs ‚úÖ Comprehensive docstrings with examples ‚úÖ Unit and integration tests ‚úÖ Example code and migration guide\nIntegration Requirements: ‚úÖ Works with existing Parameter classes ‚úÖ Compatible with Autotune orchestrator ‚úÖ Optional dependency (no breaking changes)"
  },
  {
    "objectID": "plan/completed/evosax_integration.html#future-extensions",
    "href": "plan/completed/evosax_integration.html#future-extensions",
    "title": "Evosax Integration Plan",
    "section": "",
    "text": "Short-term: - Add more evosax strategies (SNES, xNES, etc.) - Batched evaluation support for pure JAX functions - Hyperparameter auto-adaptation\nMedium-term: - Integration with quality diversity optimization - Adaptive strategy selection based on problem characteristics - Visualization of ES state (covariance ellipsoids)\nLong-term: - Multi-GPU distributed evosax - Learned evolution strategies with meta-learning - JAX-native quality diversity framework\n\nLast Updated: 2026-02-01 Status: Implementation complete, released in v0.1.5"
  },
  {
    "objectID": "plan/completed/cuda_mppi_submodule_plan.html",
    "href": "plan/completed/cuda_mppi_submodule_plan.html",
    "title": "Plan: Add cuda_mppi as git submodule",
    "section": "",
    "text": "Extract the CUDA MPPI implementation from src/cuda_mppi/ into a separate git repository and integrate it as a submodule at third_party/cuda_mppi.\n\n\n\n\nNew repository: riccardo-enr/cuda_mppi\nSubmodule path: third_party/cuda_mppi\nFeature branch: feat/cuda-mppi-submodule\nTrack branch: main (in submodule)\n\n\n\n\n\n\n\nUse gh repo create to create riccardo-enr/cuda_mppi\nSet appropriate description and visibility\nInitialize with README.md\n\n\n\n\n\nClone the new repository locally\nCopy contents from src/cuda_mppi/ to repository root\nCreate proper repository structure:\n\nCMakeLists.txt (standalone build)\nREADME.md (usage and build instructions)\nLICENSE (MIT, matching main project)\n.gitignore\n\nCommit and push initial code\n\n\n\n\n\nCreate branch: feat/cuda-mppi-submodule\nCheckout the new branch\n\n\n\n\n\nCreate third_party/ directory\nRun: git submodule add https://github.com/riccardo-enr/cuda_mppi.git third_party/cuda_mppi\nRemove old src/cuda_mppi/ directory\nCommit submodule addition\n\n\n\n\n\nUpdate root CMakeLists.txt:\n\nChange add_subdirectory(src/cuda_mppi) to add_subdirectory(third_party/cuda_mppi)\n\nVerify pyproject.toml (should work via scikit-build-core automatically)\nCheck for any hardcoded paths in:\n\nPython source files\nCMake files\nDocumentation\n\n\n\n\n\n\nClean build: rm -rf build/ _skbuild/\nTest clone from scratch with submodules\nBuild and install package: uv pip install -e .\nRun test: python examples/test_cuda_mppi.py\nVerify JIT examples work\n\n\n\n\n\nAdd submodule instructions to README.md:\n# Clone with submodules\ngit clone --recursive https://github.com/riccardo-enr/jax_mppi.git\n\n# Or if already cloned\ngit submodule update --init --recursive\nDocument build requirements in both repositories\nUpdate any relevant documentation links\n\n\n\n\n\nCommit all changes with conventional commits\nPush feature branch\nCreate PR referencing issue #23\nMove this plan to completed/ directory\n\n\n\n\n\nIf issues arise, we can:\n\nRemove submodule: git submodule deinit -f third_party/cuda_mppi\nDelete from git: git rm -f third_party/cuda_mppi\nRemove from .gitmodules\nRestore original src/cuda_mppi from git history"
  },
  {
    "objectID": "plan/completed/cuda_mppi_submodule_plan.html#overview",
    "href": "plan/completed/cuda_mppi_submodule_plan.html#overview",
    "title": "Plan: Add cuda_mppi as git submodule",
    "section": "",
    "text": "Extract the CUDA MPPI implementation from src/cuda_mppi/ into a separate git repository and integrate it as a submodule at third_party/cuda_mppi."
  },
  {
    "objectID": "plan/completed/cuda_mppi_submodule_plan.html#decisions-made",
    "href": "plan/completed/cuda_mppi_submodule_plan.html#decisions-made",
    "title": "Plan: Add cuda_mppi as git submodule",
    "section": "",
    "text": "New repository: riccardo-enr/cuda_mppi\nSubmodule path: third_party/cuda_mppi\nFeature branch: feat/cuda-mppi-submodule\nTrack branch: main (in submodule)"
  },
  {
    "objectID": "plan/completed/cuda_mppi_submodule_plan.html#implementation-steps",
    "href": "plan/completed/cuda_mppi_submodule_plan.html#implementation-steps",
    "title": "Plan: Add cuda_mppi as git submodule",
    "section": "",
    "text": "Use gh repo create to create riccardo-enr/cuda_mppi\nSet appropriate description and visibility\nInitialize with README.md\n\n\n\n\n\nClone the new repository locally\nCopy contents from src/cuda_mppi/ to repository root\nCreate proper repository structure:\n\nCMakeLists.txt (standalone build)\nREADME.md (usage and build instructions)\nLICENSE (MIT, matching main project)\n.gitignore\n\nCommit and push initial code\n\n\n\n\n\nCreate branch: feat/cuda-mppi-submodule\nCheckout the new branch\n\n\n\n\n\nCreate third_party/ directory\nRun: git submodule add https://github.com/riccardo-enr/cuda_mppi.git third_party/cuda_mppi\nRemove old src/cuda_mppi/ directory\nCommit submodule addition\n\n\n\n\n\nUpdate root CMakeLists.txt:\n\nChange add_subdirectory(src/cuda_mppi) to add_subdirectory(third_party/cuda_mppi)\n\nVerify pyproject.toml (should work via scikit-build-core automatically)\nCheck for any hardcoded paths in:\n\nPython source files\nCMake files\nDocumentation\n\n\n\n\n\n\nClean build: rm -rf build/ _skbuild/\nTest clone from scratch with submodules\nBuild and install package: uv pip install -e .\nRun test: python examples/test_cuda_mppi.py\nVerify JIT examples work\n\n\n\n\n\nAdd submodule instructions to README.md:\n# Clone with submodules\ngit clone --recursive https://github.com/riccardo-enr/jax_mppi.git\n\n# Or if already cloned\ngit submodule update --init --recursive\nDocument build requirements in both repositories\nUpdate any relevant documentation links\n\n\n\n\n\nCommit all changes with conventional commits\nPush feature branch\nCreate PR referencing issue #23\nMove this plan to completed/ directory"
  },
  {
    "objectID": "plan/completed/cuda_mppi_submodule_plan.html#rollback-plan",
    "href": "plan/completed/cuda_mppi_submodule_plan.html#rollback-plan",
    "title": "Plan: Add cuda_mppi as git submodule",
    "section": "",
    "text": "If issues arise, we can:\n\nRemove submodule: git submodule deinit -f third_party/cuda_mppi\nDelete from git: git rm -f third_party/cuda_mppi\nRemove from .gitmodules\nRestore original src/cuda_mppi from git history"
  },
  {
    "objectID": "plan/cuda_imppi_bugfixes.html",
    "href": "plan/cuda_imppi_bugfixes.html",
    "title": "Fix CUDA I-MPPI Simulation Issues",
    "section": "",
    "text": "The C++/CUDA I-MPPI feature parity implementation (Phase 1-8) is complete and builds successfully. FSMI unit tests pass, but the full informative_sim diverges because the controller never receives updated cost parameters (info field, reference trajectory) during the simulation loop. There are also build quality issues (Eigen/CUDA warnings) and the pixi-installed .so is stale.\n\n\n\n\n\nRoot cause: MPPIController stores cost_ by value (line 170 of mppi.cuh). The rollout_kernel receives cost by value at each kernel launch, copying the controller‚Äôs internal cost_ member. In informative_sim.cu, the sim loop updates a local cost variable (lines 200-202) but never propagates these changes to controller.cost_.\nThe existing update_cost_params(goal, lambda_goal) in i_mppi.cuh:63 only updates two fields. The info field pointer (info_field.d_field), field origin/dimensions, and reference trajectory pointer are never updated.\nFix: Add set_cost(const Cost& c) method to MPPIController that replaces cost_ entirely. Then call it from the sim loop after updating cost fields.\nFiles: - third_party/cuda-mppi/include/mppi/controllers/mppi.cuh ‚Äî add set_cost() to base class - third_party/cuda-mppi/src/informative_sim.cu ‚Äî call controller.set_cost(cost) after updating cost fields\n\n\n\nRoot cause: Eigen‚Äôs CUDA complex number headers trigger calling a constexpr __host__ function from a __host__ __device__ function warnings. These are cosmetic but noisy.\nFix: Add --expt-relaxed-constexpr to CUDA compile flags in CMakeLists.txt.\nFiles: - third_party/cuda-mppi/CMakeLists.txt ‚Äî add target_compile_options with --expt-relaxed-constexpr\n\n\n\nRoot cause: The .so in .pixi/envs/dev/lib/python3.12/site-packages/ is from a previous build and lacks the new types (OccupancyGrid2D, InfoField, QuadrotorDynamics, etc.). Python imports pick up the stale one.\nFix: Rebuild via pixi run or pip install from the build directory so the installed .so matches the source.\nFiles: - Run pixi run build task or pip install -e third_party/cuda-mppi/ to update\n\n\n\n\n\n\nIn third_party/cuda-mppi/include/mppi/controllers/mppi.cuh, add to the public section:\nvoid set_cost(const Cost& cost) { cost_ = cost; }\nThis is safe because the cost struct is trivially copyable (all POD + device pointers). The next compute() call will pass the updated cost_ to the rollout kernel.\nAlso add a getter for symmetry:\nconst Cost& cost() const { return cost_; }\nCost& cost() { return cost_; }\n\n\n\nReplace lines 197-207 in informative_sim.cu:\n// Update cost fields\ncost.ref_trajectory = d_ref_traj;\ncost.ref_horizon    = ifc.ref_horizon;\ncost.info_field     = info_field;\n\n// Propagate to controller\ncontroller.set_cost(cost);\n\n\n\nIn third_party/cuda-mppi/CMakeLists.txt, after the CUDA standard setting:\nadd_compile_options($&lt;$&lt;COMPILE_LANGUAGE:CUDA&gt;:--expt-relaxed-constexpr&gt;)\n\n\n\npixi run build-cuda  # or equivalent task\nIf no pixi build task exists for cuda-mppi, install manually:\ncd third_party/cuda-mppi && pip install .\n\n\n\n\n\nThe rollout kernel applies u = (u_nom + noise) * u_scale, but get_action() returns raw u_nom without scaling. The sim loop must scale: action = get_action() * u_scale.\n\n\n\nweighted_update_kernel was called with hardcoded 0.1f learning rate in both mppi.cuh and i_mppi.cuh. Standard MPPI uses 1.0. Made it configurable via MPPIConfig::learning_rate (default 1.0).\n\n\n\nu_nom starts at 0, but hover requires thrust = mass * gravity ‚âà 19.62N. With u_scale=10, initialized u_nom[t][0] = 1.962 (hover/u_scale) so the controller starts at equilibrium.\n\n\n\n\n\n\nBuild: all targets compile clean, no warnings\nfsmi_unit_test: 6/6 tests pass\ninformative_sim: quadrotor maintains stable flight (z ‚âà -2.0), depletes zone 0 from 99‚Üí5\nPython bindings build successfully"
  },
  {
    "objectID": "plan/cuda_imppi_bugfixes.html#context",
    "href": "plan/cuda_imppi_bugfixes.html#context",
    "title": "Fix CUDA I-MPPI Simulation Issues",
    "section": "",
    "text": "The C++/CUDA I-MPPI feature parity implementation (Phase 1-8) is complete and builds successfully. FSMI unit tests pass, but the full informative_sim diverges because the controller never receives updated cost parameters (info field, reference trajectory) during the simulation loop. There are also build quality issues (Eigen/CUDA warnings) and the pixi-installed .so is stale."
  },
  {
    "objectID": "plan/cuda_imppi_bugfixes.html#issues",
    "href": "plan/cuda_imppi_bugfixes.html#issues",
    "title": "Fix CUDA I-MPPI Simulation Issues",
    "section": "",
    "text": "Root cause: MPPIController stores cost_ by value (line 170 of mppi.cuh). The rollout_kernel receives cost by value at each kernel launch, copying the controller‚Äôs internal cost_ member. In informative_sim.cu, the sim loop updates a local cost variable (lines 200-202) but never propagates these changes to controller.cost_.\nThe existing update_cost_params(goal, lambda_goal) in i_mppi.cuh:63 only updates two fields. The info field pointer (info_field.d_field), field origin/dimensions, and reference trajectory pointer are never updated.\nFix: Add set_cost(const Cost& c) method to MPPIController that replaces cost_ entirely. Then call it from the sim loop after updating cost fields.\nFiles: - third_party/cuda-mppi/include/mppi/controllers/mppi.cuh ‚Äî add set_cost() to base class - third_party/cuda-mppi/src/informative_sim.cu ‚Äî call controller.set_cost(cost) after updating cost fields\n\n\n\nRoot cause: Eigen‚Äôs CUDA complex number headers trigger calling a constexpr __host__ function from a __host__ __device__ function warnings. These are cosmetic but noisy.\nFix: Add --expt-relaxed-constexpr to CUDA compile flags in CMakeLists.txt.\nFiles: - third_party/cuda-mppi/CMakeLists.txt ‚Äî add target_compile_options with --expt-relaxed-constexpr\n\n\n\nRoot cause: The .so in .pixi/envs/dev/lib/python3.12/site-packages/ is from a previous build and lacks the new types (OccupancyGrid2D, InfoField, QuadrotorDynamics, etc.). Python imports pick up the stale one.\nFix: Rebuild via pixi run or pip install from the build directory so the installed .so matches the source.\nFiles: - Run pixi run build task or pip install -e third_party/cuda-mppi/ to update"
  },
  {
    "objectID": "plan/cuda_imppi_bugfixes.html#implementation",
    "href": "plan/cuda_imppi_bugfixes.html#implementation",
    "title": "Fix CUDA I-MPPI Simulation Issues",
    "section": "",
    "text": "In third_party/cuda-mppi/include/mppi/controllers/mppi.cuh, add to the public section:\nvoid set_cost(const Cost& cost) { cost_ = cost; }\nThis is safe because the cost struct is trivially copyable (all POD + device pointers). The next compute() call will pass the updated cost_ to the rollout kernel.\nAlso add a getter for symmetry:\nconst Cost& cost() const { return cost_; }\nCost& cost() { return cost_; }\n\n\n\nReplace lines 197-207 in informative_sim.cu:\n// Update cost fields\ncost.ref_trajectory = d_ref_traj;\ncost.ref_horizon    = ifc.ref_horizon;\ncost.info_field     = info_field;\n\n// Propagate to controller\ncontroller.set_cost(cost);\n\n\n\nIn third_party/cuda-mppi/CMakeLists.txt, after the CUDA standard setting:\nadd_compile_options($&lt;$&lt;COMPILE_LANGUAGE:CUDA&gt;:--expt-relaxed-constexpr&gt;)\n\n\n\npixi run build-cuda  # or equivalent task\nIf no pixi build task exists for cuda-mppi, install manually:\ncd third_party/cuda-mppi && pip install .\n\n\n\n\n\nThe rollout kernel applies u = (u_nom + noise) * u_scale, but get_action() returns raw u_nom without scaling. The sim loop must scale: action = get_action() * u_scale.\n\n\n\nweighted_update_kernel was called with hardcoded 0.1f learning rate in both mppi.cuh and i_mppi.cuh. Standard MPPI uses 1.0. Made it configurable via MPPIConfig::learning_rate (default 1.0).\n\n\n\nu_nom starts at 0, but hover requires thrust = mass * gravity ‚âà 19.62N. With u_scale=10, initialized u_nom[t][0] = 1.962 (hover/u_scale) so the controller starts at equilibrium."
  },
  {
    "objectID": "plan/cuda_imppi_bugfixes.html#verification-completed",
    "href": "plan/cuda_imppi_bugfixes.html#verification-completed",
    "title": "Fix CUDA I-MPPI Simulation Issues",
    "section": "",
    "text": "Build: all targets compile clean, no warnings\nfsmi_unit_test: 6/6 tests pass\ninformative_sim: quadrotor maintains stable flight (z ‚âà -2.0), depletes zone 0 from 99‚Üí5\nPython bindings build successfully"
  },
  {
    "objectID": "plan/i_mppi_architecture_alignment.html",
    "href": "plan/i_mppi_architecture_alignment.html",
    "title": "I-MPPI Architecture Alignment Plan",
    "section": "",
    "text": "The current I-MPPI implementation is missing the informative term in Layer 3 (fast loop). The architecture description specifies a two-layer system:\n\nLayer 2 (FSMI Analyzer): ~5-10 Hz - Full FSMI for reference trajectory generation\nLayer 3 (I-MPPI Controller): ~50 Hz - Fast control with Uniform-FSMI for local reactivity\n\nCurrently, Layer 3 only does trajectory tracking (running_cost), which reduces I-MPPI to standard MPPI with a biased reference. This loses: - Reactive viewpoint maintenance during disturbances - Handling of occlusions detected between Layer 2 updates - True informative control behavior\n\n\n\n\n\nCreate a simplified O(n) FSMI variant for the fast loop:\nclass UniformFSMI:\n    \"\"\"\n    Uniform-FSMI from Zhang et al. (2020) for O(n) computation.\n\n    Simplifications vs full FSMI:\n    - Assumes uniform sensor noise\n    - Uses shorter ray range (local, 2-3m)\n    - Fewer beams\n    - No G_kj matrix computation (biggest speedup)\n    \"\"\"\nKey differences from full FSMI: - Sensor noise is uniform (constant sigma) - Ray range limited to 2-3m (local) - Reduced beam count (4-8 vs 16) - Direct sum without G_kj matrix: MI ‚âà sum_j P(e_j) * C_j\n\n\n\nAdd to src/jax_mppi/i_mppi/environment.py:\ndef informative_running_cost(\n    state: jax.Array,\n    action: jax.Array,\n    t: int,\n    target: jax.Array,\n    grid_map: jax.Array,\n    uniform_fsmi: UniformFSMI,\n    info_weight: float = 5.0,\n) -&gt; jax.Array:\n    \"\"\"\n    Cost function with informative term:\n    J = tracking_cost - info_weight * uniform_fsmi(state)\n    \"\"\"\n    tracking = running_cost(state, action, t, target)\n    info_gain = uniform_fsmi.compute(grid_map, state[:2], yaw)\n    return tracking - info_weight * info_gain\n\n\n\nModify examples/i_mppi/i_mppi_simulation.py:\n# Layer 3: Biased I-MPPI with informative cost\ncost_fn = partial(\n    informative_running_cost,\n    target=ref_traj,\n    grid_map=grid_map,\n    uniform_fsmi=uniform_fsmi,\n    info_weight=local_info_weight,\n)\n\n\n\nReorganize examples/i_mppi/ to clearly show the architecture:\nexamples/i_mppi/\n‚îú‚îÄ‚îÄ __init__.py\n‚îú‚îÄ‚îÄ i_mppi_simulation.py      # Main: Layer 2 (FSMI) + Layer 3 (I-MPPI with Uniform-FSMI)\n‚îú‚îÄ‚îÄ i_mppi_simulation_legacy.py  # Legacy: geometric zones\n‚îú‚îÄ‚îÄ fsmi_grid_demo.py         # Demo: Full FSMI visualization\n‚îú‚îÄ‚îÄ uniform_fsmi_demo.py      # NEW: Demonstrate Uniform-FSMI speedup\n‚îî‚îÄ‚îÄ architecture_comparison.py # NEW: Compare with/without Layer 3 info term\n\n\n\n\n\n\n\nAdd UniformFSMI class to fsmi.py\nImplement O(n) MI computation\nAdd configuration parameters (short_range, few_beams)\nBenchmark vs full FSMI\n\n\n\n\n\nAdd informative_running_cost() to environment.py\nMake grid_map accessible in cost function (via closure or state)\nHandle yaw extraction from state\n\n\n\n\n\nIntegrate Uniform-FSMI into Layer 3 cost\nAdd config parameters for local info weight\nMaintain backward compatibility flag\n\n\n\n\n\nCreate uniform_fsmi_demo.py\nCreate architecture_comparison.py showing the difference\n\n\n\n\n\nUpdate README in examples/i_mppi/\nDocument the two-layer architecture\n\n\n\n\n\n\n\nFSMIConfig(\n    use_grid_fsmi=True,\n    num_beams=16,\n    max_range=5.0,\n    ray_step=0.15,\n    trajectory_subsample_rate=8,\n    info_weight=15.0,\n)\n\n\n\nUniformFSMIConfig(\n    num_beams=6,          # Reduced\n    max_range=2.5,        # Local only\n    ray_step=0.2,         # Coarser\n    info_weight=5.0,      # Lower weight (reactive)\n)\n\n\n\n\n\n\n\nComponent\nRate\nComputation\n\n\n\n\nLayer 2 (Full FSMI)\n5 Hz\n~40-50ms\n\n\nLayer 3 (Uniform-FSMI per sample)\n50 Hz\n~0.5ms\n\n\nTotal control cycle\n50 Hz\n&lt;20ms\n\n\n\n\n\n\n\nReactive Viewpoint Maintenance: During disturbances, Layer 3 locally optimizes viewing angle\nOcclusion Handling: Can detect and respond to occlusions between Layer 2 updates\nTrue I-MPPI: Not just biased trajectory tracking\n\n\n\n\n\nStep 1: Uniform-FSMI implementation (UniformFSMI class in fsmi.py)\nStep 2: Cost function update (informative_running_cost in environment.py)\nStep 3: Simulation integration (i_mppi_simulation.py updated)\nStep 4: Demo examples (optional: architecture_comparison.py)\nStep 5: Documentation (README.md updated)"
  },
  {
    "objectID": "plan/i_mppi_architecture_alignment.html#problem-statement",
    "href": "plan/i_mppi_architecture_alignment.html#problem-statement",
    "title": "I-MPPI Architecture Alignment Plan",
    "section": "",
    "text": "The current I-MPPI implementation is missing the informative term in Layer 3 (fast loop). The architecture description specifies a two-layer system:\n\nLayer 2 (FSMI Analyzer): ~5-10 Hz - Full FSMI for reference trajectory generation\nLayer 3 (I-MPPI Controller): ~50 Hz - Fast control with Uniform-FSMI for local reactivity\n\nCurrently, Layer 3 only does trajectory tracking (running_cost), which reduces I-MPPI to standard MPPI with a biased reference. This loses: - Reactive viewpoint maintenance during disturbances - Handling of occlusions detected between Layer 2 updates - True informative control behavior"
  },
  {
    "objectID": "plan/i_mppi_architecture_alignment.html#required-changes",
    "href": "plan/i_mppi_architecture_alignment.html#required-changes",
    "title": "I-MPPI Architecture Alignment Plan",
    "section": "",
    "text": "Create a simplified O(n) FSMI variant for the fast loop:\nclass UniformFSMI:\n    \"\"\"\n    Uniform-FSMI from Zhang et al. (2020) for O(n) computation.\n\n    Simplifications vs full FSMI:\n    - Assumes uniform sensor noise\n    - Uses shorter ray range (local, 2-3m)\n    - Fewer beams\n    - No G_kj matrix computation (biggest speedup)\n    \"\"\"\nKey differences from full FSMI: - Sensor noise is uniform (constant sigma) - Ray range limited to 2-3m (local) - Reduced beam count (4-8 vs 16) - Direct sum without G_kj matrix: MI ‚âà sum_j P(e_j) * C_j\n\n\n\nAdd to src/jax_mppi/i_mppi/environment.py:\ndef informative_running_cost(\n    state: jax.Array,\n    action: jax.Array,\n    t: int,\n    target: jax.Array,\n    grid_map: jax.Array,\n    uniform_fsmi: UniformFSMI,\n    info_weight: float = 5.0,\n) -&gt; jax.Array:\n    \"\"\"\n    Cost function with informative term:\n    J = tracking_cost - info_weight * uniform_fsmi(state)\n    \"\"\"\n    tracking = running_cost(state, action, t, target)\n    info_gain = uniform_fsmi.compute(grid_map, state[:2], yaw)\n    return tracking - info_weight * info_gain\n\n\n\nModify examples/i_mppi/i_mppi_simulation.py:\n# Layer 3: Biased I-MPPI with informative cost\ncost_fn = partial(\n    informative_running_cost,\n    target=ref_traj,\n    grid_map=grid_map,\n    uniform_fsmi=uniform_fsmi,\n    info_weight=local_info_weight,\n)\n\n\n\nReorganize examples/i_mppi/ to clearly show the architecture:\nexamples/i_mppi/\n‚îú‚îÄ‚îÄ __init__.py\n‚îú‚îÄ‚îÄ i_mppi_simulation.py      # Main: Layer 2 (FSMI) + Layer 3 (I-MPPI with Uniform-FSMI)\n‚îú‚îÄ‚îÄ i_mppi_simulation_legacy.py  # Legacy: geometric zones\n‚îú‚îÄ‚îÄ fsmi_grid_demo.py         # Demo: Full FSMI visualization\n‚îú‚îÄ‚îÄ uniform_fsmi_demo.py      # NEW: Demonstrate Uniform-FSMI speedup\n‚îî‚îÄ‚îÄ architecture_comparison.py # NEW: Compare with/without Layer 3 info term"
  },
  {
    "objectID": "plan/i_mppi_architecture_alignment.html#implementation-steps",
    "href": "plan/i_mppi_architecture_alignment.html#implementation-steps",
    "title": "I-MPPI Architecture Alignment Plan",
    "section": "",
    "text": "Add UniformFSMI class to fsmi.py\nImplement O(n) MI computation\nAdd configuration parameters (short_range, few_beams)\nBenchmark vs full FSMI\n\n\n\n\n\nAdd informative_running_cost() to environment.py\nMake grid_map accessible in cost function (via closure or state)\nHandle yaw extraction from state\n\n\n\n\n\nIntegrate Uniform-FSMI into Layer 3 cost\nAdd config parameters for local info weight\nMaintain backward compatibility flag\n\n\n\n\n\nCreate uniform_fsmi_demo.py\nCreate architecture_comparison.py showing the difference\n\n\n\n\n\nUpdate README in examples/i_mppi/\nDocument the two-layer architecture"
  },
  {
    "objectID": "plan/i_mppi_architecture_alignment.html#configuration-parameters",
    "href": "plan/i_mppi_architecture_alignment.html#configuration-parameters",
    "title": "I-MPPI Architecture Alignment Plan",
    "section": "",
    "text": "FSMIConfig(\n    use_grid_fsmi=True,\n    num_beams=16,\n    max_range=5.0,\n    ray_step=0.15,\n    trajectory_subsample_rate=8,\n    info_weight=15.0,\n)\n\n\n\nUniformFSMIConfig(\n    num_beams=6,          # Reduced\n    max_range=2.5,        # Local only\n    ray_step=0.2,         # Coarser\n    info_weight=5.0,      # Lower weight (reactive)\n)"
  },
  {
    "objectID": "plan/i_mppi_architecture_alignment.html#expected-performance",
    "href": "plan/i_mppi_architecture_alignment.html#expected-performance",
    "title": "I-MPPI Architecture Alignment Plan",
    "section": "",
    "text": "Component\nRate\nComputation\n\n\n\n\nLayer 2 (Full FSMI)\n5 Hz\n~40-50ms\n\n\nLayer 3 (Uniform-FSMI per sample)\n50 Hz\n~0.5ms\n\n\nTotal control cycle\n50 Hz\n&lt;20ms"
  },
  {
    "objectID": "plan/i_mppi_architecture_alignment.html#benefits-of-proper-architecture",
    "href": "plan/i_mppi_architecture_alignment.html#benefits-of-proper-architecture",
    "title": "I-MPPI Architecture Alignment Plan",
    "section": "",
    "text": "Reactive Viewpoint Maintenance: During disturbances, Layer 3 locally optimizes viewing angle\nOcclusion Handling: Can detect and respond to occlusions between Layer 2 updates\nTrue I-MPPI: Not just biased trajectory tracking"
  },
  {
    "objectID": "plan/i_mppi_architecture_alignment.html#status",
    "href": "plan/i_mppi_architecture_alignment.html#status",
    "title": "I-MPPI Architecture Alignment Plan",
    "section": "",
    "text": "Step 1: Uniform-FSMI implementation (UniformFSMI class in fsmi.py)\nStep 2: Cost function update (informative_running_cost in environment.py)\nStep 3: Simulation integration (i_mppi_simulation.py updated)\nStep 4: Demo examples (optional: architecture_comparison.py)\nStep 5: Documentation (README.md updated)"
  },
  {
    "objectID": "autotuning.html",
    "href": "autotuning.html",
    "title": "Autotuning Guide",
    "section": "",
    "text": "JAX-MPPI includes a robust autotuning framework to optimize MPPI hyperparameters (like temperature \\(\\lambda\\), noise covariance \\(\\Sigma\\), and planning horizon). The framework supports multiple optimization strategies, including CMA-ES, Ray Tune, and Quality Diversity (QD) methods.\n\n\nThe autotuning process involves three main components:\n\nTunable Parameters: Parameters you want to optimize (e.g., LambdaParameter, NoiseSigmaParameter).\nEvaluation Function: A function that runs MPPI with a specific configuration and returns a cost (and optionally other metrics).\nOptimizer: The algorithm used to search for the best parameters (e.g., CMAESOpt).\n\n\n\n\nThe autotune module provides a simple interface for CMA-ES optimization.\nimport jax.numpy as jnp\nfrom jax_mppi import mppi, autotune\n\n# 1. Setup MPPI\nconfig, state = mppi.create(...)\nholder = autotune.ConfigStateHolder(config, state)\n\n# 2. Define evaluation\ndef evaluate():\n    # Run simulation with holder.config and holder.state\n    # Calculate performance cost\n    return autotune.EvaluationResult(mean_cost=cost, ...)\n\n# 3. Create Tuner\ntuner = autotune.Autotune(\n    params_to_tune=[\n        autotune.LambdaParameter(holder, min_value=0.1),\n        autotune.NoiseSigmaParameter(holder, min_value=0.1),\n    ],\n    evaluate_fn=evaluate,\n    optimizer=autotune.CMAESOpt(population=10),\n)\n\n# 4. Optimize\nbest_result = tuner.optimize_all(iterations=30)\nprint(f\"Best parameters: {best_result.params}\")\nSee examples/autotuning/basic.py and examples/autotuning/pendulum.py for complete running examples.\n\n\n\n\n\nFor more complex search spaces or when you want to use advanced schedulers and search algorithms (like HyperOpt or Bayesian Optimization), use autotune_global.\n\nNote: Requires ray[tune], hyperopt, and bayesian-optimization.\n\nfrom ray import tune\nfrom jax_mppi import autotune_global as autog\n\n# Define search space using Ray Tune's API\nparams = [\n    autog.GlobalLambdaParameter(holder, search_space=tune.loguniform(0.1, 10.0)),\n    autog.GlobalNoiseSigmaParameter(holder, search_space=tune.uniform(0.1, 2.0)),\n]\n\ntuner = autog.AutotuneGlobal(\n    params_to_tune=params,\n    evaluate_fn=evaluate,\n    optimizer=autog.RayOptimizer(),\n)\n\nbest = tuner.optimize_all(iterations=100)\n\n\n\nTo find a diverse set of high-performing parameters (e.g., finding parameters that work well for different environments or behavioral descriptors), use autotune_qd.\nfrom jax_mppi import autotune_qd\n\ntuner = autotune.Autotune(\n    params_to_tune=[...],\n    evaluate_fn=evaluate,\n    optimizer=autotune_qd.CMAMEOpt(population=20, bins=10),\n)\n\n\n\n\nThe framework supports tuning the following parameters out-of-the-box:\n\nLambdaParameter: MPPI temperature (\\(\\lambda\\)).\nNoiseSigmaParameter: Exploration noise covariance diagonal.\nMuParameter: Exploration noise mean.\nHorizonParameter: Planning horizon length (resizes internal buffers automatically).\n\nYou can also define custom parameters by subclassing TunableParameter.\n\n\n\nThis section details the mathematical foundations of the autotuning algorithms available in jax_mppi.\n\n\nThe goal of autotuning is to find the optimal set of hyperparameters \\(\\theta\\) (e.g., temperature \\(\\lambda\\), noise covariance \\(\\Sigma\\), horizon \\(H\\)) that minimizes the expected cost of the control task. We formulate this as an optimization problem:\n[ ^* = _{} () ]\nwhere \\(\\Theta\\) is the admissible hyperparameter space, and the objective function \\(\\mathcal{J}(\\theta)\\) is the expected cumulative cost of the closed-loop system under the MPPI controller parameterized by \\(\\theta\\):\n[ () = {{}()} ]\nHere, \\(\\tau = \\{(\\mathbf{x}_0, \\mathbf{u}_0), \\dots \\}\\) represents a trajectory rollout, and \\(c(\\mathbf{x}, \\mathbf{u})\\) is the task cost function. Since \\(\\mathcal{J}(\\theta)\\) is typically non-convex and noisy (due to the stochastic nature of MPPI and the environment), we employ derivative-free optimization methods.\n\n\n\nCMA-ES is a state-of-the-art evolutionary algorithm for continuous optimization. It models the population of candidate solutions using a multivariate normal distribution \\(\\mathcal{N}(\\mathbf{m}, \\sigma^2 \\mathbf{C})\\).\nThe algorithm proceeds in generations \\(g\\). At each generation:\n\nSampling: We sample \\(\\lambda_{pop}\\) candidate parameters \\(\\theta_i\\) (offspring): [ i ^{(g)} + ^{(g)} (, ^{(g)}) i = 1, , {pop} ]\nEvaluation: Each candidate \\(\\theta_i\\) is evaluated by running an MPPI simulation to estimate \\(\\mathcal{J}(\\theta_i)\\).\nSelection and Recombination: The candidates are sorted by their cost \\(\\mathcal{J}(\\theta_i)\\). The top \\(\\mu\\) candidates (parents) are selected to update the mean: [ ^{(g+1)} = {i=1}^{} w_i {i:_{pop}} ] where \\(w_i\\) are positive weights summing to 1, and \\(\\theta_{i:\\lambda_{pop}}\\) denotes the \\(i\\)-th best candidate.\nCovariance Adaptation: The covariance matrix \\(\\mathbf{C}^{(g)}\\) is updated to increase the likelihood of successful steps. This involves two paths:\n\nRank-1 Update: Uses the evolution path \\(\\mathbf{p}_c\\) to exploit correlations between consecutive steps.\nRank-\\(\\mu\\) Update: Uses the variance of the successful steps. [ ^{(g+1)} = (1 - c_1 - c_) ^{(g)} + c_1 c c^T + c{i=1}^{} w_i ({i:{pop}} - ^{(g)})({i:{pop}} - {(g)})T / ^{(g)2} ]\n\nStep Size Control: The global step size \\(\\sigma^{(g)}\\) is updated using the conjugate evolution path \\(\\mathbf{p}_\\sigma\\) to control the overall scale of the distribution.\n\n\n\n\nQuality Diversity (QD) algorithms optimize for a set of high-performing solutions that are diverse with respect to a user-defined measure. jax_mppi uses CMA-ME (Covariance Matrix Adaptation MAP-Elites), which combines the search power of CMA-ES with the archive maintenance of MAP-Elites.\n\n\nWe seek to find a collection of parameters \\(P = \\{\\theta_1, \\dots, \\theta_N\\}\\) that maximize the quality function \\(f(\\theta) = -\\mathcal{J}(\\theta)\\) while covering the behavior space \\(\\mathcal{B}\\).\nLet \\(\\mathbf{b}(\\theta): \\Theta \\to \\mathcal{B}\\) be a function mapping parameters to a behavior descriptor (e.g., control smoothness, risk sensitivity).\n\n\n\nThe behavior space \\(\\mathcal{B}\\) is discretized into a grid of cells (the archive \\(\\mathcal{A}\\)). Each cell \\(\\mathcal{A}_{\\mathbf{z}}\\) stores the best solution found so far that maps to that cell index \\(\\mathbf{z}\\):\n[ {} = {: (()) = } f() ]\n\n\n\nCMA-ME maintains a set of emitters, which are instances of CMA-ES optimizing for improvement in the archive.\n\nEmission: An emitter samples a candidate \\(\\theta\\) from its distribution \\(\\mathcal{N}(\\mathbf{m}, \\sigma^2 \\mathbf{C})\\).\nEvaluation: Calculate quality \\(f(\\theta)\\) and behavior \\(\\mathbf{b}(\\theta)\\).\nArchive Update:\n\nDetermine the cell index \\(\\mathbf{z} = \\text{index}(\\mathbf{b}(\\theta))\\).\nIf cell \\(\\mathcal{A}_{\\mathbf{z}}\\) is empty or \\(f(\\theta) &gt; f(\\mathcal{A}_{\\mathbf{z}})\\), replace the occupant with \\(\\theta\\).\nCalculate the ‚Äúimprovement‚Äù value \\(\\Delta\\) (e.g., \\(f(\\theta) - f(\\mathcal{A}_{\\mathbf{z}}^{old})\\)).\n\nEmitter Update: The CMA-ES emitter updates its mean and covariance based on the improvement \\(\\Delta\\), guiding the search toward regions of the behavior space where quality can be improved or new cells can be discovered.\n\n\n\n\n\nFor global search over large, potentially non-convex spaces with complex constraints, we utilize Ray Tune. The problem is formulated as:\n[ {{global}} () ]\nwhere \\(\\Theta_{global}\\) can be defined by complex distributions (e.g., Log-Uniform, Categorical).\nRay Tune orchestrates the search using algorithms like:\n\nBayesian Optimization: Uses a Gaussian Process surrogate model \\(P(f \\mid \\mathcal{D})\\) to approximate the objective and an acquisition function \\(a(\\theta)\\) (e.g., Expected Improvement) to select the next sample: [ {next} = {} a() ]\nHyperOpt (TPE): Models \\(p(\\theta \\mid y)\\) using Tree-structured Parzen Estimators to sample promising candidates.\n\nThese methods are particularly useful for ‚Äúwarm-starting‚Äù the local search (CMA-ES) or finding the best family of parameters (e.g., finding the right order of magnitude for \\(\\lambda\\)).",
    "crumbs": [
      "Home",
      "Algorithms",
      "Autotuning Guide"
    ]
  },
  {
    "objectID": "autotuning.html#overview",
    "href": "autotuning.html#overview",
    "title": "Autotuning Guide",
    "section": "",
    "text": "The autotuning process involves three main components:\n\nTunable Parameters: Parameters you want to optimize (e.g., LambdaParameter, NoiseSigmaParameter).\nEvaluation Function: A function that runs MPPI with a specific configuration and returns a cost (and optionally other metrics).\nOptimizer: The algorithm used to search for the best parameters (e.g., CMAESOpt).",
    "crumbs": [
      "Home",
      "Algorithms",
      "Autotuning Guide"
    ]
  },
  {
    "objectID": "autotuning.html#basic-usage-cma-es",
    "href": "autotuning.html#basic-usage-cma-es",
    "title": "Autotuning Guide",
    "section": "",
    "text": "The autotune module provides a simple interface for CMA-ES optimization.\nimport jax.numpy as jnp\nfrom jax_mppi import mppi, autotune\n\n# 1. Setup MPPI\nconfig, state = mppi.create(...)\nholder = autotune.ConfigStateHolder(config, state)\n\n# 2. Define evaluation\ndef evaluate():\n    # Run simulation with holder.config and holder.state\n    # Calculate performance cost\n    return autotune.EvaluationResult(mean_cost=cost, ...)\n\n# 3. Create Tuner\ntuner = autotune.Autotune(\n    params_to_tune=[\n        autotune.LambdaParameter(holder, min_value=0.1),\n        autotune.NoiseSigmaParameter(holder, min_value=0.1),\n    ],\n    evaluate_fn=evaluate,\n    optimizer=autotune.CMAESOpt(population=10),\n)\n\n# 4. Optimize\nbest_result = tuner.optimize_all(iterations=30)\nprint(f\"Best parameters: {best_result.params}\")\nSee examples/autotuning/basic.py and examples/autotuning/pendulum.py for complete running examples.",
    "crumbs": [
      "Home",
      "Algorithms",
      "Autotuning Guide"
    ]
  },
  {
    "objectID": "autotuning.html#advanced-usage",
    "href": "autotuning.html#advanced-usage",
    "title": "Autotuning Guide",
    "section": "",
    "text": "For more complex search spaces or when you want to use advanced schedulers and search algorithms (like HyperOpt or Bayesian Optimization), use autotune_global.\n\nNote: Requires ray[tune], hyperopt, and bayesian-optimization.\n\nfrom ray import tune\nfrom jax_mppi import autotune_global as autog\n\n# Define search space using Ray Tune's API\nparams = [\n    autog.GlobalLambdaParameter(holder, search_space=tune.loguniform(0.1, 10.0)),\n    autog.GlobalNoiseSigmaParameter(holder, search_space=tune.uniform(0.1, 2.0)),\n]\n\ntuner = autog.AutotuneGlobal(\n    params_to_tune=params,\n    evaluate_fn=evaluate,\n    optimizer=autog.RayOptimizer(),\n)\n\nbest = tuner.optimize_all(iterations=100)\n\n\n\nTo find a diverse set of high-performing parameters (e.g., finding parameters that work well for different environments or behavioral descriptors), use autotune_qd.\nfrom jax_mppi import autotune_qd\n\ntuner = autotune.Autotune(\n    params_to_tune=[...],\n    evaluate_fn=evaluate,\n    optimizer=autotune_qd.CMAMEOpt(population=20, bins=10),\n)",
    "crumbs": [
      "Home",
      "Algorithms",
      "Autotuning Guide"
    ]
  },
  {
    "objectID": "autotuning.html#tunable-parameters",
    "href": "autotuning.html#tunable-parameters",
    "title": "Autotuning Guide",
    "section": "",
    "text": "The framework supports tuning the following parameters out-of-the-box:\n\nLambdaParameter: MPPI temperature (\\(\\lambda\\)).\nNoiseSigmaParameter: Exploration noise covariance diagonal.\nMuParameter: Exploration noise mean.\nHorizonParameter: Planning horizon length (resizes internal buffers automatically).\n\nYou can also define custom parameters by subclassing TunableParameter.",
    "crumbs": [
      "Home",
      "Algorithms",
      "Autotuning Guide"
    ]
  },
  {
    "objectID": "autotuning.html#mathematical-formulation",
    "href": "autotuning.html#mathematical-formulation",
    "title": "Autotuning Guide",
    "section": "",
    "text": "This section details the mathematical foundations of the autotuning algorithms available in jax_mppi.\n\n\nThe goal of autotuning is to find the optimal set of hyperparameters \\(\\theta\\) (e.g., temperature \\(\\lambda\\), noise covariance \\(\\Sigma\\), horizon \\(H\\)) that minimizes the expected cost of the control task. We formulate this as an optimization problem:\n[ ^* = _{} () ]\nwhere \\(\\Theta\\) is the admissible hyperparameter space, and the objective function \\(\\mathcal{J}(\\theta)\\) is the expected cumulative cost of the closed-loop system under the MPPI controller parameterized by \\(\\theta\\):\n[ () = {{}()} ]\nHere, \\(\\tau = \\{(\\mathbf{x}_0, \\mathbf{u}_0), \\dots \\}\\) represents a trajectory rollout, and \\(c(\\mathbf{x}, \\mathbf{u})\\) is the task cost function. Since \\(\\mathcal{J}(\\theta)\\) is typically non-convex and noisy (due to the stochastic nature of MPPI and the environment), we employ derivative-free optimization methods.\n\n\n\nCMA-ES is a state-of-the-art evolutionary algorithm for continuous optimization. It models the population of candidate solutions using a multivariate normal distribution \\(\\mathcal{N}(\\mathbf{m}, \\sigma^2 \\mathbf{C})\\).\nThe algorithm proceeds in generations \\(g\\). At each generation:\n\nSampling: We sample \\(\\lambda_{pop}\\) candidate parameters \\(\\theta_i\\) (offspring): [ i ^{(g)} + ^{(g)} (, ^{(g)}) i = 1, , {pop} ]\nEvaluation: Each candidate \\(\\theta_i\\) is evaluated by running an MPPI simulation to estimate \\(\\mathcal{J}(\\theta_i)\\).\nSelection and Recombination: The candidates are sorted by their cost \\(\\mathcal{J}(\\theta_i)\\). The top \\(\\mu\\) candidates (parents) are selected to update the mean: [ ^{(g+1)} = {i=1}^{} w_i {i:_{pop}} ] where \\(w_i\\) are positive weights summing to 1, and \\(\\theta_{i:\\lambda_{pop}}\\) denotes the \\(i\\)-th best candidate.\nCovariance Adaptation: The covariance matrix \\(\\mathbf{C}^{(g)}\\) is updated to increase the likelihood of successful steps. This involves two paths:\n\nRank-1 Update: Uses the evolution path \\(\\mathbf{p}_c\\) to exploit correlations between consecutive steps.\nRank-\\(\\mu\\) Update: Uses the variance of the successful steps. [ ^{(g+1)} = (1 - c_1 - c_) ^{(g)} + c_1 c c^T + c{i=1}^{} w_i ({i:{pop}} - ^{(g)})({i:{pop}} - {(g)})T / ^{(g)2} ]\n\nStep Size Control: The global step size \\(\\sigma^{(g)}\\) is updated using the conjugate evolution path \\(\\mathbf{p}_\\sigma\\) to control the overall scale of the distribution.\n\n\n\n\nQuality Diversity (QD) algorithms optimize for a set of high-performing solutions that are diverse with respect to a user-defined measure. jax_mppi uses CMA-ME (Covariance Matrix Adaptation MAP-Elites), which combines the search power of CMA-ES with the archive maintenance of MAP-Elites.\n\n\nWe seek to find a collection of parameters \\(P = \\{\\theta_1, \\dots, \\theta_N\\}\\) that maximize the quality function \\(f(\\theta) = -\\mathcal{J}(\\theta)\\) while covering the behavior space \\(\\mathcal{B}\\).\nLet \\(\\mathbf{b}(\\theta): \\Theta \\to \\mathcal{B}\\) be a function mapping parameters to a behavior descriptor (e.g., control smoothness, risk sensitivity).\n\n\n\nThe behavior space \\(\\mathcal{B}\\) is discretized into a grid of cells (the archive \\(\\mathcal{A}\\)). Each cell \\(\\mathcal{A}_{\\mathbf{z}}\\) stores the best solution found so far that maps to that cell index \\(\\mathbf{z}\\):\n[ {} = {: (()) = } f() ]\n\n\n\nCMA-ME maintains a set of emitters, which are instances of CMA-ES optimizing for improvement in the archive.\n\nEmission: An emitter samples a candidate \\(\\theta\\) from its distribution \\(\\mathcal{N}(\\mathbf{m}, \\sigma^2 \\mathbf{C})\\).\nEvaluation: Calculate quality \\(f(\\theta)\\) and behavior \\(\\mathbf{b}(\\theta)\\).\nArchive Update:\n\nDetermine the cell index \\(\\mathbf{z} = \\text{index}(\\mathbf{b}(\\theta))\\).\nIf cell \\(\\mathcal{A}_{\\mathbf{z}}\\) is empty or \\(f(\\theta) &gt; f(\\mathcal{A}_{\\mathbf{z}})\\), replace the occupant with \\(\\theta\\).\nCalculate the ‚Äúimprovement‚Äù value \\(\\Delta\\) (e.g., \\(f(\\theta) - f(\\mathcal{A}_{\\mathbf{z}}^{old})\\)).\n\nEmitter Update: The CMA-ES emitter updates its mean and covariance based on the improvement \\(\\Delta\\), guiding the search toward regions of the behavior space where quality can be improved or new cells can be discovered.\n\n\n\n\n\nFor global search over large, potentially non-convex spaces with complex constraints, we utilize Ray Tune. The problem is formulated as:\n[ {{global}} () ]\nwhere \\(\\Theta_{global}\\) can be defined by complex distributions (e.g., Log-Uniform, Categorical).\nRay Tune orchestrates the search using algorithms like:\n\nBayesian Optimization: Uses a Gaussian Process surrogate model \\(P(f \\mid \\mathcal{D})\\) to approximate the objective and an acquisition function \\(a(\\theta)\\) (e.g., Expected Improvement) to select the next sample: [ {next} = {} a() ]\nHyperOpt (TPE): Models \\(p(\\theta \\mid y)\\) using Tree-structured Parzen Estimators to sample promising candidates.\n\nThese methods are particularly useful for ‚Äúwarm-starting‚Äù the local search (CMA-ES) or finding the best family of parameters (e.g., finding the right order of magnitude for \\(\\lambda\\)).",
    "crumbs": [
      "Home",
      "Algorithms",
      "Autotuning Guide"
    ]
  },
  {
    "objectID": "src/theory.html",
    "href": "src/theory.html",
    "title": "Theoretical Background",
    "section": "",
    "text": "This section provides the mathematical foundations for the Model Predictive Path Integral (MPPI) control algorithm and its variants implemented in jax_mppi.",
    "crumbs": [
      "Home",
      "Theory",
      "Theoretical Background"
    ]
  },
  {
    "objectID": "src/theory.html#standard-mppi",
    "href": "src/theory.html#standard-mppi",
    "title": "Theoretical Background",
    "section": "Standard MPPI",
    "text": "Standard MPPI\nModel Predictive Path Integral (MPPI) control is a sampling-based model predictive control algorithm derived from information-theoretic principles. It solves the stochastic optimal control problem by simulating multiple trajectories and updating the control policy based on their costs.\n\nStochastic Optimal Control Problem\nWe consider a discrete-time dynamical system with dynamics:\n\\[\n\\mathbf{x}_{t+1} = f(\\mathbf{x}_t, \\mathbf{u}_t) + \\mathbf{v}_t\n\\]\nwhere \\(\\mathbf{x}_t \\in \\mathbb{R}^{n_x}\\) is the state, \\(\\mathbf{u}_t \\in \\mathbb{R}^{n_u}\\) is the control input, and \\(\\mathbf{v}_t \\sim \\mathcal{N}(0, \\Sigma)\\) is Gaussian noise.\nThe objective is to find the control sequence \\(U = \\{\\mathbf{u}_0, \\dots, \\mathbf{u}_{T-1}\\}\\) that minimizes the expected cost:\n\\[\nJ(U) = \\mathbb{E} \\left[ \\phi(\\mathbf{x}_T) + \\sum_{t=0}^{T-1} \\left( q(\\mathbf{x}_t) + \\frac{1}{2} \\mathbf{u}_t^T \\Sigma^{-1} \\mathbf{u}_t \\right) \\right]\n\\]\nwhere \\(\\phi(\\mathbf{x}_T)\\) is the terminal cost and \\(q(\\mathbf{x}_t)\\) is the state-dependent running cost. The term \\(\\frac{1}{2} \\mathbf{u}_t^T \\Sigma^{-1} \\mathbf{u}_t\\) represents the control effort cost.\n\n\nInformation Theoretic Derivation\nMPPI relies on the duality between free energy and relative entropy (KL divergence). The optimal control distribution \\(p^*\\) is proportional to the exponential of the trajectory cost:\n\\[\np^*(\\tau) \\propto \\exp\\left(-\\frac{1}{\\lambda} S(\\tau)\\right)\n\\]\nwhere \\(S(\\tau)\\) is the cost of a trajectory \\(\\tau\\) and \\(\\lambda\\) is a temperature parameter.\n\n\nUpdate Law\nIn practice, we approximate the optimal control by sampling \\(K\\) trajectories around a nominal control sequence \\(\\mathbf{u}_{nom}\\). For each sample \\(k\\), we apply a perturbation \\(\\epsilon_k \\sim \\mathcal{N}(0, \\Sigma)\\):\n\\[\n\\mathbf{u}_{t, k} = \\mathbf{u}_{nom, t} + \\epsilon_{t, k}\n\\]\nThe cost for the \\(k\\)-th trajectory is computed as:\n\\[\nS_k = \\phi(\\mathbf{x}_{T, k}) + \\sum_{t=0}^{T-1} \\left( q(\\mathbf{x}_{t, k}) + \\lambda \\mathbf{u}_{nom, t}^T \\Sigma^{-1} \\epsilon_{t, k} \\right)\n\\]\nThe weights for each trajectory are computed using the softmax function:\n\\[\nw_k = \\frac{\\exp(-\\frac{1}{\\lambda} (S_k - \\beta))}{\\sum_{j=1}^K \\exp(-\\frac{1}{\\lambda} (S_j - \\beta))}\n\\]\nwhere \\(\\beta = \\min_k S_k\\) for numerical stability.\nThe control sequence is then updated by computing the weighted average of the perturbations:\n\\[\n\\mathbf{u}_{new, t} = \\mathbf{u}_{nom, t} + \\sum_{k=1}^K w_k \\epsilon_{t, k}\n\\]",
    "crumbs": [
      "Home",
      "Theory",
      "Theoretical Background"
    ]
  },
  {
    "objectID": "src/theory.html#smooth-mppi-smppi",
    "href": "src/theory.html#smooth-mppi-smppi",
    "title": "Theoretical Background",
    "section": "Smooth MPPI (SMPPI)",
    "text": "Smooth MPPI (SMPPI)\nStandard MPPI assumes the control inputs are independent across time steps, which can lead to jerky or non-smooth control signals. Smooth MPPI (SMPPI) addresses this by lifting the control problem to a higher-order space (e.g., controlling acceleration instead of velocity).\n\nLifted Control Space\nIn SMPPI, the optimizer works in a lifted control space where the nominal trajectory \\(\\mathbf{U} = \\{\\mathbf{u}_0, \\dots, \\mathbf{u}_{T-1}\\}\\) represents the derivative of the actual action (e.g., jerk when controlling acceleration, or acceleration when controlling velocity). The actual action \\(\\mathbf{a}_t\\) applied to the plant is obtained by numerical integration:\n\\[\n\\mathbf{a}_{t+1} = \\mathbf{a}_t + \\mathbf{u}_t \\, \\Delta t\n\\]\nThis ensures temporal coherence: even large perturbations \\(\\epsilon_t\\) in \\(\\mathbf{u}\\)-space produce smooth changes in \\(\\mathbf{a}\\)-space because they are integrated.\n\n\nPerturbation and Effective Noise\nNoise is sampled in the lifted space \\(\\epsilon_t^{(k)} \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma)\\), producing perturbed commands:\n\\[\n\\tilde{\\mathbf{u}}_t^{(k)} = \\text{clip}\\!\\left(\\mathbf{u}_t + \\epsilon_t^{(k)},\\; \\mathbf{u}_{\\min},\\; \\mathbf{u}_{\\max}\\right)\n\\]\nThe corresponding perturbed actions are:\n\\[\n\\tilde{\\mathbf{a}}_t^{(k)} = \\text{clip}\\!\\left(\\mathbf{a}_t + \\tilde{\\mathbf{u}}_t^{(k)} \\Delta t,\\; \\mathbf{a}_{\\min},\\; \\mathbf{a}_{\\max}\\right)\n\\]\nBecause clipping may alter the perturbation, the effective noise used for importance weighting is back-computed:\n\\[\n\\hat{\\epsilon}_t^{(k)} = \\frac{\\tilde{\\mathbf{a}}_t^{(k)} - \\mathbf{a}_t}{\\Delta t} - \\mathbf{u}_t\n\\]\nThis ensures the noise cost term \\(\\hat{\\epsilon}^T \\Sigma^{-1} \\hat{\\epsilon}\\) reflects the actual perturbation applied to the system.\n\n\nSmoothness Cost\nSMPPI adds an explicit penalty on temporal changes in the action sequence:\n\\[\nJ_{\\text{smooth}}^{(k)} = w_s \\sum_{t=0}^{T-2} \\left\\| \\tilde{\\mathbf{a}}_{t+1}^{(k)} - \\tilde{\\mathbf{a}}_t^{(k)} \\right\\|^2\n\\]\nwhere \\(w_s\\) (w_action_seq_cost in code) controls the smoothness weight.\n\n\nTotal Cost Decomposition\nThe total cost for the \\(k\\)-th sample combines three terms:\n\\[\nJ_{\\text{total}}^{(k)} = \\underbrace{J_{\\text{rollout}}^{(k)}}_{\\text{state + terminal}} + \\underbrace{J_{\\text{noise}}^{(k)}}_{\\hat{\\epsilon}^T \\Sigma^{-1} \\hat{\\epsilon}} + \\underbrace{J_{\\text{smooth}}^{(k)}}_{w_s \\|\\Delta \\mathbf{a}\\|^2}\n\\]\nImportance weights and the update law remain identical to standard MPPI:\n\\[\nw_k = \\text{softmax}\\!\\left(-\\frac{1}{\\lambda} J_{\\text{total}}^{(k)}\\right), \\qquad\n\\mathbf{u}_{t}^{new} = \\mathbf{u}_t + \\sum_k w_k \\hat{\\epsilon}_t^{(k)}\n\\]\nAfter updating \\(\\mathbf{U}\\), the action sequence is re-integrated: \\(\\mathbf{a}_t^{new} = \\mathbf{a}_t + \\mathbf{u}_t^{new} \\Delta t\\).",
    "crumbs": [
      "Home",
      "Theory",
      "Theoretical Background"
    ]
  },
  {
    "objectID": "src/theory.html#kernel-mppi-kmppi",
    "href": "src/theory.html#kernel-mppi-kmppi",
    "title": "Theoretical Background",
    "section": "Kernel MPPI (KMPPI)",
    "text": "Kernel MPPI (KMPPI)\nKernel MPPI (KMPPI) parameterizes the control trajectory using a set of basis functions defined by a kernel, rather than optimizing the control input at every time step independently. This reduces the dimensionality of the optimization problem and implicitly enforces smoothness.\n\nRKHS Formulation\nWe assume the control trajectory \\(\\mathbf{u}(t)\\) lies in a Reproducing Kernel Hilbert Space (RKHS) defined by a kernel \\(k(t, t')\\). The control is represented through \\(M\\) support points \\(\\{t_1, \\dots, t_M\\}\\) with associated parameters \\(\\boldsymbol{\\theta} \\in \\mathbb{R}^{M \\times n_u}\\).\nThe full trajectory is recovered via kernel interpolation:\n\\[\n\\mathbf{u}(t) = \\mathbf{W}(t) \\, \\boldsymbol{\\theta}\n\\]\nwhere \\(\\mathbf{W}\\) is the interpolation matrix:\n\\[\n\\mathbf{W} = \\mathbf{K}(t, t_k) \\, \\mathbf{K}(t_k, t_k)^{-1} \\in \\mathbb{R}^{T \\times M}\n\\]\nHere:\n\n\\(\\mathbf{K}(t, t_k) \\in \\mathbb{R}^{T \\times M}\\): kernel evaluated between all trajectory times and support point times\n\\(\\mathbf{K}(t_k, t_k) \\in \\mathbb{R}^{M \\times M}\\): kernel evaluated between support points (Gram matrix)\n\nIn practice, the linear system \\(\\mathbf{K}(t_k,t_k) \\, \\mathbf{w}^T = \\mathbf{K}(t,t_k)^T\\) is solved using a Cholesky factorization (positive-definite kernel).\n\n\nRBF Kernel\nThe default kernel is the Radial Basis Function (Gaussian) kernel:\n\\[\nk(t, t') = \\exp\\!\\left(-\\frac{\\|t - t'\\|^2}{2\\sigma^2}\\right)\n\\]\nThe bandwidth parameter \\(\\sigma\\) controls the smoothness of the interpolated trajectory:\n\nSmall \\(\\sigma\\): each support point only influences nearby time steps, allowing higher-frequency content\nLarge \\(\\sigma\\): broad influence, producing very smooth (low-frequency) trajectories\n\n\n\nSampling in Parameter Space\nInstead of perturbing the full control trajectory \\(\\mathbf{u} \\in \\mathbb{R}^{T \\times n_u}\\), KMPPI samples perturbations in the reduced parameter space \\(\\boldsymbol{\\theta} \\in \\mathbb{R}^{M \\times n_u}\\):\n\\[\n\\delta\\boldsymbol{\\theta}^{(k)} \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma_\\theta), \\qquad\n\\tilde{\\boldsymbol{\\theta}}^{(k)} = \\boldsymbol{\\theta} + \\delta\\boldsymbol{\\theta}^{(k)}\n\\]\nEach perturbed parameter set is interpolated back to trajectory space:\n\\[\n\\tilde{\\mathbf{U}}^{(k)} = \\mathbf{W} \\, \\tilde{\\boldsymbol{\\theta}}^{(k)}\n\\]\nRollout costs are computed using \\(\\tilde{\\mathbf{U}}^{(k)}\\), while the noise cost is evaluated in parameter space:\n\\[\nJ_{\\text{noise}}^{(k)} = \\sum_{m=1}^{M} \\delta\\boldsymbol{\\theta}_m^{(k) T} \\Sigma_\\theta^{-1} \\, \\delta\\boldsymbol{\\theta}_m^{(k)}\n\\]\nThe update is also applied in parameter space:\n\\[\n\\boldsymbol{\\theta}^{new} = \\boldsymbol{\\theta} + \\sum_k w_k \\, \\delta\\boldsymbol{\\theta}^{(k)}\n\\]\n\n\nDimensionality Reduction\nThe key advantage of KMPPI is that \\(M \\ll T\\). For example, with \\(T = 30\\) horizon steps and \\(M = 15\\) support points, the number of parameters to optimize is halved. This:\n\nReduces variance of the MPPI estimator (fewer dimensions to explore)\nImplicitly enforces smoothness through the kernel bandwidth\nLowers computational cost of noise sampling and weight computation\n\nThe trade-off is representational capacity: the kernel determines which trajectory shapes are reachable. The default choice \\(M = T/2\\) with RBF kernel \\(\\sigma = 1.0\\) provides a good balance between expressivity and smoothness.",
    "crumbs": [
      "Home",
      "Theory",
      "Theoretical Background"
    ]
  }
]