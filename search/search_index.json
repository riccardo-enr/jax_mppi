{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"jax_mppi","text":"<p>jax_mppi is a functional, JIT-compilable port of the pytorch_mppi library to JAX. It implements Model Predictive Path Integral (MPPI) control with a focus on performance and composability.</p>"},{"location":"#design-philosophy","title":"Design Philosophy","text":"<p>This library embraces JAX's functional paradigm:</p> <ul> <li>Pure Functions: Core logic is implemented as pure functions <code>command(state, mppi_state) -&gt; (action, mppi_state)</code>.</li> <li>Dataclass State: State is held in <code>jax.tree_util.register_dataclass</code> containers, allowing easy integration with <code>jit</code>, <code>vmap</code>, and <code>grad</code>.</li> <li>No Side Effects: Unlike the PyTorch version, there is no mutable <code>self</code>. State transitions are explicit.</li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Core MPPI: Robust implementation of the standard MPPI algorithm.</li> <li>Smooth MPPI (SMPPI): Maintains action sequences and smoothness costs for better trajectory generation.</li> <li>Kernel MPPI (KMPPI): Uses kernel interpolation for control points, reducing the parameter space.</li> <li>Autotuning: Built-in hyperparameter optimization using CMA-ES, Ray Tune, and Quality Diversity.</li> <li>JAX Integration:</li> <li><code>jax.vmap</code> for efficient batch processing.</li> <li><code>jax.lax.scan</code> for fast horizon loops.</li> <li>Fully compatible with JIT compilation for high-performance control loops.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code># Clone the repository\ngit clone https://github.com/yourusername/jax_mppi.git\ncd jax_mppi\n\n# Install dependencies\npip install -e .\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<pre><code>import jax\nimport jax.numpy as jnp\nfrom jax_mppi import mppi\n\n# Define dynamics and cost functions\ndef dynamics(state, action):\n    # Your dynamics model here\n    return state + action\n\ndef running_cost(state, action):\n    # Your cost function here\n    return jnp.sum(state**2) + jnp.sum(action**2)\n\n# Create configuration and initial state\nconfig, mppi_state = mppi.create(\n    nx=4, nu=2,\n    noise_sigma=jnp.eye(2) * 0.1,\n    horizon=20,\n    lambda_=1.0\n)\n\n# Control loop\nkey = jax.random.PRNGKey(0)\ncurrent_obs = jnp.zeros(4)\n\n# JIT compile the command function for performance\njitted_command = jax.jit(mppi.command, static_argnames=['dynamics', 'running_cost'])\n\nfor _ in range(100):\n    key, subkey = jax.random.split(key)\n    action, mppi_state = jitted_command(\n        config,\n        mppi_state,\n        current_obs,\n        dynamics=dynamics,\n        running_cost=running_cost\n    )\n    # Apply action to environment...\n</code></pre>"},{"location":"#project-structure","title":"Project Structure","text":"<pre><code>jax_mppi/\n\u251c\u2500\u2500 src/jax_mppi/\n\u2502   \u251c\u2500\u2500 mppi.py              # Core MPPI implementation\n\u2502   \u251c\u2500\u2500 smppi.py             # Smooth MPPI variant\n\u2502   \u251c\u2500\u2500 kmppi.py             # Kernel MPPI variant\n\u2502   \u251c\u2500\u2500 types.py             # Type definitions\n\u2502   \u251c\u2500\u2500 autotune.py          # Autotuning core &amp; CMA-ES\n\u2502   \u251c\u2500\u2500 autotune_global.py   # Ray Tune integration\n\u2502   \u2514\u2500\u2500 autotune_qd.py       # Quality Diversity optimization\n\u251c\u2500\u2500 examples/\n\u2502   \u251c\u2500\u2500 pendulum.py          # Pendulum environment example\n\u2502   \u251c\u2500\u2500 autotune_basic.py    # Basic autotuning example\n\u2502   \u251c\u2500\u2500 autotune_pendulum.py # Autotuning pendulum\n\u2502   \u2514\u2500\u2500 smooth_comparison.py # Comparison of MPPI variants\n\u2514\u2500\u2500 tests/                   # Unit and integration tests\n</code></pre>"},{"location":"#roadmap","title":"Roadmap","text":"<p>The development is structured in phases:</p> <ol> <li>Core MPPI: Basic implementation with JAX parity.</li> <li>Integration: Pendulum example and verification.</li> <li>Smooth MPPI: Implementation of smoothness constraints.</li> <li>Kernel MPPI: Kernel-based control parameterization.</li> <li>Comparisons: Benchmarking and visual comparisons.</li> <li>Autotuning: Parameter optimization using CMA-ES, Ray Tune, and QD.</li> </ol>"},{"location":"#credits","title":"Credits","text":"<p>This project is a direct port of pytorch_mppi. We aim to maintain parity with the original implementation while leveraging JAX's unique features for performance and flexibility.</p>"},{"location":"#completed-plans","title":"Completed Plans","text":"<ul> <li>Porting Plan</li> <li>Evosax Integration Plan</li> </ul>"},{"location":"autotuning/","title":"Autotuning Guide","text":"<p>JAX-MPPI includes a robust autotuning framework to optimize MPPI hyperparameters (like temperature \\(\\lambda\\), noise covariance \\(\\Sigma\\), and planning horizon). The framework supports multiple optimization strategies, including CMA-ES, Ray Tune, and Quality Diversity (QD) methods.</p>"},{"location":"autotuning/#overview","title":"Overview","text":"<p>The autotuning process involves three main components:</p> <ol> <li>Tunable Parameters: Parameters you want to optimize (e.g., <code>LambdaParameter</code>, <code>NoiseSigmaParameter</code>).</li> <li>Evaluation Function: A function that runs MPPI with a specific configuration and returns a cost (and optionally other metrics).</li> <li>Optimizer: The algorithm used to search for the best parameters (e.g., <code>CMAESOpt</code>).</li> </ol>"},{"location":"autotuning/#basic-usage-cma-es","title":"Basic Usage (CMA-ES)","text":"<p>The <code>autotune</code> module provides a simple interface for CMA-ES optimization.</p> <pre><code>import jax.numpy as jnp\nfrom jax_mppi import mppi, autotune\n\n# 1. Setup MPPI\nconfig, state = mppi.create(...)\nholder = autotune.ConfigStateHolder(config, state)\n\n# 2. Define evaluation\ndef evaluate():\n    # Run simulation with holder.config and holder.state\n    # Calculate performance cost\n    return autotune.EvaluationResult(mean_cost=cost, ...)\n\n# 3. Create Tuner\ntuner = autotune.Autotune(\n    params_to_tune=[\n        autotune.LambdaParameter(holder, min_value=0.1),\n        autotune.NoiseSigmaParameter(holder, min_value=0.1),\n    ],\n    evaluate_fn=evaluate,\n    optimizer=autotune.CMAESOpt(population=10),\n)\n\n# 4. Optimize\nbest_result = tuner.optimize_all(iterations=30)\nprint(f\"Best parameters: {best_result.params}\")\n</code></pre> <p>See <code>examples/autotune_basic.py</code> and <code>examples/autotune_pendulum.py</code> for complete running examples.</p>"},{"location":"autotuning/#advanced-usage","title":"Advanced Usage","text":""},{"location":"autotuning/#global-optimization-with-ray-tune","title":"Global Optimization with Ray Tune","text":"<p>For more complex search spaces or when you want to use advanced schedulers and search algorithms (like HyperOpt or Bayesian Optimization), use <code>autotune_global</code>.</p> <p>Note: Requires <code>ray[tune]</code>, <code>hyperopt</code>, and <code>bayesian-optimization</code>.</p> <pre><code>from ray import tune\nfrom jax_mppi import autotune_global as autog\n\n# Define search space using Ray Tune's API\nparams = [\n    autog.GlobalLambdaParameter(holder, search_space=tune.loguniform(0.1, 10.0)),\n    autog.GlobalNoiseSigmaParameter(holder, search_space=tune.uniform(0.1, 2.0)),\n]\n\ntuner = autog.AutotuneGlobal(\n    params_to_tune=params,\n    evaluate_fn=evaluate,\n    optimizer=autog.RayOptimizer(),\n)\n\nbest = tuner.optimize_all(iterations=100)\n</code></pre>"},{"location":"autotuning/#quality-diversity-qd","title":"Quality Diversity (QD)","text":"<p>To find a diverse set of high-performing parameters (e.g., finding parameters that work well for different environments or behavioral descriptors), use <code>autotune_qd</code>.</p> <pre><code>from jax_mppi import autotune_qd\n\ntuner = autotune.Autotune(\n    params_to_tune=[...],\n    evaluate_fn=evaluate,\n    optimizer=autotune_qd.CMAMEOpt(population=20, bins=10),\n)\n</code></pre>"},{"location":"autotuning/#tunable-parameters","title":"Tunable Parameters","text":"<p>The framework supports tuning the following parameters out-of-the-box:</p> <ul> <li><code>LambdaParameter</code>: MPPI temperature (\\(\\lambda\\)).</li> <li><code>NoiseSigmaParameter</code>: Exploration noise covariance diagonal.</li> <li><code>MuParameter</code>: Exploration noise mean.</li> <li><code>HorizonParameter</code>: Planning horizon length (resizes internal buffers automatically).</li> </ul> <p>You can also define custom parameters by subclassing <code>TunableParameter</code>.</p>"},{"location":"autotuning/#mathematical-formulation","title":"Mathematical Formulation","text":"<p>This section details the mathematical foundations of the autotuning algorithms available in <code>jax_mppi</code>.</p>"},{"location":"autotuning/#hyperparameter-optimization-problem","title":"Hyperparameter Optimization Problem","text":"<p>The goal of autotuning is to find the optimal set of hyperparameters \\(\\theta\\) (e.g., temperature \\(\\lambda\\), noise covariance \\(\\Sigma\\), horizon \\(H\\)) that minimizes the expected cost of the control task. We formulate this as an optimization problem:</p> \\[ \\theta^* = \\arg\\min_{\\theta \\in \\Theta} \\mathcal{J}(\\theta) \\] <p>where \\(\\Theta\\) is the admissible hyperparameter space, and the objective function \\(\\mathcal{J}(\\theta)\\) is the expected cumulative cost of the closed-loop system under the MPPI controller parameterized by \\(\\theta\\):</p> \\[ \\mathcal{J}(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\text{MPPI}}(\\theta)} \\left[ \\sum_{t=0}^{T_{task}} c(\\mathbf{x}_t, \\mathbf{u}_t) \\right] \\] <p>Here, \\(\\tau = \\{(\\mathbf{x}_0, \\mathbf{u}_0), \\dots \\}\\) represents a trajectory rollout, and \\(c(\\mathbf{x}, \\mathbf{u})\\) is the task cost function. Since \\(\\mathcal{J}(\\theta)\\) is typically non-convex and noisy (due to the stochastic nature of MPPI and the environment), we employ derivative-free optimization methods.</p>"},{"location":"autotuning/#cma-es-covariance-matrix-adaptation-evolution-strategy","title":"CMA-ES (Covariance Matrix Adaptation Evolution Strategy)","text":"<p>CMA-ES is a state-of-the-art evolutionary algorithm for continuous optimization. It models the population of candidate solutions using a multivariate normal distribution \\(\\mathcal{N}(\\mathbf{m}, \\sigma^2 \\mathbf{C})\\).</p> <p>The algorithm proceeds in generations \\(g\\). At each generation:</p> <ol> <li> <p>Sampling: We sample \\(\\lambda_{pop}\\) candidate parameters \\(\\theta_i\\) (offspring):     [     \\theta_i \\sim \\mathbf{m}^{(g)} + \\sigma^{(g)} \\mathcal{N}(\\mathbf{0}, \\mathbf{C}^{(g)}) \\quad \\text{for } i = 1, \\dots, \\lambda_{pop}     ]</p> </li> <li> <p>Evaluation: Each candidate \\(\\theta_i\\) is evaluated by running an MPPI simulation to estimate \\(\\mathcal{J}(\\theta_i)\\).</p> </li> <li> <p>Selection and Recombination: The candidates are sorted by their cost \\(\\mathcal{J}(\\theta_i)\\). The top \\(\\mu\\) candidates (parents) are selected to update the mean:     [     \\mathbf{m}^{(g+1)} = \\sum_{i=1}^{\\mu} w_i \\theta_{i:\\lambda_{pop}}     ]     where \\(w_i\\) are positive weights summing to 1, and \\(\\theta_{i:\\lambda_{pop}}\\) denotes the \\(i\\)-th best candidate.</p> </li> <li> <p>Covariance Adaptation: The covariance matrix \\(\\mathbf{C}^{(g)}\\) is updated to increase the likelihood of successful steps. This involves two paths:</p> <ul> <li>Rank-1 Update: Uses the evolution path \\(\\mathbf{p}_c\\) to exploit correlations between consecutive steps.</li> <li>Rank-\\(\\mu\\) Update: Uses the variance of the successful steps. [ \\mathbf{C}^{(g+1)} = (1 - c_1 - c_\\mu) \\mathbf{C}^{(g)} + c_1 \\mathbf{p}c \\mathbf{p}_c^T + c\\mu \\sum_{i=1}^{\\mu} w_i (\\theta_{i:\\lambda_{pop}} - \\mathbf{m}^{(g)})(\\theta_{i:\\lambda_{pop}} - \\mathbf{m}^{(g)})^T / \\sigma^{(g)2} ]</li> </ul> </li> <li> <p>Step Size Control: The global step size \\(\\sigma^{(g)}\\) is updated using the conjugate evolution path \\(\\mathbf{p}_\\sigma\\) to control the overall scale of the distribution.</p> </li> </ol>"},{"location":"autotuning/#quality-diversity-with-cma-me","title":"Quality Diversity with CMA-ME","text":"<p>Quality Diversity (QD) algorithms optimize for a set of high-performing solutions that are diverse with respect to a user-defined measure. <code>jax_mppi</code> uses CMA-ME (Covariance Matrix Adaptation MAP-Elites), which combines the search power of CMA-ES with the archive maintenance of MAP-Elites.</p>"},{"location":"autotuning/#problem-formulation","title":"Problem Formulation","text":"<p>We seek to find a collection of parameters \\(P = \\{\\theta_1, \\dots, \\theta_N\\}\\) that maximize the quality function \\(f(\\theta) = -\\mathcal{J}(\\theta)\\) while covering the behavior space \\(\\mathcal{B}\\).</p> <p>Let \\(\\mathbf{b}(\\theta): \\Theta \\to \\mathcal{B}\\) be a function mapping parameters to a behavior descriptor (e.g., control smoothness, risk sensitivity).</p>"},{"location":"autotuning/#map-elites-archive","title":"MAP-Elites Archive","text":"<p>The behavior space \\(\\mathcal{B}\\) is discretized into a grid of cells (the archive \\(\\mathcal{A}\\)). Each cell \\(\\mathcal{A}_{\\mathbf{z}}\\) stores the best solution found so far that maps to that cell index \\(\\mathbf{z}\\):</p> \\[ \\mathcal{A}_{\\mathbf{z}} = \\arg\\max_{\\theta: \\text{index}(\\mathbf{b}(\\theta)) = \\mathbf{z}} f(\\theta) \\]"},{"location":"autotuning/#cma-me-algorithm","title":"CMA-ME Algorithm","text":"<p>CMA-ME maintains a set of emitters, which are instances of CMA-ES optimizing for improvement in the archive.</p> <ol> <li>Emission: An emitter samples a candidate \\(\\theta\\) from its distribution \\(\\mathcal{N}(\\mathbf{m}, \\sigma^2 \\mathbf{C})\\).</li> <li>Evaluation: Calculate quality \\(f(\\theta)\\) and behavior \\(\\mathbf{b}(\\theta)\\).</li> <li>Archive Update:<ul> <li>Determine the cell index \\(\\mathbf{z} = \\text{index}(\\mathbf{b}(\\theta))\\).</li> <li>If cell \\(\\mathcal{A}_{\\mathbf{z}}\\) is empty or \\(f(\\theta) &gt; f(\\mathcal{A}_{\\mathbf{z}})\\), replace the occupant with \\(\\theta\\).</li> <li>Calculate the \"improvement\" value \\(\\Delta\\) (e.g., \\(f(\\theta) - f(\\mathcal{A}_{\\mathbf{z}}^{old})\\)).</li> </ul> </li> <li>Emitter Update: The CMA-ES emitter updates its mean and covariance based on the improvement \\(\\Delta\\), guiding the search toward regions of the behavior space where quality can be improved or new cells can be discovered.</li> </ol>"},{"location":"autotuning/#global-optimization-with-ray-tune_1","title":"Global Optimization with Ray Tune","text":"<p>For global search over large, potentially non-convex spaces with complex constraints, we utilize Ray Tune. The problem is formulated as:</p> \\[ \\min_{\\theta \\in \\Theta_{global}} \\mathcal{J}(\\theta) \\] <p>where \\(\\Theta_{global}\\) can be defined by complex distributions (e.g., Log-Uniform, Categorical).</p> <p>Ray Tune orchestrates the search using algorithms like:</p> <ul> <li>Bayesian Optimization: Uses a Gaussian Process surrogate model \\(P(f \\mid \\mathcal{D})\\) to approximate the objective and an acquisition function \\(a(\\theta)\\) (e.g., Expected Improvement) to select the next sample:     [     \\theta_{next} = \\arg\\max_{\\theta} a(\\theta)     ]</li> <li>HyperOpt (TPE): Models \\(p(\\theta \\mid y)\\) using Tree-structured Parzen Estimators to sample promising candidates.</li> </ul> <p>These methods are particularly useful for \"warm-starting\" the local search (CMA-ES) or finding the best family of parameters (e.g., finding the right order of magnitude for \\(\\lambda\\)).</p>"},{"location":"releasing/","title":"Releasing Guide","text":"<p>This guide describes how to release a new version of <code>jax_mppi</code> to PyPI.</p>"},{"location":"releasing/#prerequisites","title":"Prerequisites","text":"<p>The release process is automated using GitHub Actions, but it requires the repository to be configured as a Trusted Publisher on PyPI.</p>"},{"location":"releasing/#pypi-trusted-publisher-setup","title":"PyPI Trusted Publisher Setup","text":"<ol> <li>Log in to your PyPI account.</li> <li>Go to Publishing in your project settings (or create a new project if this is the first release).</li> <li>Add a new Trusted Publisher.</li> <li>Select GitHub.</li> <li>Enter the following details:<ul> <li>Owner: <code>riccardo-enr</code></li> <li>Repository name: <code>jax_mppi</code></li> <li>Workflow name: <code>publish.yml</code></li> <li>Environment name: (Leave empty)</li> </ul> </li> <li>Click Add.</li> </ol> <p>This allows the GitHub Action to authenticate with PyPI using OIDC tokens without needing a long-lived API token or password.</p>"},{"location":"releasing/#release-process","title":"Release Process","text":"<p>To release a new version:</p> <ol> <li> <p>Update Version:     Update the version number in <code>pyproject.toml</code>:</p> <p><code>toml [project] version = \"0.1.6\"  # Example version</code></p> </li> <li> <p>Commit and Push:     Commit the version change and push to <code>main</code>.</p> <p><code>bash git add pyproject.toml git commit -m \"Bump version to 0.1.6\" git push origin main</code></p> </li> <li> <p>Create Tag:     Create a git tag for the release. The tag must start with <code>v</code>.</p> <p><code>bash git tag v0.1.6 git push origin v0.1.6</code></p> </li> <li> <p>Wait for Action:     The <code>Publish to PyPI</code> GitHub Action will automatically run when the tag is pushed. It will build the package and upload it to PyPI.</p> </li> <li> <p>Verify:     Check the PyPI page to confirm the new version is available.</p> </li> </ol>"},{"location":"testing/","title":"Testing Guide","text":"<p>This guide explains the testing stack for <code>jax_mppi</code> and provides instructions on how to run and write tests.</p>"},{"location":"testing/#running-tests","title":"Running Tests","text":"<p>The project uses <code>pytest</code> for running tests. You can run all tests using <code>uv</code>:</p> <pre><code>uv run pytest\n</code></pre> <p>To run a specific test file:</p> <pre><code>uv run pytest tests/test_mppi.py\n</code></pre> <p>To run a specific test case:</p> <pre><code>uv run pytest tests/test_mppi.py::TestMPPICommand::test_command_returns_correct_shapes\n</code></pre>"},{"location":"testing/#test-suite-structure","title":"Test Suite Structure","text":"<p>The tests are located in the <code>tests/</code> directory and mirror the source code structure where appropriate. The test suite is divided into several files, each covering a specific flavor or aspect of the library.</p>"},{"location":"testing/#core-mppi-flavors","title":"Core MPPI Flavors","text":"<ul> <li><code>tests/test_mppi.py</code>: Tests for the base MPPI implementation (<code>jax_mppi.mppi</code>).</li> <li>Goal: Ensure the correctness of the core algorithm, state management, and configuration options.</li> <li> <p>Scope:</p> <ul> <li>Initialization: Verifies that <code>create()</code> returns correct shapes and types for <code>config</code> and <code>state</code>.</li> <li>Command Generation: Tests the <code>command()</code> function to ensure it generates valid actions within bounds and correctly updates the state.</li> <li>Configuration Options: Validates various settings like <code>u_per_command</code> (multi-step control), <code>step_dependent_dynamics</code> (time-varying systems), <code>sample_null_action</code> (ensuring baseline inclusion), and <code>u_scale</code> (control authority scaling).</li> <li>Integration: Includes basic convergence tests to verify that the cost decreases over iterations (e.g., <code>TestMPPIIntegration</code>).</li> </ul> </li> <li> <p><code>tests/test_smppi.py</code>: Tests for Smooth MPPI (<code>jax_mppi.smppi</code>).</p> </li> <li>Goal: Verify that the \"smooth\" variant correctly operates in the lifted velocity control space and produces continuous action sequences.</li> <li> <p>Scope:</p> <ul> <li>Lifted Space: Checks that the internal state (<code>U</code>) represents control velocity/acceleration, while <code>action_sequence</code> represents the integrated actions.</li> <li>Smoothness: Verifies that the smoothness cost penalty (<code>w_action_seq_cost</code>) effectively reduces action variance.</li> <li>Bounds: Tests that bounds are respected for both the control velocity (<code>u_min</code>/<code>u_max</code>) and the final action (<code>action_min</code>/<code>action_max</code>).</li> <li>Continuity: checks that the <code>shift</code> operation maintains continuity in the action space, preventing jumps during receding horizon updates.</li> </ul> </li> <li> <p><code>tests/test_kmppi.py</code>: Tests for Kernel MPPI (<code>jax_mppi.kmppi</code>).</p> </li> <li>Goal: Ensure that kernel-based interpolation works correctly and that optimization occurs effectively in the reduced control point space.</li> <li>Scope:<ul> <li>Kernels: Tests the properties of time-domain kernels (e.g., <code>RBFKernel</code>), such as shape and distance decay.</li> <li>Interpolation: Verifies that control points (<code>theta</code>) are correctly mapped to full trajectories (<code>U</code>) via <code>_kernel_interpolate</code>, preserving values at control points.</li> <li>Optimization: Checks that the MPPI update rule is applied to the control points (<code>theta</code>) rather than the full trajectory.</li> <li>Smoothness: Confirms that the resulting trajectories are smooth due to the kernel properties (e.g., by checking second derivatives).</li> </ul> </li> </ul>"},{"location":"testing/#integration-examples","title":"Integration &amp; Examples","text":"<ul> <li><code>tests/test_pendulum.py</code>: End-to-end integration tests using a Pendulum environment.</li> <li>Goal: Validate that the algorithms can solve a concrete, non-linear control task.</li> <li>Scope:<ul> <li>Stabilization: Tests if MPPI can stabilize the pendulum at the upright position.</li> <li>Swing-up: Tests the more difficult task of swinging up from a hanging position.</li> <li>Physics: Sanity checks the pendulum dynamics and cost functions.</li> </ul> </li> </ul>"},{"location":"testing/#autotuning","title":"Autotuning","text":"<ul> <li><code>tests/test_autotune.py</code>: Unit tests for the autotuning framework (<code>jax_mppi.autotune</code>).</li> <li>Goal: Verify the components of the hyperparameter optimization system.</li> <li><code>tests/test_autotune_integration.py</code>: Integration tests for autotuning.</li> <li>Goal: Ensure that the autotuner can successfully improve performance on a benchmark task (finding better parameters than the default).</li> </ul>"},{"location":"testing/#writing-new-tests","title":"Writing New Tests","text":"<p>When adding new features or fixing bugs, please add corresponding tests.</p> <ol> <li>Locate the appropriate test file: If you are modifying <code>mppi.py</code>, add tests to <code>tests/test_mppi.py</code>.</li> <li>Use Class-Based Structure: Group related tests into classes (e.g., <code>TestMPPIBasics</code>, <code>TestMPPICommand</code>).</li> <li>Property-Based Testing: Where possible, test properties (e.g., \"output shape depends on input shape in this way\") rather than just hardcoded values.</li> <li>Integration Tests: For significant algorithmic changes, ensure that <code>tests/test_pendulum.py</code> still passes or add a similar simple control task to verify efficacy.</li> <li>JAX Compatibility: Ensure tests check that functions can be JIT-compiled if they are intended to be used within <code>jax.jit</code>.</li> </ol>"},{"location":"testing/#example-test-case","title":"Example Test Case","text":"<pre><code>def test_new_feature(self):\n    nx, nu = 2, 1\n    config, state = mppi.create(nx=nx, nu=nu, noise_sigma=jnp.eye(nu))\n\n    # ... perform action ...\n    action, new_state = mppi.command(config, state, ...)\n\n    # ... assert expected behavior ...\n    assert action.shape == (nu,)\n</code></pre>"},{"location":"theory/","title":"Theoretical Background","text":"<p>This section provides the mathematical foundations for the Model Predictive Path Integral (MPPI) control algorithm and its variants implemented in <code>jax_mppi</code>.</p>"},{"location":"theory/#standard-mppi","title":"Standard MPPI","text":"<p>Model Predictive Path Integral (MPPI) control is a sampling-based model predictive control algorithm derived from information-theoretic principles. It solves the stochastic optimal control problem by simulating multiple trajectories and updating the control policy based on their costs.</p>"},{"location":"theory/#stochastic-optimal-control-problem","title":"Stochastic Optimal Control Problem","text":"<p>We consider a discrete-time dynamical system with dynamics:</p> \\[ \\mathbf{x}_{t+1} = f(\\mathbf{x}_t, \\mathbf{u}_t) + \\mathbf{v}_t \\] <p>where \\(\\mathbf{x}_t \\in \\mathbb{R}^{n_x}\\) is the state, \\(\\mathbf{u}_t \\in \\mathbb{R}^{n_u}\\) is the control input, and \\(\\mathbf{v}_t \\sim \\mathcal{N}(0, \\Sigma)\\) is Gaussian noise.</p> <p>The objective is to find the control sequence \\(U = \\{\\mathbf{u}_0, \\dots, \\mathbf{u}_{T-1}\\}\\) that minimizes the expected cost:</p> \\[ J(U) = \\mathbb{E} \\left[ \\phi(\\mathbf{x}_T) + \\sum_{t=0}^{T-1} \\left( q(\\mathbf{x}_t) + \\frac{1}{2} \\mathbf{u}_t^T \\Sigma^{-1} \\mathbf{u}_t \\right) \\right] \\] <p>where \\(\\phi(\\mathbf{x}_T)\\) is the terminal cost and \\(q(\\mathbf{x}_t)\\) is the state-dependent running cost. The term \\(\\frac{1}{2} \\mathbf{u}_t^T \\Sigma^{-1} \\mathbf{u}_t\\) represents the control effort cost.</p>"},{"location":"theory/#information-theoretic-derivation","title":"Information Theoretic Derivation","text":"<p>MPPI relies on the duality between free energy and relative entropy (KL divergence). The optimal control distribution \\(p^*\\) is proportional to the exponential of the trajectory cost:</p> \\[ p^*(\\tau) \\propto \\exp\\left(-\\frac{1}{\\lambda} S(\\tau)\\right) \\] <p>where \\(S(\\tau)\\) is the cost of a trajectory \\(\\tau\\) and \\(\\lambda\\) is a temperature parameter.</p>"},{"location":"theory/#update-law","title":"Update Law","text":"<p>In practice, we approximate the optimal control by sampling \\(K\\) trajectories around a nominal control sequence \\(\\mathbf{u}_{nom}\\). For each sample \\(k\\), we apply a perturbation \\(\\epsilon_k \\sim \\mathcal{N}(0, \\Sigma)\\):</p> \\[ \\mathbf{u}_{t, k} = \\mathbf{u}_{nom, t} + \\epsilon_{t, k} \\] <p>The cost for the \\(k\\)-th trajectory is computed as:</p> \\[ S_k = \\phi(\\mathbf{x}_{T, k}) + \\sum_{t=0}^{T-1} \\left( q(\\mathbf{x}_{t, k}) + \\lambda \\mathbf{u}_{nom, t}^T \\Sigma^{-1} \\epsilon_{t, k} \\right) \\] <p>The weights for each trajectory are computed using the softmax function:</p> \\[ w_k = \\frac{\\exp(-\\frac{1}{\\lambda} (S_k - \\beta))}{\\sum_{j=1}^K \\exp(-\\frac{1}{\\lambda} (S_j - \\beta))} \\] <p>where \\(\\beta = \\min_k S_k\\) for numerical stability.</p> <p>The control sequence is then updated by computing the weighted average of the perturbations:</p> \\[ \\mathbf{u}_{new, t} = \\mathbf{u}_{nom, t} + \\sum_{k=1}^K w_k \\epsilon_{t, k} \\]"},{"location":"theory/#smooth-mppi-smppi","title":"Smooth MPPI (SMPPI)","text":"<p>Standard MPPI assumes the control inputs are independent across time steps, which can lead to jerky or non-smooth control signals. Smooth MPPI (SMPPI) addresses this by lifting the control problem to a higher-order space (e.g., controlling acceleration instead of velocity).</p>"},{"location":"theory/#state-augmentation","title":"State Augmentation","text":"<p>In SMPPI, the nominal trajectory \\(U\\) represents the derivative of the actual action (e.g., acceleration). The actual action \\(\\mathbf{a}_t\\) is part of the state or computed by integrating \\(U\\).</p> <p>Let \\(\\mathbf{u}_t\\) be the command at time \\(t\\) (from the optimizer). The action applied to the system is \\(\\mathbf{a}_t\\), updated as:</p> \\[ \\mathbf{a}_{t+1} = \\mathbf{a}_t + \\mathbf{u}_t \\Delta t \\]"},{"location":"theory/#smoothness-cost","title":"Smoothness Cost","text":"<p>SMPPI explicitly penalizes changes in the action sequence to encourage smoothness. The cost function includes a term for the magnitude of the command \\(\\mathbf{u}_t\\) (which corresponds to the change in action):</p> \\[ J_{smooth} = \\sum_{t=0}^{T-1} ||\\mathbf{u}_t||^2 = \\sum_{t=0}^{T-1} ||\\frac{\\mathbf{a}_{t+1} - \\mathbf{a}_t}{\\Delta t}||^2 \\] <p>This formulation ensures that the generated trajectories are smooth and feasible for systems with actuation limits or bandwidth constraints.</p>"},{"location":"theory/#kernel-mppi-kmppi","title":"Kernel MPPI (KMPPI)","text":"<p>Kernel MPPI (KMPPI) parameterizes the control trajectory using a set of basis functions or kernels, rather than optimizing the control input at every time step independently. This reduces the dimensionality of the optimization problem and implicitly enforces smoothness.</p>"},{"location":"theory/#rkhs-formulation","title":"RKHS Formulation","text":"<p>We assume the control trajectory \\(\\mathbf{u}(t)\\) lies in a Reproducing Kernel Hilbert Space (RKHS) defined by a kernel \\(k(t, t')\\). The control is represented as a linear combination of basis functions centered at support points \\(t_i\\):</p> \\[ \\mathbf{u}(t) = \\sum_{i=1}^{M} \\alpha_i k(t, t_i) \\] <p>where \\(M\\) is the number of support points (often \\(M &lt; T\\)), and \\(\\alpha_i\\) are the weights (parameters) to be optimized.</p>"},{"location":"theory/#optimization","title":"Optimization","text":"<p>Instead of perturbing the control inputs \\(\\mathbf{u}_t\\) directly, KMPPI perturbs the parameters \\(\\alpha_i\\) (or equivalent control points).</p> <p>Let \\(\\theta\\) represent the parameters. We sample perturbations \\(\\delta \\theta_k \\sim \\mathcal{N}(0, \\Sigma_\\theta)\\). The corresponding control trajectory is:</p> \\[ \\mathbf{u}_k(t) = \\text{Interpolate}(\\theta + \\delta \\theta_k) \\] <p>The update rule is applied to \\(\\theta\\):</p> \\[ \\theta_{new} = \\theta_{nom} + \\sum_{k=1}^K w_k \\delta \\theta_k \\] <p>By choosing an appropriate kernel (e.g., RBF kernel), we can control the smoothness and frequency content of the resulting trajectories.</p>"},{"location":"api/autotune/","title":"Autotune","text":"<p>Autotuning framework for JAX-MPPI.</p> <p>This module provides automatic hyperparameter optimization for MPPI controllers using multiple optimization backends. Supports tuning of: - Lambda (temperature parameter) - Noise sigma (exploration covariance) - Noise mu (exploration mean) - Horizon (planning horizon)</p> <p>Compatible with MPPI, SMPPI, and KMPPI variants.</p> <p>Optimizers: - CMAESOpt (from <code>cma</code> library) - Classic CMA-ES - CMAESOpt, SepCMAESOpt, OpenESOpt (from <code>evosax</code>) - JAX-native, GPU-accelerated</p> Example with cma <p>import jax_mppi as jmppi config, state = jmppi.mppi.create(...)</p> <p>holder = jmppi.autotune.ConfigStateHolder(config, state) def evaluate(): ...     # Run MPPI rollout, return cost ...     return jmppi.autotune.EvaluationResult(...)</p> <p>tuner = jmppi.autotune.Autotune( ...     params_to_tune=[ ...         jmppi.autotune.LambdaParameter(holder), ...         jmppi.autotune.NoiseSigmaParameter(holder), ...     ], ...     evaluate_fn=evaluate, ...     optimizer=jmppi.autotune.CMAESOpt(population=10), ... ) best = tuner.optimize_all(iterations=30)</p> <p>Example with evosax (JAX-native, GPU-accelerated):     &gt;&gt;&gt; from jax_mppi import autotune_evosax     &gt;&gt;&gt; tuner = jmppi.autotune.Autotune(     ...     params_to_tune=[...],     ...     evaluate_fn=evaluate,     ...     optimizer=autotune_evosax.CMAESOpt(population=10),  # JAX-native     ... )     &gt;&gt;&gt; best = tuner.optimize_all(iterations=30)</p>"},{"location":"api/autotune/#jax_mppi.autotune.Autotune","title":"<code>Autotune</code>","text":"<p>Main autotuning orchestrator.</p> <p>Manages parameter optimization using a specified optimizer. Handles flattening/unflattening of parameters and result tracking.</p> Example <p>holder = ConfigStateHolder(config, state) tuner = Autotune( ...     params_to_tune=[LambdaParameter(holder), NoiseSigmaParameter(holder)], ...     evaluate_fn=my_evaluate_fn, ...     optimizer=CMAESOpt(population=10), ... ) best = tuner.optimize_all(iterations=30)</p> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>class Autotune:\n    \"\"\"Main autotuning orchestrator.\n\n    Manages parameter optimization using a specified optimizer.\n    Handles flattening/unflattening of parameters and result tracking.\n\n    Example:\n        &gt;&gt;&gt; holder = ConfigStateHolder(config, state)\n        &gt;&gt;&gt; tuner = Autotune(\n        ...     params_to_tune=[LambdaParameter(holder), NoiseSigmaParameter(holder)],\n        ...     evaluate_fn=my_evaluate_fn,\n        ...     optimizer=CMAESOpt(population=10),\n        ... )\n        &gt;&gt;&gt; best = tuner.optimize_all(iterations=30)\n    \"\"\"\n\n    def __init__(\n        self,\n        params_to_tune: list[TunableParameter],\n        evaluate_fn: Callable[[], EvaluationResult],\n        optimizer: Optional[Optimizer] = None,\n        reload_state_fn: Optional[Callable] = None,\n    ):\n        \"\"\"Initialize autotuner.\n\n        Args:\n            params_to_tune: List of parameters to optimize\n            evaluate_fn: Function that runs MPPI and returns EvaluationResult\n            optimizer: Optimizer instance (defaults to CMAESOpt)\n            reload_state_fn: Optional function to reload state (for multiprocessing)\n        \"\"\"\n        self.params_to_tune = params_to_tune\n        self.evaluate_fn = evaluate_fn\n        self.reload_state_fn = reload_state_fn\n        self.optimizer = optimizer if optimizer is not None else CMAESOpt()\n\n        self.best_result: Optional[EvaluationResult] = None\n        self.iteration_count = 0\n\n        # Setup optimizer with initial parameters\n        initial_params = self.flatten_params()\n\n        def _wrapped_evaluate(x: np.ndarray) -&gt; EvaluationResult:\n            # Unflatten and apply parameters\n            param_dict = self.unflatten_params(x, apply=True)\n\n            # Reload state if needed (for multiprocessing)\n            if self.reload_state_fn is not None:\n                self.reload_state_fn()\n\n            # Evaluate with current parameters\n            result = self.evaluate_fn()\n\n            # Track iteration\n            result = result._replace(\n                params=param_dict,\n                iteration=self.iteration_count,\n            )\n            self.iteration_count += 1\n\n            # Update best result\n            if (\n                self.best_result is None\n                or result.mean_cost &lt; self.best_result.mean_cost\n            ):\n                self.best_result = result\n\n            return result\n\n        self.optimizer.setup_optimization(initial_params, _wrapped_evaluate)\n\n    def flatten_params(self) -&gt; np.ndarray:\n        \"\"\"Flatten all parameters to 1D array.\"\"\"\n        return flatten_params(self.params_to_tune)\n\n    def unflatten_params(\n        self, x: np.ndarray, apply: bool = True\n    ) -&gt; dict[str, np.ndarray]:\n        \"\"\"Unflatten parameter vector and optionally apply.\"\"\"\n        return unflatten_params(x, self.params_to_tune, apply=apply)\n\n    def apply_parameters(self, param_values: dict[str, np.ndarray]) -&gt; None:\n        \"\"\"Apply parameter dictionary to MPPI config/state.\n\n        Args:\n            param_values: Dict mapping parameter names to values\n        \"\"\"\n        for param in self.params_to_tune:\n            if param.name() in param_values:\n                value = param_values[param.name()]\n                param.apply_parameter_value(value)\n\n    def optimize_step(self) -&gt; EvaluationResult:\n        \"\"\"Execute one optimization iteration.\n\n        Returns:\n            Best result from this step\n        \"\"\"\n        return self.optimizer.optimize_step()\n\n    def optimize_all(self, iterations: int) -&gt; EvaluationResult:\n        \"\"\"Run full optimization loop.\n\n        Args:\n            iterations: Number of optimization iterations\n\n        Returns:\n            Best result found across all iterations\n        \"\"\"\n        for _ in range(iterations):\n            self.optimize_step()\n\n        return self.get_best_result()\n\n    def get_best_result(self) -&gt; EvaluationResult:\n        \"\"\"Get best result found so far.\n\n        Returns:\n            Best evaluation result\n\n        Raises:\n            RuntimeError: If no results have been evaluated yet\n        \"\"\"\n        if not isinstance(self.best_result, EvaluationResult):\n            raise RuntimeError(\"No results available yet\")\n        return self.best_result\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.Autotune.__init__","title":"<code>__init__(params_to_tune, evaluate_fn, optimizer=None, reload_state_fn=None)</code>","text":"<p>Initialize autotuner.</p> <p>Parameters:</p> Name Type Description Default <code>params_to_tune</code> <code>list[TunableParameter]</code> <p>List of parameters to optimize</p> required <code>evaluate_fn</code> <code>Callable[[], EvaluationResult]</code> <p>Function that runs MPPI and returns EvaluationResult</p> required <code>optimizer</code> <code>Optional[Optimizer]</code> <p>Optimizer instance (defaults to CMAESOpt)</p> <code>None</code> <code>reload_state_fn</code> <code>Optional[Callable]</code> <p>Optional function to reload state (for multiprocessing)</p> <code>None</code> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>def __init__(\n    self,\n    params_to_tune: list[TunableParameter],\n    evaluate_fn: Callable[[], EvaluationResult],\n    optimizer: Optional[Optimizer] = None,\n    reload_state_fn: Optional[Callable] = None,\n):\n    \"\"\"Initialize autotuner.\n\n    Args:\n        params_to_tune: List of parameters to optimize\n        evaluate_fn: Function that runs MPPI and returns EvaluationResult\n        optimizer: Optimizer instance (defaults to CMAESOpt)\n        reload_state_fn: Optional function to reload state (for multiprocessing)\n    \"\"\"\n    self.params_to_tune = params_to_tune\n    self.evaluate_fn = evaluate_fn\n    self.reload_state_fn = reload_state_fn\n    self.optimizer = optimizer if optimizer is not None else CMAESOpt()\n\n    self.best_result: Optional[EvaluationResult] = None\n    self.iteration_count = 0\n\n    # Setup optimizer with initial parameters\n    initial_params = self.flatten_params()\n\n    def _wrapped_evaluate(x: np.ndarray) -&gt; EvaluationResult:\n        # Unflatten and apply parameters\n        param_dict = self.unflatten_params(x, apply=True)\n\n        # Reload state if needed (for multiprocessing)\n        if self.reload_state_fn is not None:\n            self.reload_state_fn()\n\n        # Evaluate with current parameters\n        result = self.evaluate_fn()\n\n        # Track iteration\n        result = result._replace(\n            params=param_dict,\n            iteration=self.iteration_count,\n        )\n        self.iteration_count += 1\n\n        # Update best result\n        if (\n            self.best_result is None\n            or result.mean_cost &lt; self.best_result.mean_cost\n        ):\n            self.best_result = result\n\n        return result\n\n    self.optimizer.setup_optimization(initial_params, _wrapped_evaluate)\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.Autotune.apply_parameters","title":"<code>apply_parameters(param_values)</code>","text":"<p>Apply parameter dictionary to MPPI config/state.</p> <p>Parameters:</p> Name Type Description Default <code>param_values</code> <code>dict[str, ndarray]</code> <p>Dict mapping parameter names to values</p> required Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>def apply_parameters(self, param_values: dict[str, np.ndarray]) -&gt; None:\n    \"\"\"Apply parameter dictionary to MPPI config/state.\n\n    Args:\n        param_values: Dict mapping parameter names to values\n    \"\"\"\n    for param in self.params_to_tune:\n        if param.name() in param_values:\n            value = param_values[param.name()]\n            param.apply_parameter_value(value)\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.Autotune.flatten_params","title":"<code>flatten_params()</code>","text":"<p>Flatten all parameters to 1D array.</p> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>def flatten_params(self) -&gt; np.ndarray:\n    \"\"\"Flatten all parameters to 1D array.\"\"\"\n    return flatten_params(self.params_to_tune)\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.Autotune.get_best_result","title":"<code>get_best_result()</code>","text":"<p>Get best result found so far.</p> <p>Returns:</p> Type Description <code>EvaluationResult</code> <p>Best evaluation result</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If no results have been evaluated yet</p> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>def get_best_result(self) -&gt; EvaluationResult:\n    \"\"\"Get best result found so far.\n\n    Returns:\n        Best evaluation result\n\n    Raises:\n        RuntimeError: If no results have been evaluated yet\n    \"\"\"\n    if not isinstance(self.best_result, EvaluationResult):\n        raise RuntimeError(\"No results available yet\")\n    return self.best_result\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.Autotune.optimize_all","title":"<code>optimize_all(iterations)</code>","text":"<p>Run full optimization loop.</p> <p>Parameters:</p> Name Type Description Default <code>iterations</code> <code>int</code> <p>Number of optimization iterations</p> required <p>Returns:</p> Type Description <code>EvaluationResult</code> <p>Best result found across all iterations</p> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>def optimize_all(self, iterations: int) -&gt; EvaluationResult:\n    \"\"\"Run full optimization loop.\n\n    Args:\n        iterations: Number of optimization iterations\n\n    Returns:\n        Best result found across all iterations\n    \"\"\"\n    for _ in range(iterations):\n        self.optimize_step()\n\n    return self.get_best_result()\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.Autotune.optimize_step","title":"<code>optimize_step()</code>","text":"<p>Execute one optimization iteration.</p> <p>Returns:</p> Type Description <code>EvaluationResult</code> <p>Best result from this step</p> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>def optimize_step(self) -&gt; EvaluationResult:\n    \"\"\"Execute one optimization iteration.\n\n    Returns:\n        Best result from this step\n    \"\"\"\n    return self.optimizer.optimize_step()\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.Autotune.unflatten_params","title":"<code>unflatten_params(x, apply=True)</code>","text":"<p>Unflatten parameter vector and optionally apply.</p> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>def unflatten_params(\n    self, x: np.ndarray, apply: bool = True\n) -&gt; dict[str, np.ndarray]:\n    \"\"\"Unflatten parameter vector and optionally apply.\"\"\"\n    return unflatten_params(x, self.params_to_tune, apply=apply)\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.CMAESOpt","title":"<code>CMAESOpt</code>","text":"<p>               Bases: <code>Optimizer</code></p> <p>CMA-ES optimizer using the cma library.</p> <p>CMA-ES (Covariance Matrix Adaptation Evolution Strategy) is a gradient-free optimization algorithm well-suited for hyperparameter tuning.</p> <p>Attributes:</p> Name Type Description <code>population</code> <p>Population size (number of samples per iteration)</p> <code>sigma</code> <p>Initial step size (exploration width)</p> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>class CMAESOpt(Optimizer):\n    \"\"\"CMA-ES optimizer using the cma library.\n\n    CMA-ES (Covariance Matrix Adaptation Evolution Strategy) is a\n    gradient-free optimization algorithm well-suited for hyperparameter tuning.\n\n    Attributes:\n        population: Population size (number of samples per iteration)\n        sigma: Initial step size (exploration width)\n    \"\"\"\n\n    def __init__(self, population: int = 10, sigma: float = 0.1):\n        \"\"\"Initialize CMA-ES optimizer.\n\n        Args:\n            population: Population size (popsize in cma)\n            sigma: Initial standard deviation for sampling\n        \"\"\"\n        try:\n            import cma\n        except ImportError:\n            raise ImportError(\n                \"CMA-ES optimizer requires the 'cma' package. Install with: pip install cma\"\n            )\n\n        self.population = population\n        self.sigma = sigma\n        self.cma = cma\n        self.es = None\n        self.evaluate_fn = None\n\n    def setup_optimization(\n        self,\n        initial_params: np.ndarray,\n        evaluate_fn: Callable[[np.ndarray], EvaluationResult],\n    ) -&gt; None:\n        \"\"\"Initialize CMA-ES with starting parameters.\n\n        Args:\n            initial_params: Initial parameter values\n            evaluate_fn: Evaluation function\n        \"\"\"\n        self.evaluate_fn = evaluate_fn\n\n        # Initialize CMA-ES\n        opts = {\n            \"popsize\": self.population,\n            \"verbose\": -9,  # Suppress output\n        }\n\n        self.es = self.cma.CMAEvolutionStrategy(\n            initial_params.tolist(),\n            self.sigma,\n            opts,\n        )\n\n    def optimize_step(self) -&gt; EvaluationResult:\n        \"\"\"Execute one CMA-ES iteration (ask-tell loop).\n\n        Returns:\n            Best result from this iteration\n        \"\"\"\n        if self.es is None:\n            raise RuntimeError(\"Must call setup_optimization() first\")\n\n        # Ask: sample population\n        solutions = self.es.ask()\n\n        # Evaluate all solutions\n        results = []\n        for x in solutions:\n            result = self.evaluate_fn(np.array(x))  # type: ignore\n            results.append(result)\n\n        # Tell: update CMA-ES with costs\n        costs = [r.mean_cost for r in results]\n        self.es.tell(solutions, costs)\n\n        # Return best result\n        best_idx = np.argmin(costs)\n        return results[best_idx]\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.CMAESOpt.__init__","title":"<code>__init__(population=10, sigma=0.1)</code>","text":"<p>Initialize CMA-ES optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>population</code> <code>int</code> <p>Population size (popsize in cma)</p> <code>10</code> <code>sigma</code> <code>float</code> <p>Initial standard deviation for sampling</p> <code>0.1</code> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>def __init__(self, population: int = 10, sigma: float = 0.1):\n    \"\"\"Initialize CMA-ES optimizer.\n\n    Args:\n        population: Population size (popsize in cma)\n        sigma: Initial standard deviation for sampling\n    \"\"\"\n    try:\n        import cma\n    except ImportError:\n        raise ImportError(\n            \"CMA-ES optimizer requires the 'cma' package. Install with: pip install cma\"\n        )\n\n    self.population = population\n    self.sigma = sigma\n    self.cma = cma\n    self.es = None\n    self.evaluate_fn = None\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.CMAESOpt.optimize_step","title":"<code>optimize_step()</code>","text":"<p>Execute one CMA-ES iteration (ask-tell loop).</p> <p>Returns:</p> Type Description <code>EvaluationResult</code> <p>Best result from this iteration</p> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>def optimize_step(self) -&gt; EvaluationResult:\n    \"\"\"Execute one CMA-ES iteration (ask-tell loop).\n\n    Returns:\n        Best result from this iteration\n    \"\"\"\n    if self.es is None:\n        raise RuntimeError(\"Must call setup_optimization() first\")\n\n    # Ask: sample population\n    solutions = self.es.ask()\n\n    # Evaluate all solutions\n    results = []\n    for x in solutions:\n        result = self.evaluate_fn(np.array(x))  # type: ignore\n        results.append(result)\n\n    # Tell: update CMA-ES with costs\n    costs = [r.mean_cost for r in results]\n    self.es.tell(solutions, costs)\n\n    # Return best result\n    best_idx = np.argmin(costs)\n    return results[best_idx]\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.CMAESOpt.setup_optimization","title":"<code>setup_optimization(initial_params, evaluate_fn)</code>","text":"<p>Initialize CMA-ES with starting parameters.</p> <p>Parameters:</p> Name Type Description Default <code>initial_params</code> <code>ndarray</code> <p>Initial parameter values</p> required <code>evaluate_fn</code> <code>Callable[[ndarray], EvaluationResult]</code> <p>Evaluation function</p> required Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>def setup_optimization(\n    self,\n    initial_params: np.ndarray,\n    evaluate_fn: Callable[[np.ndarray], EvaluationResult],\n) -&gt; None:\n    \"\"\"Initialize CMA-ES with starting parameters.\n\n    Args:\n        initial_params: Initial parameter values\n        evaluate_fn: Evaluation function\n    \"\"\"\n    self.evaluate_fn = evaluate_fn\n\n    # Initialize CMA-ES\n    opts = {\n        \"popsize\": self.population,\n        \"verbose\": -9,  # Suppress output\n    }\n\n    self.es = self.cma.CMAEvolutionStrategy(\n        initial_params.tolist(),\n        self.sigma,\n        opts,\n    )\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.ConfigStateHolder","title":"<code>ConfigStateHolder</code>","text":"<p>Mutable holder for MPPI config and state.</p> <p>This allows parameters to access and update config/state through a shared reference.</p> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>class ConfigStateHolder:\n    \"\"\"Mutable holder for MPPI config and state.\n\n    This allows parameters to access and update config/state through a shared reference.\n    \"\"\"\n\n    def __init__(self, config: Any, state: Any):\n        \"\"\"Initialize holder.\n\n        Args:\n            config: MPPI configuration object\n            state: MPPI state object\n        \"\"\"\n        self.config = config\n        self.state = state\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.ConfigStateHolder.__init__","title":"<code>__init__(config, state)</code>","text":"<p>Initialize holder.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Any</code> <p>MPPI configuration object</p> required <code>state</code> <code>Any</code> <p>MPPI state object</p> required Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>def __init__(self, config: Any, state: Any):\n    \"\"\"Initialize holder.\n\n    Args:\n        config: MPPI configuration object\n        state: MPPI state object\n    \"\"\"\n    self.config = config\n    self.state = state\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.EvaluationResult","title":"<code>EvaluationResult</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Result from evaluating a parameter configuration.</p> <p>Attributes:</p> Name Type Description <code>mean_cost</code> <code>float</code> <p>Average cost across rollouts (lower is better)</p> <code>rollouts</code> <code>Array</code> <p>Trajectory rollouts, shape (N, H+1, nx)</p> <code>params</code> <code>dict</code> <p>Parameter values used for this evaluation</p> <code>iteration</code> <code>int</code> <p>Iteration number in optimization</p> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>class EvaluationResult(NamedTuple):\n    \"\"\"Result from evaluating a parameter configuration.\n\n    Attributes:\n        mean_cost: Average cost across rollouts (lower is better)\n        rollouts: Trajectory rollouts, shape (N, H+1, nx)\n        params: Parameter values used for this evaluation\n        iteration: Iteration number in optimization\n    \"\"\"\n\n    mean_cost: float\n    rollouts: jax.Array\n    params: dict\n    iteration: int\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.HorizonParameter","title":"<code>HorizonParameter</code>","text":"<p>               Bases: <code>TunableParameter</code></p> <p>Tunes planning horizon (config.horizon).</p> <p>Longer horizons allow for more foresight but increase computation. Changing horizon requires resizing the control trajectory U.</p> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>class HorizonParameter(TunableParameter):\n    \"\"\"Tunes planning horizon (config.horizon).\n\n    Longer horizons allow for more foresight but increase computation.\n    Changing horizon requires resizing the control trajectory U.\n    \"\"\"\n\n    def __init__(\n        self,\n        holder: ConfigStateHolder,\n        min_value: int = 5,\n        max_value: int = 100,\n    ):\n        \"\"\"Initialize horizon parameter.\n\n        Args:\n            holder: Config/state holder\n            min_value: Minimum allowed horizon\n            max_value: Maximum allowed horizon\n        \"\"\"\n        self.holder = holder\n        self.min_value = min_value\n        self.max_value = max_value\n\n    @staticmethod\n    def name() -&gt; str:\n        return \"horizon\"\n\n    def dim(self) -&gt; int:\n        return 1\n\n    def get_current_parameter_value(self) -&gt; np.ndarray:\n        \"\"\"Extract horizon from config.\"\"\"\n        return np.array([float(self.holder.config.horizon)])\n\n    def ensure_valid_value(self, value: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Clip horizon to [min_value, max_value] and round to int.\"\"\"\n        value = np.clip(value, self.min_value, self.max_value)\n        value = np.round(value)\n        return value\n\n    def apply_parameter_value(self, value: np.ndarray) -&gt; None:\n        \"\"\"Update config.horizon and resize U trajectory.\n\n        Handles horizon changes for all MPPI variants:\n        - MPPI: Resize U\n        - SMPPI: Resize U and action_sequence\n        - KMPPI: Rebuild Tk/Hs grids, reinterpolate U from theta\n        \"\"\"\n        new_horizon = int(value[0])\n        old_horizon = self.holder.config.horizon\n\n        if new_horizon == old_horizon:\n            return  # No change needed\n\n        # Update config\n        self.holder.config = replace(self.holder.config, horizon=new_horizon)\n\n        # Resize U trajectory\n        old_U = self.holder.state.U\n\n        if new_horizon &gt; old_horizon:\n            # Extend with u_init\n            extension = jnp.tile(\n                self.holder.state.u_init, (new_horizon - old_horizon, 1)\n            )\n            new_U = jnp.concatenate([old_U, extension], axis=0)\n        else:\n            # Truncate\n            new_U = old_U[:new_horizon]\n\n        # Update state with new U\n        self.holder.state = replace(self.holder.state, U=new_U)\n\n        # Handle variant-specific state updates\n        if hasattr(self.holder.state, \"action_sequence\"):\n            # SMPPI: also resize action_sequence\n            old_seq = self.holder.state.action_sequence\n            if new_horizon &gt; old_horizon:\n                extension = jnp.tile(\n                    self.holder.state.u_init, (new_horizon - old_horizon, 1)\n                )\n                new_seq = jnp.concatenate([old_seq, extension], axis=0)\n            else:\n                new_seq = old_seq[:new_horizon]\n            self.holder.state = replace(\n                self.holder.state, action_sequence=new_seq\n            )\n\n        if hasattr(self.holder.state, \"theta\"):\n            # KMPPI: rebuild time grids and reinterpolate\n            # This is more complex - for now, keep theta unchanged and rebuild Hs\n            # A full implementation would reinterpolate U from theta\n            num_support_pts = self.holder.config.num_support_pts\n            new_Tk = jnp.linspace(0, new_horizon - 1, num_support_pts)\n            new_Hs = jnp.arange(new_horizon, dtype=jnp.float32)\n            self.holder.state = replace(\n                self.holder.state,\n                Tk=new_Tk,\n                Hs=new_Hs,\n            )\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.HorizonParameter.__init__","title":"<code>__init__(holder, min_value=5, max_value=100)</code>","text":"<p>Initialize horizon parameter.</p> <p>Parameters:</p> Name Type Description Default <code>holder</code> <code>ConfigStateHolder</code> <p>Config/state holder</p> required <code>min_value</code> <code>int</code> <p>Minimum allowed horizon</p> <code>5</code> <code>max_value</code> <code>int</code> <p>Maximum allowed horizon</p> <code>100</code> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>def __init__(\n    self,\n    holder: ConfigStateHolder,\n    min_value: int = 5,\n    max_value: int = 100,\n):\n    \"\"\"Initialize horizon parameter.\n\n    Args:\n        holder: Config/state holder\n        min_value: Minimum allowed horizon\n        max_value: Maximum allowed horizon\n    \"\"\"\n    self.holder = holder\n    self.min_value = min_value\n    self.max_value = max_value\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.HorizonParameter.apply_parameter_value","title":"<code>apply_parameter_value(value)</code>","text":"<p>Update config.horizon and resize U trajectory.</p> <p>Handles horizon changes for all MPPI variants: - MPPI: Resize U - SMPPI: Resize U and action_sequence - KMPPI: Rebuild Tk/Hs grids, reinterpolate U from theta</p> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>def apply_parameter_value(self, value: np.ndarray) -&gt; None:\n    \"\"\"Update config.horizon and resize U trajectory.\n\n    Handles horizon changes for all MPPI variants:\n    - MPPI: Resize U\n    - SMPPI: Resize U and action_sequence\n    - KMPPI: Rebuild Tk/Hs grids, reinterpolate U from theta\n    \"\"\"\n    new_horizon = int(value[0])\n    old_horizon = self.holder.config.horizon\n\n    if new_horizon == old_horizon:\n        return  # No change needed\n\n    # Update config\n    self.holder.config = replace(self.holder.config, horizon=new_horizon)\n\n    # Resize U trajectory\n    old_U = self.holder.state.U\n\n    if new_horizon &gt; old_horizon:\n        # Extend with u_init\n        extension = jnp.tile(\n            self.holder.state.u_init, (new_horizon - old_horizon, 1)\n        )\n        new_U = jnp.concatenate([old_U, extension], axis=0)\n    else:\n        # Truncate\n        new_U = old_U[:new_horizon]\n\n    # Update state with new U\n    self.holder.state = replace(self.holder.state, U=new_U)\n\n    # Handle variant-specific state updates\n    if hasattr(self.holder.state, \"action_sequence\"):\n        # SMPPI: also resize action_sequence\n        old_seq = self.holder.state.action_sequence\n        if new_horizon &gt; old_horizon:\n            extension = jnp.tile(\n                self.holder.state.u_init, (new_horizon - old_horizon, 1)\n            )\n            new_seq = jnp.concatenate([old_seq, extension], axis=0)\n        else:\n            new_seq = old_seq[:new_horizon]\n        self.holder.state = replace(\n            self.holder.state, action_sequence=new_seq\n        )\n\n    if hasattr(self.holder.state, \"theta\"):\n        # KMPPI: rebuild time grids and reinterpolate\n        # This is more complex - for now, keep theta unchanged and rebuild Hs\n        # A full implementation would reinterpolate U from theta\n        num_support_pts = self.holder.config.num_support_pts\n        new_Tk = jnp.linspace(0, new_horizon - 1, num_support_pts)\n        new_Hs = jnp.arange(new_horizon, dtype=jnp.float32)\n        self.holder.state = replace(\n            self.holder.state,\n            Tk=new_Tk,\n            Hs=new_Hs,\n        )\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.HorizonParameter.ensure_valid_value","title":"<code>ensure_valid_value(value)</code>","text":"<p>Clip horizon to [min_value, max_value] and round to int.</p> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>def ensure_valid_value(self, value: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Clip horizon to [min_value, max_value] and round to int.\"\"\"\n    value = np.clip(value, self.min_value, self.max_value)\n    value = np.round(value)\n    return value\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.HorizonParameter.get_current_parameter_value","title":"<code>get_current_parameter_value()</code>","text":"<p>Extract horizon from config.</p> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>def get_current_parameter_value(self) -&gt; np.ndarray:\n    \"\"\"Extract horizon from config.\"\"\"\n    return np.array([float(self.holder.config.horizon)])\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.LambdaParameter","title":"<code>LambdaParameter</code>","text":"<p>               Bases: <code>TunableParameter</code></p> <p>Tunes temperature parameter (config.lambda_).</p> <p>Lower lambda_ values lead to more aggressive exploitation of low-cost trajectories. Higher values provide more exploration.</p> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>class LambdaParameter(TunableParameter):\n    \"\"\"Tunes temperature parameter (config.lambda_).\n\n    Lower lambda_ values lead to more aggressive exploitation of low-cost trajectories.\n    Higher values provide more exploration.\n    \"\"\"\n\n    def __init__(self, holder: ConfigStateHolder, min_value: float = 0.0001):\n        \"\"\"Initialize lambda parameter.\n\n        Args:\n            holder: Config/state holder\n            min_value: Minimum allowed lambda value\n        \"\"\"\n        self.holder = holder\n        self.min_value = min_value\n\n    @staticmethod\n    def name() -&gt; str:\n        return \"lambda\"\n\n    def dim(self) -&gt; int:\n        return 1\n\n    def get_current_parameter_value(self) -&gt; np.ndarray:\n        \"\"\"Extract lambda from config.\"\"\"\n        return np.array([self.holder.config.lambda_])\n\n    def ensure_valid_value(self, value: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Ensure lambda &gt;= min_value.\"\"\"\n        return np.maximum(value, self.min_value)\n\n    def apply_parameter_value(self, value: np.ndarray) -&gt; None:\n        \"\"\"Update config.lambda_ with new value.\"\"\"\n        new_lambda = float(value[0])\n        self.holder.config = replace(self.holder.config, lambda_=new_lambda)\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.LambdaParameter.__init__","title":"<code>__init__(holder, min_value=0.0001)</code>","text":"<p>Initialize lambda parameter.</p> <p>Parameters:</p> Name Type Description Default <code>holder</code> <code>ConfigStateHolder</code> <p>Config/state holder</p> required <code>min_value</code> <code>float</code> <p>Minimum allowed lambda value</p> <code>0.0001</code> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>def __init__(self, holder: ConfigStateHolder, min_value: float = 0.0001):\n    \"\"\"Initialize lambda parameter.\n\n    Args:\n        holder: Config/state holder\n        min_value: Minimum allowed lambda value\n    \"\"\"\n    self.holder = holder\n    self.min_value = min_value\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.LambdaParameter.apply_parameter_value","title":"<code>apply_parameter_value(value)</code>","text":"<p>Update config.lambda_ with new value.</p> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>def apply_parameter_value(self, value: np.ndarray) -&gt; None:\n    \"\"\"Update config.lambda_ with new value.\"\"\"\n    new_lambda = float(value[0])\n    self.holder.config = replace(self.holder.config, lambda_=new_lambda)\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.LambdaParameter.ensure_valid_value","title":"<code>ensure_valid_value(value)</code>","text":"<p>Ensure lambda &gt;= min_value.</p> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>def ensure_valid_value(self, value: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Ensure lambda &gt;= min_value.\"\"\"\n    return np.maximum(value, self.min_value)\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.LambdaParameter.get_current_parameter_value","title":"<code>get_current_parameter_value()</code>","text":"<p>Extract lambda from config.</p> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>def get_current_parameter_value(self) -&gt; np.ndarray:\n    \"\"\"Extract lambda from config.\"\"\"\n    return np.array([self.holder.config.lambda_])\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.MuParameter","title":"<code>MuParameter</code>","text":"<p>               Bases: <code>TunableParameter</code></p> <p>Tunes noise mean (state.noise_mu).</p> <p>Shifts the exploration distribution. Can be used to bias exploration toward certain actions.</p> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>class MuParameter(TunableParameter):\n    \"\"\"Tunes noise mean (state.noise_mu).\n\n    Shifts the exploration distribution. Can be used to bias exploration\n    toward certain actions.\n    \"\"\"\n\n    def __init__(self, holder: ConfigStateHolder):\n        \"\"\"Initialize noise mu parameter.\n\n        Args:\n            holder: Config/state holder\n        \"\"\"\n        self.holder = holder\n\n    @staticmethod\n    def name() -&gt; str:\n        return \"noise_mu\"\n\n    def dim(self) -&gt; int:\n        \"\"\"Returns nu (number of control dimensions).\"\"\"\n        return self.holder.config.nu\n\n    def get_current_parameter_value(self) -&gt; np.ndarray:\n        \"\"\"Extract noise mean from state.\"\"\"\n        return np.array(self.holder.state.noise_mu)\n\n    def ensure_valid_value(self, value: np.ndarray) -&gt; np.ndarray:\n        \"\"\"No constraints on mu (can be any value).\"\"\"\n        return value\n\n    def apply_parameter_value(self, value: np.ndarray) -&gt; None:\n        \"\"\"Update state.noise_mu with new mean.\"\"\"\n        noise_mu = jnp.array(value)\n        self.holder.state = replace(self.holder.state, noise_mu=noise_mu)\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.MuParameter.__init__","title":"<code>__init__(holder)</code>","text":"<p>Initialize noise mu parameter.</p> <p>Parameters:</p> Name Type Description Default <code>holder</code> <code>ConfigStateHolder</code> <p>Config/state holder</p> required Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>def __init__(self, holder: ConfigStateHolder):\n    \"\"\"Initialize noise mu parameter.\n\n    Args:\n        holder: Config/state holder\n    \"\"\"\n    self.holder = holder\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.MuParameter.apply_parameter_value","title":"<code>apply_parameter_value(value)</code>","text":"<p>Update state.noise_mu with new mean.</p> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>def apply_parameter_value(self, value: np.ndarray) -&gt; None:\n    \"\"\"Update state.noise_mu with new mean.\"\"\"\n    noise_mu = jnp.array(value)\n    self.holder.state = replace(self.holder.state, noise_mu=noise_mu)\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.MuParameter.dim","title":"<code>dim()</code>","text":"<p>Returns nu (number of control dimensions).</p> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>def dim(self) -&gt; int:\n    \"\"\"Returns nu (number of control dimensions).\"\"\"\n    return self.holder.config.nu\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.MuParameter.ensure_valid_value","title":"<code>ensure_valid_value(value)</code>","text":"<p>No constraints on mu (can be any value).</p> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>def ensure_valid_value(self, value: np.ndarray) -&gt; np.ndarray:\n    \"\"\"No constraints on mu (can be any value).\"\"\"\n    return value\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.MuParameter.get_current_parameter_value","title":"<code>get_current_parameter_value()</code>","text":"<p>Extract noise mean from state.</p> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>def get_current_parameter_value(self) -&gt; np.ndarray:\n    \"\"\"Extract noise mean from state.\"\"\"\n    return np.array(self.holder.state.noise_mu)\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.NoiseSigmaParameter","title":"<code>NoiseSigmaParameter</code>","text":"<p>               Bases: <code>TunableParameter</code></p> <p>Tunes noise covariance diagonal (state.noise_sigma).</p> <p>Controls exploration in action space. Higher sigma values increase exploration.</p> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>class NoiseSigmaParameter(TunableParameter):\n    \"\"\"Tunes noise covariance diagonal (state.noise_sigma).\n\n    Controls exploration in action space. Higher sigma values increase exploration.\n    \"\"\"\n\n    def __init__(self, holder: ConfigStateHolder, min_value: float = 0.0001):\n        \"\"\"Initialize noise sigma parameter.\n\n        Args:\n            holder: Config/state holder\n            min_value: Minimum allowed sigma value\n        \"\"\"\n        self.holder = holder\n        self.min_value = min_value\n\n    @staticmethod\n    def name() -&gt; str:\n        return \"noise_sigma\"\n\n    def dim(self) -&gt; int:\n        \"\"\"Returns nu (number of control dimensions).\"\"\"\n        return self.holder.config.nu\n\n    def get_current_parameter_value(self) -&gt; np.ndarray:\n        \"\"\"Extract diagonal of noise covariance matrix.\"\"\"\n        # noise_sigma is (nu, nu), extract diagonal\n        sigma_diag = np.diag(np.array(self.holder.state.noise_sigma))\n        return sigma_diag\n\n    def ensure_valid_value(self, value: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Ensure all sigma values &gt;= min_value.\"\"\"\n        return np.maximum(value, self.min_value)\n\n    def apply_parameter_value(self, value: np.ndarray) -&gt; None:\n        \"\"\"Update state.noise_sigma with diagonal matrix.\n\n        Also updates noise_sigma_inv (inverse covariance).\n        \"\"\"\n        # Create diagonal covariance matrix\n        noise_sigma = jnp.diag(jnp.array(value))\n        noise_sigma_inv = jnp.diag(1.0 / jnp.array(value))\n\n        self.holder.state = replace(\n            self.holder.state,\n            noise_sigma=noise_sigma,\n            noise_sigma_inv=noise_sigma_inv,\n        )\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.NoiseSigmaParameter.__init__","title":"<code>__init__(holder, min_value=0.0001)</code>","text":"<p>Initialize noise sigma parameter.</p> <p>Parameters:</p> Name Type Description Default <code>holder</code> <code>ConfigStateHolder</code> <p>Config/state holder</p> required <code>min_value</code> <code>float</code> <p>Minimum allowed sigma value</p> <code>0.0001</code> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>def __init__(self, holder: ConfigStateHolder, min_value: float = 0.0001):\n    \"\"\"Initialize noise sigma parameter.\n\n    Args:\n        holder: Config/state holder\n        min_value: Minimum allowed sigma value\n    \"\"\"\n    self.holder = holder\n    self.min_value = min_value\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.NoiseSigmaParameter.apply_parameter_value","title":"<code>apply_parameter_value(value)</code>","text":"<p>Update state.noise_sigma with diagonal matrix.</p> <p>Also updates noise_sigma_inv (inverse covariance).</p> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>def apply_parameter_value(self, value: np.ndarray) -&gt; None:\n    \"\"\"Update state.noise_sigma with diagonal matrix.\n\n    Also updates noise_sigma_inv (inverse covariance).\n    \"\"\"\n    # Create diagonal covariance matrix\n    noise_sigma = jnp.diag(jnp.array(value))\n    noise_sigma_inv = jnp.diag(1.0 / jnp.array(value))\n\n    self.holder.state = replace(\n        self.holder.state,\n        noise_sigma=noise_sigma,\n        noise_sigma_inv=noise_sigma_inv,\n    )\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.NoiseSigmaParameter.dim","title":"<code>dim()</code>","text":"<p>Returns nu (number of control dimensions).</p> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>def dim(self) -&gt; int:\n    \"\"\"Returns nu (number of control dimensions).\"\"\"\n    return self.holder.config.nu\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.NoiseSigmaParameter.ensure_valid_value","title":"<code>ensure_valid_value(value)</code>","text":"<p>Ensure all sigma values &gt;= min_value.</p> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>def ensure_valid_value(self, value: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Ensure all sigma values &gt;= min_value.\"\"\"\n    return np.maximum(value, self.min_value)\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.NoiseSigmaParameter.get_current_parameter_value","title":"<code>get_current_parameter_value()</code>","text":"<p>Extract diagonal of noise covariance matrix.</p> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>def get_current_parameter_value(self) -&gt; np.ndarray:\n    \"\"\"Extract diagonal of noise covariance matrix.\"\"\"\n    # noise_sigma is (nu, nu), extract diagonal\n    sigma_diag = np.diag(np.array(self.holder.state.noise_sigma))\n    return sigma_diag\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.Optimizer","title":"<code>Optimizer</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for parameter optimizers.</p> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>class Optimizer(abc.ABC):\n    \"\"\"Abstract base class for parameter optimizers.\"\"\"\n\n    @abc.abstractmethod\n    def setup_optimization(\n        self,\n        initial_params: np.ndarray,\n        evaluate_fn: Callable[[np.ndarray], EvaluationResult],\n    ) -&gt; None:\n        \"\"\"Initialize optimizer with starting parameters and evaluation function.\n\n        Args:\n            initial_params: Initial parameter values, shape (D,)\n            evaluate_fn: Function that evaluates parameter vector and returns cost\n        \"\"\"\n        ...\n\n    @abc.abstractmethod\n    def optimize_step(self) -&gt; EvaluationResult:\n        \"\"\"Execute one optimization iteration.\n\n        Returns:\n            Best evaluation result from this step\n        \"\"\"\n        ...\n\n    def optimize_all(self, iterations: int) -&gt; EvaluationResult:\n        \"\"\"Run full optimization loop.\n\n        Args:\n            iterations: Number of optimization iterations\n\n        Returns:\n            Best evaluation result found\n        \"\"\"\n        best_result = None\n        for i in range(iterations):\n            result = self.optimize_step()\n            if best_result is None or result.mean_cost &lt; best_result.mean_cost:\n                best_result = result\n        return best_result  # type: ignore\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.Optimizer.optimize_all","title":"<code>optimize_all(iterations)</code>","text":"<p>Run full optimization loop.</p> <p>Parameters:</p> Name Type Description Default <code>iterations</code> <code>int</code> <p>Number of optimization iterations</p> required <p>Returns:</p> Type Description <code>EvaluationResult</code> <p>Best evaluation result found</p> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>def optimize_all(self, iterations: int) -&gt; EvaluationResult:\n    \"\"\"Run full optimization loop.\n\n    Args:\n        iterations: Number of optimization iterations\n\n    Returns:\n        Best evaluation result found\n    \"\"\"\n    best_result = None\n    for i in range(iterations):\n        result = self.optimize_step()\n        if best_result is None or result.mean_cost &lt; best_result.mean_cost:\n            best_result = result\n    return best_result  # type: ignore\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.Optimizer.optimize_step","title":"<code>optimize_step()</code>  <code>abstractmethod</code>","text":"<p>Execute one optimization iteration.</p> <p>Returns:</p> Type Description <code>EvaluationResult</code> <p>Best evaluation result from this step</p> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>@abc.abstractmethod\ndef optimize_step(self) -&gt; EvaluationResult:\n    \"\"\"Execute one optimization iteration.\n\n    Returns:\n        Best evaluation result from this step\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.Optimizer.setup_optimization","title":"<code>setup_optimization(initial_params, evaluate_fn)</code>  <code>abstractmethod</code>","text":"<p>Initialize optimizer with starting parameters and evaluation function.</p> <p>Parameters:</p> Name Type Description Default <code>initial_params</code> <code>ndarray</code> <p>Initial parameter values, shape (D,)</p> required <code>evaluate_fn</code> <code>Callable[[ndarray], EvaluationResult]</code> <p>Function that evaluates parameter vector and returns cost</p> required Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>@abc.abstractmethod\ndef setup_optimization(\n    self,\n    initial_params: np.ndarray,\n    evaluate_fn: Callable[[np.ndarray], EvaluationResult],\n) -&gt; None:\n    \"\"\"Initialize optimizer with starting parameters and evaluation function.\n\n    Args:\n        initial_params: Initial parameter values, shape (D,)\n        evaluate_fn: Function that evaluates parameter vector and returns cost\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.TunableParameter","title":"<code>TunableParameter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for tunable MPPI parameters.</p> <p>Concrete implementations must define how to extract, validate, and apply parameter values to MPPI config/state.</p> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>class TunableParameter(abc.ABC):\n    \"\"\"Abstract base class for tunable MPPI parameters.\n\n    Concrete implementations must define how to extract, validate,\n    and apply parameter values to MPPI config/state.\n    \"\"\"\n\n    @staticmethod\n    @abc.abstractmethod\n    def name() -&gt; str:\n        \"\"\"Unique identifier for this parameter type.\"\"\"\n        ...\n\n    @abc.abstractmethod\n    def dim(self) -&gt; int:\n        \"\"\"Dimensionality when flattened to 1D array.\"\"\"\n        ...\n\n    @abc.abstractmethod\n    def get_current_parameter_value(self) -&gt; np.ndarray:\n        \"\"\"Extract current parameter value from MPPI config/state.\n\n        Returns:\n            Parameter value as numpy array, shape (dim(),)\n        \"\"\"\n        ...\n\n    @abc.abstractmethod\n    def ensure_valid_value(self, value: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Validate and constrain parameter value.\n\n        Args:\n            value: Raw parameter value, shape (dim(),)\n\n        Returns:\n            Validated and constrained value, shape (dim(),)\n        \"\"\"\n        ...\n\n    @abc.abstractmethod\n    def apply_parameter_value(self, value: np.ndarray) -&gt; None:\n        \"\"\"Update MPPI config/state with new parameter value.\n\n        Args:\n            value: Validated parameter value, shape (dim(),)\n\n        Side effects:\n            Updates holder.config and/or holder.state with new values\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.TunableParameter.apply_parameter_value","title":"<code>apply_parameter_value(value)</code>  <code>abstractmethod</code>","text":"<p>Update MPPI config/state with new parameter value.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>ndarray</code> <p>Validated parameter value, shape (dim(),)</p> required Side effects <p>Updates holder.config and/or holder.state with new values</p> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>@abc.abstractmethod\ndef apply_parameter_value(self, value: np.ndarray) -&gt; None:\n    \"\"\"Update MPPI config/state with new parameter value.\n\n    Args:\n        value: Validated parameter value, shape (dim(),)\n\n    Side effects:\n        Updates holder.config and/or holder.state with new values\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.TunableParameter.dim","title":"<code>dim()</code>  <code>abstractmethod</code>","text":"<p>Dimensionality when flattened to 1D array.</p> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>@abc.abstractmethod\ndef dim(self) -&gt; int:\n    \"\"\"Dimensionality when flattened to 1D array.\"\"\"\n    ...\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.TunableParameter.ensure_valid_value","title":"<code>ensure_valid_value(value)</code>  <code>abstractmethod</code>","text":"<p>Validate and constrain parameter value.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>ndarray</code> <p>Raw parameter value, shape (dim(),)</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Validated and constrained value, shape (dim(),)</p> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>@abc.abstractmethod\ndef ensure_valid_value(self, value: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Validate and constrain parameter value.\n\n    Args:\n        value: Raw parameter value, shape (dim(),)\n\n    Returns:\n        Validated and constrained value, shape (dim(),)\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.TunableParameter.get_current_parameter_value","title":"<code>get_current_parameter_value()</code>  <code>abstractmethod</code>","text":"<p>Extract current parameter value from MPPI config/state.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>Parameter value as numpy array, shape (dim(),)</p> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>@abc.abstractmethod\ndef get_current_parameter_value(self) -&gt; np.ndarray:\n    \"\"\"Extract current parameter value from MPPI config/state.\n\n    Returns:\n        Parameter value as numpy array, shape (dim(),)\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.TunableParameter.name","title":"<code>name()</code>  <code>abstractmethod</code> <code>staticmethod</code>","text":"<p>Unique identifier for this parameter type.</p> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>@staticmethod\n@abc.abstractmethod\ndef name() -&gt; str:\n    \"\"\"Unique identifier for this parameter type.\"\"\"\n    ...\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.flatten_params","title":"<code>flatten_params(params)</code>","text":"<p>Flatten list of parameters to 1D array.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>list[TunableParameter]</code> <p>List of tunable parameters</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Concatenated parameter values, shape (total_dim,)</p> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>def flatten_params(params: list[TunableParameter]) -&gt; np.ndarray:\n    \"\"\"Flatten list of parameters to 1D array.\n\n    Args:\n        params: List of tunable parameters\n\n    Returns:\n        Concatenated parameter values, shape (total_dim,)\n    \"\"\"\n    if not params:\n        return np.array([])\n\n    values = []\n    for param in params:\n        value = param.get_current_parameter_value()\n        values.append(value.flatten())\n    return np.concatenate(values)\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.save_convergence_plot","title":"<code>save_convergence_plot(costs, initial_cost, output_path='docs/media/autotune_convergence.png', title='Autotuning Convergence', **kwargs)</code>","text":"<p>Save convergence plot to docs/media.</p> <p>Parameters:</p> Name Type Description Default <code>costs</code> <code>list[float]</code> <p>List of costs at each iteration</p> required <code>initial_cost</code> <code>float</code> <p>Initial cost before optimization</p> required <code>output_path</code> <code>str | Path</code> <p>Path to save the plot (default: docs/media/autotune_convergence.png)</p> <code>'docs/media/autotune_convergence.png'</code> <code>title</code> <code>str</code> <p>Plot title</p> <code>'Autotuning Convergence'</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to plt.savefig (dpi, figsize, etc.)</p> <code>{}</code> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>def save_convergence_plot(\n    costs: list[float],\n    initial_cost: float,\n    output_path: str | Path = \"docs/media/autotune_convergence.png\",\n    title: str = \"Autotuning Convergence\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Save convergence plot to docs/media.\n\n    Args:\n        costs: List of costs at each iteration\n        initial_cost: Initial cost before optimization\n        output_path: Path to save the plot (default: docs/media/autotune_convergence.png)\n        title: Plot title\n        **kwargs: Additional arguments passed to plt.savefig (dpi, figsize, etc.)\n    \"\"\"\n    try:\n        import matplotlib.pyplot as plt\n    except ImportError:\n        print(\"Warning: matplotlib not available, skipping visualization\")\n        return\n\n    output_path = Path(output_path)\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n\n    plt.figure(figsize=kwargs.get(\"figsize\", (10, 6)))\n    plt.plot(costs, marker=\"o\", linewidth=2, markersize=6, label=\"Current\")\n    plt.axhline(\n        y=initial_cost, color=\"r\", linestyle=\"--\", linewidth=2, label=\"Initial\"\n    )\n    if costs:\n        plt.axhline(\n            y=min(costs), color=\"g\", linestyle=\"--\", linewidth=2, label=\"Best\"\n        )\n    plt.xlabel(\"Iteration\", fontsize=12)\n    plt.ylabel(\"Cost\", fontsize=12)\n    plt.title(title, fontsize=14)\n    plt.legend(fontsize=11)\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n\n    dpi = kwargs.get(\"dpi\", 150)\n    plt.savefig(output_path, dpi=dpi)\n    plt.close()\n    print(f\"Saved convergence plot to: {output_path}\")\n</code></pre>"},{"location":"api/autotune/#jax_mppi.autotune.unflatten_params","title":"<code>unflatten_params(x, params, apply=True)</code>","text":"<p>Unflatten 1D array to parameter dict and optionally apply.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Flattened parameter vector, shape (total_dim,)</p> required <code>params</code> <code>list[TunableParameter]</code> <p>List of tunable parameters (defines structure)</p> required <code>apply</code> <code>bool</code> <p>If True, apply values to parameters</p> <code>True</code> <p>Returns:</p> Type Description <code>dict[str, ndarray]</code> <p>Dictionary mapping parameter names to values</p> Source code in <code>src/jax_mppi/autotune.py</code> <pre><code>def unflatten_params(\n    x: np.ndarray,\n    params: list[TunableParameter],\n    apply: bool = True,\n) -&gt; dict[str, np.ndarray]:\n    \"\"\"Unflatten 1D array to parameter dict and optionally apply.\n\n    Args:\n        x: Flattened parameter vector, shape (total_dim,)\n        params: List of tunable parameters (defines structure)\n        apply: If True, apply values to parameters\n\n    Returns:\n        Dictionary mapping parameter names to values\n    \"\"\"\n    param_dict = {}\n    offset = 0\n\n    for param in params:\n        dim = param.dim()\n        value = x[offset : offset + dim]\n        value = param.ensure_valid_value(value)\n\n        if apply:\n            param.apply_parameter_value(value)\n\n        param_dict[param.name()] = value\n        offset += dim\n\n    return param_dict\n</code></pre>"},{"location":"api/autotune_global/","title":"Autotune Global","text":"<p>Global search extensions for JAX-MPPI autotuning.</p> <p>This module provides Ray Tune integration for distributed global hyperparameter search. Requires optional dependencies: ray[tune], hyperopt, bayesian-optimization</p> Example <p>from ray import tune from jax_mppi import autotune_global as autog</p>"},{"location":"api/autotune_global/#jax_mppi.autotune_global--define-parameters-with-search-spaces","title":"Define parameters with search spaces","text":"<p>params = [ ...     autog.GlobalLambdaParameter( ...         holder, ...         search_space=tune.loguniform(0.1, 10.0) ...     ), ...     autog.GlobalNoiseSigmaParameter( ...         holder, ...         search_space=tune.uniform(0.1, 2.0) ...     ), ... ]</p> <p>tuner = autog.AutotuneGlobal( ...     params_to_tune=params, ...     evaluate_fn=evaluate, ...     optimizer=autog.RayOptimizer(num_samples=100), ... ) best = tuner.optimize_all(iterations=100)</p>"},{"location":"api/autotune_global/#jax_mppi.autotune_global.AutotuneGlobal","title":"<code>AutotuneGlobal</code>","text":"<p>               Bases: <code>Autotune</code></p> <p>Extended Autotune with global search space support.</p> <p>Integrates Ray Tune for distributed hyperparameter search over large search spaces using advanced algorithms like HyperOpt and BayesOpt.</p> Example <p>from ray import tune params = [ ...     GlobalLambdaParameter(holder, search_space=tune.loguniform(0.1, 10)), ...     GlobalNoiseSigmaParameter(holder, search_space=tune.uniform(0.1, 2.0)), ... ] tuner = AutotuneGlobal( ...     params_to_tune=params, ...     evaluate_fn=evaluate, ...     optimizer=RayOptimizer(num_samples=100), ... ) best = tuner.optimize_all(iterations=100)</p> Source code in <code>src/jax_mppi/autotune_global.py</code> <pre><code>class AutotuneGlobal(Autotune):\n    \"\"\"Extended Autotune with global search space support.\n\n    Integrates Ray Tune for distributed hyperparameter search over\n    large search spaces using advanced algorithms like HyperOpt and BayesOpt.\n\n    Example:\n        &gt;&gt;&gt; from ray import tune\n        &gt;&gt;&gt; params = [\n        ...     GlobalLambdaParameter(holder, search_space=tune.loguniform(0.1, 10)),\n        ...     GlobalNoiseSigmaParameter(holder, search_space=tune.uniform(0.1, 2.0)),\n        ... ]\n        &gt;&gt;&gt; tuner = AutotuneGlobal(\n        ...     params_to_tune=params,\n        ...     evaluate_fn=evaluate,\n        ...     optimizer=RayOptimizer(num_samples=100),\n        ... )\n        &gt;&gt;&gt; best = tuner.optimize_all(iterations=100)\n    \"\"\"\n\n    def __init__(\n        self,\n        params_to_tune: list[GlobalTunableParameter],\n        evaluate_fn: Callable[[], EvaluationResult],\n        optimizer: Optional[RayOptimizer] = None,\n        reload_state_fn: Optional[Callable] = None,\n    ):\n        \"\"\"Initialize global autotuner.\n\n        Args:\n            params_to_tune: List of global tunable parameters\n            evaluate_fn: Evaluation function\n            optimizer: RayOptimizer instance\n            reload_state_fn: Optional state reload function\n        \"\"\"\n        if optimizer is None:\n            optimizer = RayOptimizer()\n\n        if not isinstance(optimizer, RayOptimizer):\n            raise ValueError(\"AutotuneGlobal requires RayOptimizer\")\n\n        # Don't call parent __init__ since we handle setup differently\n        self.params_to_tune = params_to_tune\n        self.evaluate_fn = evaluate_fn\n        self.reload_state_fn = reload_state_fn\n        self.optimizer = optimizer\n        self.best_result = None\n        self.iteration_count = 0\n\n    def define_search_space(self) -&gt; dict:\n        \"\"\"Define Ray Tune search space from parameters.\n\n        Returns:\n            Dictionary mapping parameter names to search spaces\n        \"\"\"\n        search_space = {}\n        for param in self.params_to_tune:\n            if isinstance(param, GlobalTunableParameter):\n                search_space.update(param.get_search_space_dict())\n        return search_space\n\n    def optimize_all(self, iterations: int) -&gt; EvaluationResult:\n        \"\"\"Run Ray Tune global search.\n\n        Args:\n            iterations: Number of trials to run\n\n        Returns:\n            Best evaluation result found\n        \"\"\"\n        from ray import tune  # type: ignore\n        from ray.tune.search import ConcurrencyLimiter  # type: ignore\n\n        # Define search space\n        search_space = self.define_search_space()\n\n        # Create trainable function for Ray Tune\n        def trainable(config: dict):\n            \"\"\"Ray Tune trainable function.\"\"\"\n            # Unflatten config to parameter values\n            param_values = {}\n\n            for param in self.params_to_tune:\n                if param.dim() == 1:\n                    # Single-valued parameter\n                    param_values[param.name()] = np.array([\n                        config[param.name()]\n                    ])\n                else:\n                    # Multi-valued parameter\n                    values = []\n                    for i in range(param.dim()):\n                        key = f\"{param.name()}_{i}\"\n                        values.append(config[key])\n                    param_values[param.name()] = np.array(values)\n\n            # Apply parameters\n            for param in self.params_to_tune:\n                validated = param.ensure_valid_value(param_values[param.name()])\n                param.apply_parameter_value(validated)\n\n            # Reload state if needed\n            if self.reload_state_fn is not None:\n                self.reload_state_fn()\n\n            # Evaluate\n            result = self.evaluate_fn()\n\n            # Report to Ray Tune\n            tune.report(mean_cost=result.mean_cost)\n\n        # Setup search algorithm\n        if self.optimizer.search_alg == \"hyperopt\":\n            from ray.tune.search.hyperopt import HyperOptSearch  # type: ignore\n\n            search_alg = HyperOptSearch(metric=\"mean_cost\", mode=\"min\")\n        elif self.optimizer.search_alg == \"bayesopt\":\n            from ray.tune.search.bayesopt import BayesOptSearch  # type: ignore\n\n            search_alg = BayesOptSearch(metric=\"mean_cost\", mode=\"min\")\n        else:\n            search_alg = None  # Random search\n\n        # Limit concurrency to avoid overwhelming the system\n        if search_alg is not None:\n            search_alg = ConcurrencyLimiter(search_alg, max_concurrent=4)\n\n        # Run tuning\n        analysis = tune.run(\n            trainable,\n            config=search_space,\n            num_samples=iterations,\n            search_alg=search_alg,\n            verbose=1,\n        )\n\n        # Get best result\n        best_config = analysis.get_best_config(metric=\"mean_cost\", mode=\"min\")\n\n        # Apply best config and evaluate one more time to get full result\n        for param in self.params_to_tune:\n            if param.dim() == 1:\n                value = np.array([best_config[param.name()]])\n            else:\n                value = np.array([\n                    best_config[f\"{param.name()}_{i}\"]\n                    for i in range(param.dim())\n                ])\n            validated = param.ensure_valid_value(value)\n            param.apply_parameter_value(validated)\n\n        final_result = self.evaluate_fn()\n        self.best_result = final_result._replace(\n            params=best_config, iteration=iterations\n        )\n\n        return self.best_result\n\n    def get_best_result(self) -&gt; EvaluationResult:\n        \"\"\"Get best result found.\n\n        Returns:\n            Best evaluation result\n\n        Raises:\n            RuntimeError: If no results available yet\n        \"\"\"\n        if self.best_result is None:\n            raise RuntimeError(\"No results available yet\")\n        return self.best_result\n</code></pre>"},{"location":"api/autotune_global/#jax_mppi.autotune_global.AutotuneGlobal.__init__","title":"<code>__init__(params_to_tune, evaluate_fn, optimizer=None, reload_state_fn=None)</code>","text":"<p>Initialize global autotuner.</p> <p>Parameters:</p> Name Type Description Default <code>params_to_tune</code> <code>list[GlobalTunableParameter]</code> <p>List of global tunable parameters</p> required <code>evaluate_fn</code> <code>Callable[[], EvaluationResult]</code> <p>Evaluation function</p> required <code>optimizer</code> <code>Optional[RayOptimizer]</code> <p>RayOptimizer instance</p> <code>None</code> <code>reload_state_fn</code> <code>Optional[Callable]</code> <p>Optional state reload function</p> <code>None</code> Source code in <code>src/jax_mppi/autotune_global.py</code> <pre><code>def __init__(\n    self,\n    params_to_tune: list[GlobalTunableParameter],\n    evaluate_fn: Callable[[], EvaluationResult],\n    optimizer: Optional[RayOptimizer] = None,\n    reload_state_fn: Optional[Callable] = None,\n):\n    \"\"\"Initialize global autotuner.\n\n    Args:\n        params_to_tune: List of global tunable parameters\n        evaluate_fn: Evaluation function\n        optimizer: RayOptimizer instance\n        reload_state_fn: Optional state reload function\n    \"\"\"\n    if optimizer is None:\n        optimizer = RayOptimizer()\n\n    if not isinstance(optimizer, RayOptimizer):\n        raise ValueError(\"AutotuneGlobal requires RayOptimizer\")\n\n    # Don't call parent __init__ since we handle setup differently\n    self.params_to_tune = params_to_tune\n    self.evaluate_fn = evaluate_fn\n    self.reload_state_fn = reload_state_fn\n    self.optimizer = optimizer\n    self.best_result = None\n    self.iteration_count = 0\n</code></pre>"},{"location":"api/autotune_global/#jax_mppi.autotune_global.AutotuneGlobal.define_search_space","title":"<code>define_search_space()</code>","text":"<p>Define Ray Tune search space from parameters.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary mapping parameter names to search spaces</p> Source code in <code>src/jax_mppi/autotune_global.py</code> <pre><code>def define_search_space(self) -&gt; dict:\n    \"\"\"Define Ray Tune search space from parameters.\n\n    Returns:\n        Dictionary mapping parameter names to search spaces\n    \"\"\"\n    search_space = {}\n    for param in self.params_to_tune:\n        if isinstance(param, GlobalTunableParameter):\n            search_space.update(param.get_search_space_dict())\n    return search_space\n</code></pre>"},{"location":"api/autotune_global/#jax_mppi.autotune_global.AutotuneGlobal.get_best_result","title":"<code>get_best_result()</code>","text":"<p>Get best result found.</p> <p>Returns:</p> Type Description <code>EvaluationResult</code> <p>Best evaluation result</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If no results available yet</p> Source code in <code>src/jax_mppi/autotune_global.py</code> <pre><code>def get_best_result(self) -&gt; EvaluationResult:\n    \"\"\"Get best result found.\n\n    Returns:\n        Best evaluation result\n\n    Raises:\n        RuntimeError: If no results available yet\n    \"\"\"\n    if self.best_result is None:\n        raise RuntimeError(\"No results available yet\")\n    return self.best_result\n</code></pre>"},{"location":"api/autotune_global/#jax_mppi.autotune_global.AutotuneGlobal.optimize_all","title":"<code>optimize_all(iterations)</code>","text":"<p>Run Ray Tune global search.</p> <p>Parameters:</p> Name Type Description Default <code>iterations</code> <code>int</code> <p>Number of trials to run</p> required <p>Returns:</p> Type Description <code>EvaluationResult</code> <p>Best evaluation result found</p> Source code in <code>src/jax_mppi/autotune_global.py</code> <pre><code>def optimize_all(self, iterations: int) -&gt; EvaluationResult:\n    \"\"\"Run Ray Tune global search.\n\n    Args:\n        iterations: Number of trials to run\n\n    Returns:\n        Best evaluation result found\n    \"\"\"\n    from ray import tune  # type: ignore\n    from ray.tune.search import ConcurrencyLimiter  # type: ignore\n\n    # Define search space\n    search_space = self.define_search_space()\n\n    # Create trainable function for Ray Tune\n    def trainable(config: dict):\n        \"\"\"Ray Tune trainable function.\"\"\"\n        # Unflatten config to parameter values\n        param_values = {}\n\n        for param in self.params_to_tune:\n            if param.dim() == 1:\n                # Single-valued parameter\n                param_values[param.name()] = np.array([\n                    config[param.name()]\n                ])\n            else:\n                # Multi-valued parameter\n                values = []\n                for i in range(param.dim()):\n                    key = f\"{param.name()}_{i}\"\n                    values.append(config[key])\n                param_values[param.name()] = np.array(values)\n\n        # Apply parameters\n        for param in self.params_to_tune:\n            validated = param.ensure_valid_value(param_values[param.name()])\n            param.apply_parameter_value(validated)\n\n        # Reload state if needed\n        if self.reload_state_fn is not None:\n            self.reload_state_fn()\n\n        # Evaluate\n        result = self.evaluate_fn()\n\n        # Report to Ray Tune\n        tune.report(mean_cost=result.mean_cost)\n\n    # Setup search algorithm\n    if self.optimizer.search_alg == \"hyperopt\":\n        from ray.tune.search.hyperopt import HyperOptSearch  # type: ignore\n\n        search_alg = HyperOptSearch(metric=\"mean_cost\", mode=\"min\")\n    elif self.optimizer.search_alg == \"bayesopt\":\n        from ray.tune.search.bayesopt import BayesOptSearch  # type: ignore\n\n        search_alg = BayesOptSearch(metric=\"mean_cost\", mode=\"min\")\n    else:\n        search_alg = None  # Random search\n\n    # Limit concurrency to avoid overwhelming the system\n    if search_alg is not None:\n        search_alg = ConcurrencyLimiter(search_alg, max_concurrent=4)\n\n    # Run tuning\n    analysis = tune.run(\n        trainable,\n        config=search_space,\n        num_samples=iterations,\n        search_alg=search_alg,\n        verbose=1,\n    )\n\n    # Get best result\n    best_config = analysis.get_best_config(metric=\"mean_cost\", mode=\"min\")\n\n    # Apply best config and evaluate one more time to get full result\n    for param in self.params_to_tune:\n        if param.dim() == 1:\n            value = np.array([best_config[param.name()]])\n        else:\n            value = np.array([\n                best_config[f\"{param.name()}_{i}\"]\n                for i in range(param.dim())\n            ])\n        validated = param.ensure_valid_value(value)\n        param.apply_parameter_value(validated)\n\n    final_result = self.evaluate_fn()\n    self.best_result = final_result._replace(\n        params=best_config, iteration=iterations\n    )\n\n    return self.best_result\n</code></pre>"},{"location":"api/autotune_global/#jax_mppi.autotune_global.GlobalHorizonParameter","title":"<code>GlobalHorizonParameter</code>  <code>dataclass</code>","text":"<p>               Bases: <code>HorizonParameter</code>, <code>GlobalTunableParameter</code></p> <p>Horizon parameter with global search space.</p> Source code in <code>src/jax_mppi/autotune_global.py</code> <pre><code>@dataclass\nclass GlobalHorizonParameter(HorizonParameter, GlobalTunableParameter):\n    \"\"\"Horizon parameter with global search space.\"\"\"\n\n    search_space: Any = None\n\n    def __init__(\n        self,\n        holder: ConfigStateHolder,\n        search_space: Any,\n        min_value: int = 5,\n        max_value: int = 100,\n    ):\n        \"\"\"Initialize global horizon parameter.\n\n        Args:\n            holder: Config/state holder\n            search_space: Ray Tune search space (e.g., tune.randint(5, 20))\n            min_value: Minimum value constraint\n            max_value: Maximum value constraint\n        \"\"\"\n        HorizonParameter.__init__(self, holder, min_value, max_value)\n        self.search_space = search_space\n\n    def get_search_space_dict(self) -&gt; Dict[str, Any]:\n        return {self.name(): self.search_space}\n</code></pre>"},{"location":"api/autotune_global/#jax_mppi.autotune_global.GlobalHorizonParameter.__init__","title":"<code>__init__(holder, search_space, min_value=5, max_value=100)</code>","text":"<p>Initialize global horizon parameter.</p> <p>Parameters:</p> Name Type Description Default <code>holder</code> <code>ConfigStateHolder</code> <p>Config/state holder</p> required <code>search_space</code> <code>Any</code> <p>Ray Tune search space (e.g., tune.randint(5, 20))</p> required <code>min_value</code> <code>int</code> <p>Minimum value constraint</p> <code>5</code> <code>max_value</code> <code>int</code> <p>Maximum value constraint</p> <code>100</code> Source code in <code>src/jax_mppi/autotune_global.py</code> <pre><code>def __init__(\n    self,\n    holder: ConfigStateHolder,\n    search_space: Any,\n    min_value: int = 5,\n    max_value: int = 100,\n):\n    \"\"\"Initialize global horizon parameter.\n\n    Args:\n        holder: Config/state holder\n        search_space: Ray Tune search space (e.g., tune.randint(5, 20))\n        min_value: Minimum value constraint\n        max_value: Maximum value constraint\n    \"\"\"\n    HorizonParameter.__init__(self, holder, min_value, max_value)\n    self.search_space = search_space\n</code></pre>"},{"location":"api/autotune_global/#jax_mppi.autotune_global.GlobalLambdaParameter","title":"<code>GlobalLambdaParameter</code>  <code>dataclass</code>","text":"<p>               Bases: <code>LambdaParameter</code>, <code>GlobalTunableParameter</code></p> <p>Lambda parameter with global search space.</p> Source code in <code>src/jax_mppi/autotune_global.py</code> <pre><code>@dataclass\nclass GlobalLambdaParameter(LambdaParameter, GlobalTunableParameter):\n    \"\"\"Lambda parameter with global search space.\"\"\"\n\n    search_space: Any = None\n\n    def __init__(\n        self,\n        holder: ConfigStateHolder,\n        search_space: Any,\n        min_value: float = 0.0001,\n    ):\n        \"\"\"Initialize global lambda parameter.\n\n        Args:\n            holder: Config/state holder\n            search_space: Ray Tune search space\n            min_value: Minimum value constraint\n        \"\"\"\n        LambdaParameter.__init__(self, holder, min_value)\n        self.search_space = search_space\n\n    def get_search_space_dict(self) -&gt; Dict[str, Any]:\n        return {self.name(): self.search_space}\n</code></pre>"},{"location":"api/autotune_global/#jax_mppi.autotune_global.GlobalLambdaParameter.__init__","title":"<code>__init__(holder, search_space, min_value=0.0001)</code>","text":"<p>Initialize global lambda parameter.</p> <p>Parameters:</p> Name Type Description Default <code>holder</code> <code>ConfigStateHolder</code> <p>Config/state holder</p> required <code>search_space</code> <code>Any</code> <p>Ray Tune search space</p> required <code>min_value</code> <code>float</code> <p>Minimum value constraint</p> <code>0.0001</code> Source code in <code>src/jax_mppi/autotune_global.py</code> <pre><code>def __init__(\n    self,\n    holder: ConfigStateHolder,\n    search_space: Any,\n    min_value: float = 0.0001,\n):\n    \"\"\"Initialize global lambda parameter.\n\n    Args:\n        holder: Config/state holder\n        search_space: Ray Tune search space\n        min_value: Minimum value constraint\n    \"\"\"\n    LambdaParameter.__init__(self, holder, min_value)\n    self.search_space = search_space\n</code></pre>"},{"location":"api/autotune_global/#jax_mppi.autotune_global.GlobalMuParameter","title":"<code>GlobalMuParameter</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MuParameter</code>, <code>GlobalTunableParameter</code></p> <p>Noise mu parameter with global search space.</p> Source code in <code>src/jax_mppi/autotune_global.py</code> <pre><code>@dataclass\nclass GlobalMuParameter(MuParameter, GlobalTunableParameter):\n    \"\"\"Noise mu parameter with global search space.\"\"\"\n\n    search_space: Any = None\n\n    def __init__(self, holder: ConfigStateHolder, search_space: Any):\n        \"\"\"Initialize global noise mu parameter.\n\n        Args:\n            holder: Config/state holder\n            search_space: Ray Tune search space (applied per-dimension)\n        \"\"\"\n        MuParameter.__init__(self, holder)\n        self.search_space = search_space\n\n    def get_search_space_dict(self) -&gt; Dict[str, Any]:\n        nu = self.holder.config.nu\n        if nu == 1:\n            return {self.name(): self.search_space}\n        else:\n            return {f\"{self.name()}_{i}\": self.search_space for i in range(nu)}\n</code></pre>"},{"location":"api/autotune_global/#jax_mppi.autotune_global.GlobalMuParameter.__init__","title":"<code>__init__(holder, search_space)</code>","text":"<p>Initialize global noise mu parameter.</p> <p>Parameters:</p> Name Type Description Default <code>holder</code> <code>ConfigStateHolder</code> <p>Config/state holder</p> required <code>search_space</code> <code>Any</code> <p>Ray Tune search space (applied per-dimension)</p> required Source code in <code>src/jax_mppi/autotune_global.py</code> <pre><code>def __init__(self, holder: ConfigStateHolder, search_space: Any):\n    \"\"\"Initialize global noise mu parameter.\n\n    Args:\n        holder: Config/state holder\n        search_space: Ray Tune search space (applied per-dimension)\n    \"\"\"\n    MuParameter.__init__(self, holder)\n    self.search_space = search_space\n</code></pre>"},{"location":"api/autotune_global/#jax_mppi.autotune_global.GlobalNoiseSigmaParameter","title":"<code>GlobalNoiseSigmaParameter</code>  <code>dataclass</code>","text":"<p>               Bases: <code>NoiseSigmaParameter</code>, <code>GlobalTunableParameter</code></p> <p>Noise sigma parameter with global search space.</p> Source code in <code>src/jax_mppi/autotune_global.py</code> <pre><code>@dataclass\nclass GlobalNoiseSigmaParameter(NoiseSigmaParameter, GlobalTunableParameter):\n    \"\"\"Noise sigma parameter with global search space.\"\"\"\n\n    search_space: Any = None\n\n    def __init__(\n        self,\n        holder: ConfigStateHolder,\n        search_space: Any,\n        min_value: float = 0.0001,\n    ):\n        \"\"\"Initialize global noise sigma parameter.\n\n        Args:\n            holder: Config/state holder\n            search_space: Ray Tune search space (applied per-dimension)\n            min_value: Minimum value constraint\n        \"\"\"\n        NoiseSigmaParameter.__init__(self, holder, min_value)\n        self.search_space = search_space\n\n    def get_search_space_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"Return search space dict with dimensionality.\n\n        For multi-dimensional parameters, creates separate search spaces\n        for each dimension.\n        \"\"\"\n        nu = self.holder.config.nu\n        if nu == 1:\n            return {self.name(): self.search_space}\n        else:\n            # Create search space for each dimension\n            return {f\"{self.name()}_{i}\": self.search_space for i in range(nu)}\n</code></pre>"},{"location":"api/autotune_global/#jax_mppi.autotune_global.GlobalNoiseSigmaParameter.__init__","title":"<code>__init__(holder, search_space, min_value=0.0001)</code>","text":"<p>Initialize global noise sigma parameter.</p> <p>Parameters:</p> Name Type Description Default <code>holder</code> <code>ConfigStateHolder</code> <p>Config/state holder</p> required <code>search_space</code> <code>Any</code> <p>Ray Tune search space (applied per-dimension)</p> required <code>min_value</code> <code>float</code> <p>Minimum value constraint</p> <code>0.0001</code> Source code in <code>src/jax_mppi/autotune_global.py</code> <pre><code>def __init__(\n    self,\n    holder: ConfigStateHolder,\n    search_space: Any,\n    min_value: float = 0.0001,\n):\n    \"\"\"Initialize global noise sigma parameter.\n\n    Args:\n        holder: Config/state holder\n        search_space: Ray Tune search space (applied per-dimension)\n        min_value: Minimum value constraint\n    \"\"\"\n    NoiseSigmaParameter.__init__(self, holder, min_value)\n    self.search_space = search_space\n</code></pre>"},{"location":"api/autotune_global/#jax_mppi.autotune_global.GlobalNoiseSigmaParameter.get_search_space_dict","title":"<code>get_search_space_dict()</code>","text":"<p>Return search space dict with dimensionality.</p> <p>For multi-dimensional parameters, creates separate search spaces for each dimension.</p> Source code in <code>src/jax_mppi/autotune_global.py</code> <pre><code>def get_search_space_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Return search space dict with dimensionality.\n\n    For multi-dimensional parameters, creates separate search spaces\n    for each dimension.\n    \"\"\"\n    nu = self.holder.config.nu\n    if nu == 1:\n        return {self.name(): self.search_space}\n    else:\n        # Create search space for each dimension\n        return {f\"{self.name()}_{i}\": self.search_space for i in range(nu)}\n</code></pre>"},{"location":"api/autotune_global/#jax_mppi.autotune_global.GlobalTunableParameter","title":"<code>GlobalTunableParameter</code>","text":"<p>               Bases: <code>TunableParameter</code></p> <p>Parameter with Ray Tune search space definition.</p> <p>Extends TunableParameter with search space information for global optimization.</p> Source code in <code>src/jax_mppi/autotune_global.py</code> <pre><code>class GlobalTunableParameter(TunableParameter):\n    \"\"\"Parameter with Ray Tune search space definition.\n\n    Extends TunableParameter with search space information for global optimization.\n    \"\"\"\n\n    def __init__(self, search_space: Any):\n        \"\"\"Initialize with Ray Tune search space.\n\n        Args:\n            search_space: Ray Tune search space (e.g., tune.loguniform(0.1, 10))\n        \"\"\"\n        self.search_space = search_space\n\n    def get_search_space_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"Return dictionary mapping parameter name to search space.\n\n        Returns:\n            Dictionary for Ray Tune configuration\n        \"\"\"\n        return {self.name(): self.search_space}\n</code></pre>"},{"location":"api/autotune_global/#jax_mppi.autotune_global.GlobalTunableParameter.__init__","title":"<code>__init__(search_space)</code>","text":"<p>Initialize with Ray Tune search space.</p> <p>Parameters:</p> Name Type Description Default <code>search_space</code> <code>Any</code> <p>Ray Tune search space (e.g., tune.loguniform(0.1, 10))</p> required Source code in <code>src/jax_mppi/autotune_global.py</code> <pre><code>def __init__(self, search_space: Any):\n    \"\"\"Initialize with Ray Tune search space.\n\n    Args:\n        search_space: Ray Tune search space (e.g., tune.loguniform(0.1, 10))\n    \"\"\"\n    self.search_space = search_space\n</code></pre>"},{"location":"api/autotune_global/#jax_mppi.autotune_global.GlobalTunableParameter.get_search_space_dict","title":"<code>get_search_space_dict()</code>","text":"<p>Return dictionary mapping parameter name to search space.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary for Ray Tune configuration</p> Source code in <code>src/jax_mppi/autotune_global.py</code> <pre><code>def get_search_space_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Return dictionary mapping parameter name to search space.\n\n    Returns:\n        Dictionary for Ray Tune configuration\n    \"\"\"\n    return {self.name(): self.search_space}\n</code></pre>"},{"location":"api/autotune_global/#jax_mppi.autotune_global.RayOptimizer","title":"<code>RayOptimizer</code>","text":"<p>               Bases: <code>Optimizer</code></p> <p>Ray Tune optimizer for distributed global search.</p> <p>Uses Ray Tune's hyperparameter optimization algorithms (HyperOpt, BayesOpt, etc.) for distributed search over large parameter spaces.</p> <p>Note: Only supports optimize_all(), not step-wise optimization.</p> Source code in <code>src/jax_mppi/autotune_global.py</code> <pre><code>class RayOptimizer(Optimizer):\n    \"\"\"Ray Tune optimizer for distributed global search.\n\n    Uses Ray Tune's hyperparameter optimization algorithms (HyperOpt, BayesOpt, etc.)\n    for distributed search over large parameter spaces.\n\n    Note: Only supports optimize_all(), not step-wise optimization.\n    \"\"\"\n\n    def __init__(\n        self,\n        search_alg: str = \"hyperopt\",\n        num_samples: int = 100,\n        metric: str = \"mean_cost\",\n        mode: str = \"min\",\n    ):\n        \"\"\"Initialize Ray Tune optimizer.\n\n        Args:\n            search_alg: Search algorithm (\"hyperopt\", \"bayesopt\", or \"random\")\n            num_samples: Number of parameter configurations to try\n            metric: Metric to optimize (always \"mean_cost\")\n            mode: Optimization mode (always \"min\")\n        \"\"\"\n        try:\n            import ray  # type: ignore\n            from ray import tune  # type: ignore\n        except ImportError:\n            raise ImportError(\n                \"Ray Tune optimizer requires 'ray[tune]'. \"\n                \"Install with: pip install 'ray[tune]' hyperopt bayesian-optimization\"\n            )\n\n        self.search_alg = search_alg\n        self.num_samples = num_samples\n        self.metric = metric\n        self.mode = mode\n        self.ray = ray\n        self.tune = tune\n        self.evaluate_fn = None\n        self.best_result = None\n\n    def setup_optimization(\n        self,\n        initial_params: np.ndarray,\n        evaluate_fn: Callable[[np.ndarray], EvaluationResult],\n    ) -&gt; None:\n        \"\"\"Setup Ray Tune optimization.\n\n        Args:\n            initial_params: Initial parameters (not used by Ray Tune)\n            evaluate_fn: Evaluation function\n        \"\"\"\n        self.evaluate_fn = evaluate_fn\n\n    def optimize_step(self) -&gt; EvaluationResult:\n        \"\"\"Not supported for Ray Tune (requires full run).\"\"\"\n        raise NotImplementedError(\n            \"RayOptimizer only supports optimize_all(), not step-wise optimization\"\n        )\n\n    def optimize_all(self, iterations: int) -&gt; EvaluationResult:\n        \"\"\"Run full Ray Tune search.\n\n        Args:\n            iterations: Number of trials (overrides num_samples if provided)\n\n        Returns:\n            Best evaluation result found\n        \"\"\"\n        if self.evaluate_fn is None:\n            raise RuntimeError(\"Must call setup_optimization() first\")\n\n        # Use iterations if provided, otherwise use num_samples\n        num_trials = iterations if iterations &gt; 0 else self.num_samples\n\n        # This will be set by AutotuneGlobal\n        raise NotImplementedError(\n            \"RayOptimizer.optimize_all() must be called through AutotuneGlobal\"\n        )\n</code></pre>"},{"location":"api/autotune_global/#jax_mppi.autotune_global.RayOptimizer.__init__","title":"<code>__init__(search_alg='hyperopt', num_samples=100, metric='mean_cost', mode='min')</code>","text":"<p>Initialize Ray Tune optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>search_alg</code> <code>str</code> <p>Search algorithm (\"hyperopt\", \"bayesopt\", or \"random\")</p> <code>'hyperopt'</code> <code>num_samples</code> <code>int</code> <p>Number of parameter configurations to try</p> <code>100</code> <code>metric</code> <code>str</code> <p>Metric to optimize (always \"mean_cost\")</p> <code>'mean_cost'</code> <code>mode</code> <code>str</code> <p>Optimization mode (always \"min\")</p> <code>'min'</code> Source code in <code>src/jax_mppi/autotune_global.py</code> <pre><code>def __init__(\n    self,\n    search_alg: str = \"hyperopt\",\n    num_samples: int = 100,\n    metric: str = \"mean_cost\",\n    mode: str = \"min\",\n):\n    \"\"\"Initialize Ray Tune optimizer.\n\n    Args:\n        search_alg: Search algorithm (\"hyperopt\", \"bayesopt\", or \"random\")\n        num_samples: Number of parameter configurations to try\n        metric: Metric to optimize (always \"mean_cost\")\n        mode: Optimization mode (always \"min\")\n    \"\"\"\n    try:\n        import ray  # type: ignore\n        from ray import tune  # type: ignore\n    except ImportError:\n        raise ImportError(\n            \"Ray Tune optimizer requires 'ray[tune]'. \"\n            \"Install with: pip install 'ray[tune]' hyperopt bayesian-optimization\"\n        )\n\n    self.search_alg = search_alg\n    self.num_samples = num_samples\n    self.metric = metric\n    self.mode = mode\n    self.ray = ray\n    self.tune = tune\n    self.evaluate_fn = None\n    self.best_result = None\n</code></pre>"},{"location":"api/autotune_global/#jax_mppi.autotune_global.RayOptimizer.optimize_all","title":"<code>optimize_all(iterations)</code>","text":"<p>Run full Ray Tune search.</p> <p>Parameters:</p> Name Type Description Default <code>iterations</code> <code>int</code> <p>Number of trials (overrides num_samples if provided)</p> required <p>Returns:</p> Type Description <code>EvaluationResult</code> <p>Best evaluation result found</p> Source code in <code>src/jax_mppi/autotune_global.py</code> <pre><code>def optimize_all(self, iterations: int) -&gt; EvaluationResult:\n    \"\"\"Run full Ray Tune search.\n\n    Args:\n        iterations: Number of trials (overrides num_samples if provided)\n\n    Returns:\n        Best evaluation result found\n    \"\"\"\n    if self.evaluate_fn is None:\n        raise RuntimeError(\"Must call setup_optimization() first\")\n\n    # Use iterations if provided, otherwise use num_samples\n    num_trials = iterations if iterations &gt; 0 else self.num_samples\n\n    # This will be set by AutotuneGlobal\n    raise NotImplementedError(\n        \"RayOptimizer.optimize_all() must be called through AutotuneGlobal\"\n    )\n</code></pre>"},{"location":"api/autotune_global/#jax_mppi.autotune_global.RayOptimizer.optimize_step","title":"<code>optimize_step()</code>","text":"<p>Not supported for Ray Tune (requires full run).</p> Source code in <code>src/jax_mppi/autotune_global.py</code> <pre><code>def optimize_step(self) -&gt; EvaluationResult:\n    \"\"\"Not supported for Ray Tune (requires full run).\"\"\"\n    raise NotImplementedError(\n        \"RayOptimizer only supports optimize_all(), not step-wise optimization\"\n    )\n</code></pre>"},{"location":"api/autotune_global/#jax_mppi.autotune_global.RayOptimizer.setup_optimization","title":"<code>setup_optimization(initial_params, evaluate_fn)</code>","text":"<p>Setup Ray Tune optimization.</p> <p>Parameters:</p> Name Type Description Default <code>initial_params</code> <code>ndarray</code> <p>Initial parameters (not used by Ray Tune)</p> required <code>evaluate_fn</code> <code>Callable[[ndarray], EvaluationResult]</code> <p>Evaluation function</p> required Source code in <code>src/jax_mppi/autotune_global.py</code> <pre><code>def setup_optimization(\n    self,\n    initial_params: np.ndarray,\n    evaluate_fn: Callable[[np.ndarray], EvaluationResult],\n) -&gt; None:\n    \"\"\"Setup Ray Tune optimization.\n\n    Args:\n        initial_params: Initial parameters (not used by Ray Tune)\n        evaluate_fn: Evaluation function\n    \"\"\"\n    self.evaluate_fn = evaluate_fn\n</code></pre>"},{"location":"api/autotune_global/#jax_mppi.autotune_global.save_search_progress_plot","title":"<code>save_search_progress_plot(iteration_costs, output_path='docs/media/autotune_global_progress.png', title='Global Hyperparameter Search Progress', **kwargs)</code>","text":"<p>Save global search progress plot to docs/media.</p> <p>Parameters:</p> Name Type Description Default <code>iteration_costs</code> <code>list[float]</code> <p>List of best costs at each iteration</p> required <code>output_path</code> <code>str | Path</code> <p>Path to save the plot</p> <code>'docs/media/autotune_global_progress.png'</code> <code>title</code> <code>str</code> <p>Plot title</p> <code>'Global Hyperparameter Search Progress'</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to plt.savefig</p> <code>{}</code> Source code in <code>src/jax_mppi/autotune_global.py</code> <pre><code>def save_search_progress_plot(\n    iteration_costs: list[float],\n    output_path: str | Path = \"docs/media/autotune_global_progress.png\",\n    title: str = \"Global Hyperparameter Search Progress\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Save global search progress plot to docs/media.\n\n    Args:\n        iteration_costs: List of best costs at each iteration\n        output_path: Path to save the plot\n        title: Plot title\n        **kwargs: Additional arguments passed to plt.savefig\n    \"\"\"\n    try:\n        import matplotlib.pyplot as plt\n    except ImportError:\n        print(\"Warning: matplotlib not available, skipping visualization\")\n        return\n\n    output_path = Path(output_path)\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n\n    plt.figure(figsize=kwargs.get(\"figsize\", (10, 6)))\n    plt.plot(\n        iteration_costs,\n        marker=\"o\",\n        linewidth=2,\n        markersize=6,\n        label=\"Best Cost\",\n    )\n    plt.fill_between(\n        range(len(iteration_costs)),\n        iteration_costs,\n        alpha=0.3,\n        label=\"Running Best\",\n    )\n    plt.xlabel(\"Iteration\", fontsize=12)\n    plt.ylabel(\"Cost\", fontsize=12)\n    plt.title(title, fontsize=14)\n    plt.legend(fontsize=11)\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n\n    dpi = kwargs.get(\"dpi\", 150)\n    plt.savefig(output_path, dpi=dpi)\n    plt.close()\n    print(f\"Saved search progress plot to: {output_path}\")\n</code></pre>"},{"location":"api/autotune_qd/","title":"Autotune QD","text":"<p>Quality diversity optimization for JAX-MPPI autotuning.</p> <p>This module provides CMA-ME (Covariance Matrix Adaptation MAP-Elites) integration for finding diverse, high-performing parameter configurations.</p> <p>Requires optional dependency: ribs[all]</p> Example <p>from jax_mppi import autotune, autotune_qd</p> <p>tuner = autotune.Autotune( ...     params_to_tune=[...], ...     evaluate_fn=evaluate, ...     optimizer=autotune_qd.CMAMEOpt(population=20, bins=10), ... ) best = tuner.optimize_all(iterations=50)</p>"},{"location":"api/autotune_qd/#jax_mppi.autotune_qd--get-diverse-set-of-solutions","title":"Get diverse set of solutions","text":"<p>diverse_params = tuner.optimizer.get_diverse_top_parameters(n=10)</p>"},{"location":"api/autotune_qd/#jax_mppi.autotune_qd.CMAMEOpt","title":"<code>CMAMEOpt</code>","text":"<p>               Bases: <code>Optimizer</code></p> <p>CMA-ME optimizer for quality diversity optimization.</p> <p>CMA-ME (Covariance Matrix Adaptation MAP-Elites) maintains an archive of diverse, high-performing solutions. This is useful when you want to find multiple good parameter configurations that work well in different scenarios.</p> <p>The archive is organized by \"behavior characteristics\" - properties of the solution that define diversity. For MPPI, these could be: - Mean cost (performance) - Control variance (how aggressive the controller is) - Horizon used etc.</p> <p>Attributes:</p> Name Type Description <code>population</code> <p>Population size per iteration</p> <code>sigma</code> <p>Initial step size</p> <code>bins</code> <p>Number of bins per behavior dimension</p> <code>archive</code> <p>Solution archive (ribs Archive)</p> Source code in <code>src/jax_mppi/autotune_qd.py</code> <pre><code>class CMAMEOpt(Optimizer):\n    \"\"\"CMA-ME optimizer for quality diversity optimization.\n\n    CMA-ME (Covariance Matrix Adaptation MAP-Elites) maintains an archive\n    of diverse, high-performing solutions. This is useful when you want\n    to find multiple good parameter configurations that work well in\n    different scenarios.\n\n    The archive is organized by \"behavior characteristics\" - properties\n    of the solution that define diversity. For MPPI, these could be:\n    - Mean cost (performance)\n    - Control variance (how aggressive the controller is)\n    - Horizon used\n    etc.\n\n    Attributes:\n        population: Population size per iteration\n        sigma: Initial step size\n        bins: Number of bins per behavior dimension\n        archive: Solution archive (ribs Archive)\n    \"\"\"\n\n    def __init__(\n        self,\n        population: int = 20,\n        sigma: float = 0.1,\n        bins: int = 10,\n        behavior_dim: int = 1,\n    ):\n        \"\"\"Initialize CMA-ME optimizer.\n\n        Args:\n            population: Population size per iteration\n            sigma: Initial step size\n            bins: Number of bins per behavior dimension\n            behavior_dim: Dimensionality of behavior space\n        \"\"\"\n        try:\n            from ribs.archives import GridArchive  # type: ignore\n            from ribs.emitters import EvolutionStrategyEmitter  # type: ignore\n            from ribs.schedulers import Scheduler  # type: ignore\n        except ImportError:\n            raise ImportError(\n                \"CMA-ME optimizer requires 'ribs'. Install with: pip install 'ribs[all]'\"\n            )\n\n        self.population = population\n        self.sigma = sigma\n        self.bins = bins\n        self.behavior_dim = behavior_dim\n\n        self.ribs = None  # Will be initialized in setup\n        self.archive = None\n        self.emitters = None\n        self.scheduler = None\n        self.evaluate_fn = None\n        self.solution_dim = None\n\n    def setup_optimization(\n        self,\n        initial_params: np.ndarray,\n        evaluate_fn: Callable[[np.ndarray], EvaluationResult],\n    ) -&gt; None:\n        \"\"\"Initialize CMA-ME archive and emitters.\n\n        Args:\n            initial_params: Initial parameter values\n            evaluate_fn: Evaluation function\n        \"\"\"\n        from ribs.archives import GridArchive  # type: ignore\n        from ribs.emitters import EvolutionStrategyEmitter  # type: ignore\n        from ribs.schedulers import Scheduler  # type: ignore\n\n        self.evaluate_fn = evaluate_fn\n        self.solution_dim = len(initial_params)\n\n        # Create archive with behavior dimensions\n        # For MPPI, we use a simple 1D behavior: normalized cost\n        # Range [0, 1] with bins\n        bounds = [(0.0, 1.0) for _ in range(self.behavior_dim)]\n\n        self.archive = GridArchive(\n            solution_dim=self.solution_dim,\n            dims=[self.bins] * self.behavior_dim,\n            ranges=bounds,\n        )\n\n        # Create emitters (ES-based)\n        self.emitters = [\n            EvolutionStrategyEmitter(\n                archive=self.archive,\n                x0=initial_params,\n                sigma0=self.sigma,\n                batch_size=self.population,\n            )\n        ]\n\n        # Create scheduler\n        self.scheduler = Scheduler(self.archive, self.emitters)\n\n    def _compute_behavior(self, result: EvaluationResult) -&gt; np.ndarray:\n        \"\"\"Compute behavior characteristics from evaluation result.\n\n        For simplicity, we use normalized cost as the behavior.\n        In practice, you might want to use multiple characteristics.\n\n        Args:\n            result: Evaluation result\n\n        Returns:\n            Behavior vector (shape: (behavior_dim,))\n        \"\"\"\n        # Simple behavior: map cost to [0, 1]\n        # Use a heuristic normalization\n        normalized_cost = 1.0 / (1.0 + result.mean_cost)\n        return np.array([normalized_cost])\n\n    def optimize_step(self) -&gt; EvaluationResult:\n        \"\"\"Execute one CMA-ME iteration.\n\n        Returns:\n            Best result from this iteration\n        \"\"\"\n        if self.scheduler is None:\n            raise RuntimeError(\"Must call setup_optimization() first\")\n\n        # Ask for solutions\n        solutions = self.scheduler.ask()\n\n        # Evaluate all solutions\n        results = []\n        objectives = []\n        behaviors = []\n\n        for solution in solutions:\n            result = self.evaluate_fn(solution)\n            results.append(result)\n\n            # Objective is negative cost (we want to maximize quality)\n            objectives.append(-result.mean_cost)\n\n            # Behavior characteristics\n            behavior = self._compute_behavior(result)\n            behaviors.append(behavior)\n\n        # Tell scheduler about results\n        self.scheduler.tell(\n            objectives=np.array(objectives),\n            behaviors=np.array(behaviors),\n        )\n\n        # Return best result from this iteration\n        best_idx = np.argmax(objectives)\n        return results[best_idx]\n\n    def get_diverse_top_parameters(\n        self, n: int = 10\n    ) -&gt; List[Tuple[np.ndarray, float, np.ndarray]]:\n        \"\"\"Get diverse set of top-performing parameters from archive.\n\n        Args:\n            n: Number of diverse solutions to retrieve\n\n        Returns:\n            List of (parameters, cost, behavior) tuples\n        \"\"\"\n        if self.archive is None:\n            raise RuntimeError(\"Must run optimization first\")\n\n        # Get all elite solutions from archive\n        df = self.archive.as_pandas(include_solutions=True)\n\n        if len(df) == 0:\n            return []\n\n        # Sort by objective (negative cost, so higher is better)\n        df = df.sort_values(\"objective\", ascending=False)\n\n        # Take top n diverse solutions\n        results = []\n        for idx in range(min(n, len(df))):\n            row = df.iloc[idx]\n            params = row[\"solution\"]\n            # Objective is negative cost, so negate to get cost\n            cost = -row[\"objective\"]\n            # Behavior is stored in columns like \"behavior_0\", \"behavior_1\", etc.\n            behavior = np.array([\n                row[f\"index_{i}\"] for i in range(self.behavior_dim)\n            ])\n            results.append((params, cost, behavior))\n\n        return results\n\n    def get_archive_stats(self) -&gt; dict:\n        \"\"\"Get statistics about the archive.\n\n        Returns:\n            Dictionary with archive statistics\n        \"\"\"\n        if self.archive is None:\n            raise RuntimeError(\"Must run optimization first\")\n\n        stats = self.archive.stats\n        return {\n            \"num_elites\": stats.num_elites,\n            \"coverage\": stats.coverage,\n            \"qd_score\": stats.qd_score,\n            \"best_objective\": stats.obj_max,\n        }\n</code></pre>"},{"location":"api/autotune_qd/#jax_mppi.autotune_qd.CMAMEOpt.__init__","title":"<code>__init__(population=20, sigma=0.1, bins=10, behavior_dim=1)</code>","text":"<p>Initialize CMA-ME optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>population</code> <code>int</code> <p>Population size per iteration</p> <code>20</code> <code>sigma</code> <code>float</code> <p>Initial step size</p> <code>0.1</code> <code>bins</code> <code>int</code> <p>Number of bins per behavior dimension</p> <code>10</code> <code>behavior_dim</code> <code>int</code> <p>Dimensionality of behavior space</p> <code>1</code> Source code in <code>src/jax_mppi/autotune_qd.py</code> <pre><code>def __init__(\n    self,\n    population: int = 20,\n    sigma: float = 0.1,\n    bins: int = 10,\n    behavior_dim: int = 1,\n):\n    \"\"\"Initialize CMA-ME optimizer.\n\n    Args:\n        population: Population size per iteration\n        sigma: Initial step size\n        bins: Number of bins per behavior dimension\n        behavior_dim: Dimensionality of behavior space\n    \"\"\"\n    try:\n        from ribs.archives import GridArchive  # type: ignore\n        from ribs.emitters import EvolutionStrategyEmitter  # type: ignore\n        from ribs.schedulers import Scheduler  # type: ignore\n    except ImportError:\n        raise ImportError(\n            \"CMA-ME optimizer requires 'ribs'. Install with: pip install 'ribs[all]'\"\n        )\n\n    self.population = population\n    self.sigma = sigma\n    self.bins = bins\n    self.behavior_dim = behavior_dim\n\n    self.ribs = None  # Will be initialized in setup\n    self.archive = None\n    self.emitters = None\n    self.scheduler = None\n    self.evaluate_fn = None\n    self.solution_dim = None\n</code></pre>"},{"location":"api/autotune_qd/#jax_mppi.autotune_qd.CMAMEOpt.get_archive_stats","title":"<code>get_archive_stats()</code>","text":"<p>Get statistics about the archive.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with archive statistics</p> Source code in <code>src/jax_mppi/autotune_qd.py</code> <pre><code>def get_archive_stats(self) -&gt; dict:\n    \"\"\"Get statistics about the archive.\n\n    Returns:\n        Dictionary with archive statistics\n    \"\"\"\n    if self.archive is None:\n        raise RuntimeError(\"Must run optimization first\")\n\n    stats = self.archive.stats\n    return {\n        \"num_elites\": stats.num_elites,\n        \"coverage\": stats.coverage,\n        \"qd_score\": stats.qd_score,\n        \"best_objective\": stats.obj_max,\n    }\n</code></pre>"},{"location":"api/autotune_qd/#jax_mppi.autotune_qd.CMAMEOpt.get_diverse_top_parameters","title":"<code>get_diverse_top_parameters(n=10)</code>","text":"<p>Get diverse set of top-performing parameters from archive.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of diverse solutions to retrieve</p> <code>10</code> <p>Returns:</p> Type Description <code>List[Tuple[ndarray, float, ndarray]]</code> <p>List of (parameters, cost, behavior) tuples</p> Source code in <code>src/jax_mppi/autotune_qd.py</code> <pre><code>def get_diverse_top_parameters(\n    self, n: int = 10\n) -&gt; List[Tuple[np.ndarray, float, np.ndarray]]:\n    \"\"\"Get diverse set of top-performing parameters from archive.\n\n    Args:\n        n: Number of diverse solutions to retrieve\n\n    Returns:\n        List of (parameters, cost, behavior) tuples\n    \"\"\"\n    if self.archive is None:\n        raise RuntimeError(\"Must run optimization first\")\n\n    # Get all elite solutions from archive\n    df = self.archive.as_pandas(include_solutions=True)\n\n    if len(df) == 0:\n        return []\n\n    # Sort by objective (negative cost, so higher is better)\n    df = df.sort_values(\"objective\", ascending=False)\n\n    # Take top n diverse solutions\n    results = []\n    for idx in range(min(n, len(df))):\n        row = df.iloc[idx]\n        params = row[\"solution\"]\n        # Objective is negative cost, so negate to get cost\n        cost = -row[\"objective\"]\n        # Behavior is stored in columns like \"behavior_0\", \"behavior_1\", etc.\n        behavior = np.array([\n            row[f\"index_{i}\"] for i in range(self.behavior_dim)\n        ])\n        results.append((params, cost, behavior))\n\n    return results\n</code></pre>"},{"location":"api/autotune_qd/#jax_mppi.autotune_qd.CMAMEOpt.optimize_step","title":"<code>optimize_step()</code>","text":"<p>Execute one CMA-ME iteration.</p> <p>Returns:</p> Type Description <code>EvaluationResult</code> <p>Best result from this iteration</p> Source code in <code>src/jax_mppi/autotune_qd.py</code> <pre><code>def optimize_step(self) -&gt; EvaluationResult:\n    \"\"\"Execute one CMA-ME iteration.\n\n    Returns:\n        Best result from this iteration\n    \"\"\"\n    if self.scheduler is None:\n        raise RuntimeError(\"Must call setup_optimization() first\")\n\n    # Ask for solutions\n    solutions = self.scheduler.ask()\n\n    # Evaluate all solutions\n    results = []\n    objectives = []\n    behaviors = []\n\n    for solution in solutions:\n        result = self.evaluate_fn(solution)\n        results.append(result)\n\n        # Objective is negative cost (we want to maximize quality)\n        objectives.append(-result.mean_cost)\n\n        # Behavior characteristics\n        behavior = self._compute_behavior(result)\n        behaviors.append(behavior)\n\n    # Tell scheduler about results\n    self.scheduler.tell(\n        objectives=np.array(objectives),\n        behaviors=np.array(behaviors),\n    )\n\n    # Return best result from this iteration\n    best_idx = np.argmax(objectives)\n    return results[best_idx]\n</code></pre>"},{"location":"api/autotune_qd/#jax_mppi.autotune_qd.CMAMEOpt.setup_optimization","title":"<code>setup_optimization(initial_params, evaluate_fn)</code>","text":"<p>Initialize CMA-ME archive and emitters.</p> <p>Parameters:</p> Name Type Description Default <code>initial_params</code> <code>ndarray</code> <p>Initial parameter values</p> required <code>evaluate_fn</code> <code>Callable[[ndarray], EvaluationResult]</code> <p>Evaluation function</p> required Source code in <code>src/jax_mppi/autotune_qd.py</code> <pre><code>def setup_optimization(\n    self,\n    initial_params: np.ndarray,\n    evaluate_fn: Callable[[np.ndarray], EvaluationResult],\n) -&gt; None:\n    \"\"\"Initialize CMA-ME archive and emitters.\n\n    Args:\n        initial_params: Initial parameter values\n        evaluate_fn: Evaluation function\n    \"\"\"\n    from ribs.archives import GridArchive  # type: ignore\n    from ribs.emitters import EvolutionStrategyEmitter  # type: ignore\n    from ribs.schedulers import Scheduler  # type: ignore\n\n    self.evaluate_fn = evaluate_fn\n    self.solution_dim = len(initial_params)\n\n    # Create archive with behavior dimensions\n    # For MPPI, we use a simple 1D behavior: normalized cost\n    # Range [0, 1] with bins\n    bounds = [(0.0, 1.0) for _ in range(self.behavior_dim)]\n\n    self.archive = GridArchive(\n        solution_dim=self.solution_dim,\n        dims=[self.bins] * self.behavior_dim,\n        ranges=bounds,\n    )\n\n    # Create emitters (ES-based)\n    self.emitters = [\n        EvolutionStrategyEmitter(\n            archive=self.archive,\n            x0=initial_params,\n            sigma0=self.sigma,\n            batch_size=self.population,\n        )\n    ]\n\n    # Create scheduler\n    self.scheduler = Scheduler(self.archive, self.emitters)\n</code></pre>"},{"location":"api/autotune_qd/#jax_mppi.autotune_qd.save_qd_heatmap","title":"<code>save_qd_heatmap(costs, output_path='docs/media/autotune_qd_heatmap.png', title='Quality Diversity Archive Heatmap', **kwargs)</code>","text":"<p>Save quality diversity optimization progress to docs/media.</p> <p>Parameters:</p> Name Type Description Default <code>costs</code> <code>list[float]</code> <p>List of best costs at each iteration</p> required <code>output_path</code> <code>str | Path</code> <p>Path to save the plot</p> <code>'docs/media/autotune_qd_heatmap.png'</code> <code>title</code> <code>str</code> <p>Plot title</p> <code>'Quality Diversity Archive Heatmap'</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to plt.savefig</p> <code>{}</code> Source code in <code>src/jax_mppi/autotune_qd.py</code> <pre><code>def save_qd_heatmap(\n    costs: list[float],\n    output_path: str | Path = \"docs/media/autotune_qd_heatmap.png\",\n    title: str = \"Quality Diversity Archive Heatmap\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Save quality diversity optimization progress to docs/media.\n\n    Args:\n        costs: List of best costs at each iteration\n        output_path: Path to save the plot\n        title: Plot title\n        **kwargs: Additional arguments passed to plt.savefig\n    \"\"\"\n    try:\n        import matplotlib.pyplot as plt\n    except ImportError:\n        print(\"Warning: matplotlib not available, skipping visualization\")\n        return\n\n    output_path = Path(output_path)\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n\n    fig, ax = plt.subplots(figsize=kwargs.get(\"figsize\", (10, 6)))\n\n    # Plot convergence\n    ax.plot(\n        costs,\n        marker=\"o\",\n        linewidth=2,\n        markersize=6,\n        label=\"Best Cost\",\n        color=\"blue\",\n    )\n    ax.fill_between(range(len(costs)), costs, alpha=0.2, color=\"blue\")\n    ax.set_xlabel(\"Iteration\", fontsize=12)\n    ax.set_ylabel(\"Cost\", fontsize=12)\n    ax.set_title(title, fontsize=14)\n    ax.legend(fontsize=11)\n    ax.grid(True, alpha=0.3)\n    fig.tight_layout()\n\n    dpi = kwargs.get(\"dpi\", 150)\n    plt.savefig(output_path, dpi=dpi)\n    plt.close()\n    print(f\"Saved QD progress plot to: {output_path}\")\n</code></pre>"},{"location":"api/kmppi/","title":"KMPPI","text":"<p>Kernel MPPI (KMPPI) implementation in JAX.</p> <p>KMPPI uses kernel interpolation to smooth control trajectories by working with a reduced set of control points (theta) rather than the full trajectory. This allows smoother actions with fewer parameters to optimize.</p> <p>Reference: Based on pytorch_mppi KMPPI implementation</p>"},{"location":"api/kmppi/#jax_mppi.kmppi.KMPPIConfig","title":"<code>KMPPIConfig</code>  <code>dataclass</code>","text":"<p>Configuration for Kernel MPPI.</p> Source code in <code>src/jax_mppi/kmppi.py</code> <pre><code>@dataclass(frozen=True)\nclass KMPPIConfig:\n    \"\"\"Configuration for Kernel MPPI.\"\"\"\n\n    # Base MPPI parameters\n    num_samples: int  # K\n    horizon: int  # T\n    nx: int\n    nu: int\n    lambda_: float\n    u_scale: float\n    u_per_command: int\n    step_dependent_dynamics: bool\n    rollout_samples: int  # M\n    rollout_var_cost: float\n    rollout_var_discount: float\n    sample_null_action: bool\n    noise_abs_cost: bool\n\n    # KMPPI-specific parameters\n    num_support_pts: int  # Number of control points for interpolation\n</code></pre>"},{"location":"api/kmppi/#jax_mppi.kmppi.KMPPIState","title":"<code>KMPPIState</code>  <code>dataclass</code>","text":"<p>State for Kernel MPPI.</p> Source code in <code>src/jax_mppi/kmppi.py</code> <pre><code>@register_pytree_node_class\n@dataclass\nclass KMPPIState:\n    \"\"\"State for Kernel MPPI.\"\"\"\n\n    # Base parameters\n    U: jax.Array  # (T, nu) full trajectory (interpolated from theta)\n    u_init: jax.Array  # (nu,) default action for shift\n    noise_mu: jax.Array  # (nu,)\n    noise_sigma: jax.Array  # (nu, nu)\n    noise_sigma_inv: jax.Array\n    u_min: Optional[jax.Array]\n    u_max: Optional[jax.Array]\n    key: jax.Array  # PRNG key\n\n    # KMPPI-specific state\n    theta: jax.Array  # (num_support_pts, nu) control points\n    Tk: jax.Array  # (num_support_pts,) control point times\n    Hs: jax.Array  # (T,) full trajectory times\n\n    def tree_flatten(self):\n        return (\n            (\n                self.U,\n                self.u_init,\n                self.noise_mu,\n                self.noise_sigma,\n                self.noise_sigma_inv,\n                self.u_min,\n                self.u_max,\n                self.key,\n                self.theta,\n                self.Tk,\n                self.Hs,\n            ),\n            None,\n        )\n\n    @classmethod\n    def tree_unflatten(cls, aux_data, children):\n        return cls(*children)\n</code></pre>"},{"location":"api/kmppi/#jax_mppi.kmppi.RBFKernel","title":"<code>RBFKernel</code>","text":"<p>Radial Basis Function kernel for time-domain interpolation.</p> Source code in <code>src/jax_mppi/kmppi.py</code> <pre><code>class RBFKernel:\n    \"\"\"Radial Basis Function kernel for time-domain interpolation.\"\"\"\n\n    def __init__(self, sigma: float = 1.0):\n        \"\"\"Initialize RBF kernel.\n\n        Args:\n            sigma: Bandwidth parameter (controls kernel width)\n        \"\"\"\n        self.sigma = sigma\n\n    def __call__(self, t: jax.Array, tk: jax.Array) -&gt; jax.Array:\n        \"\"\"Evaluate RBF kernel: k(t, tk) = exp(-||t - tk||^2 / (2*sigma^2))\n\n        Args:\n            t: Query times, shape (T,) or (T, 1)\n            tk: Control point times, shape (num_support_pts,) or (num_support_pts, 1)\n\n        Returns:\n            K: kernel matrix, shape (T, num_support_pts)\n        \"\"\"\n        # Ensure proper shapes for broadcasting\n        if t.ndim == 1:\n            t = t[:, None]  # (T, 1)\n        if tk.ndim == 1:\n            tk = tk[None, :]  # (1, num_support_pts)\n\n        # Squared Euclidean distance in 1D time space\n        # t[:, None] - tk creates (T, num_support_pts) difference matrix\n        d = (t - tk) ** 2\n\n        # RBF formula\n        k = jnp.exp(-d / (2 * self.sigma**2 + 1e-8))\n\n        return k\n</code></pre>"},{"location":"api/kmppi/#jax_mppi.kmppi.RBFKernel.__call__","title":"<code>__call__(t, tk)</code>","text":"<p>Evaluate RBF kernel: k(t, tk) = exp(-||t - tk||^2 / (2*sigma^2))</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Array</code> <p>Query times, shape (T,) or (T, 1)</p> required <code>tk</code> <code>Array</code> <p>Control point times, shape (num_support_pts,) or (num_support_pts, 1)</p> required <p>Returns:</p> Name Type Description <code>K</code> <code>Array</code> <p>kernel matrix, shape (T, num_support_pts)</p> Source code in <code>src/jax_mppi/kmppi.py</code> <pre><code>def __call__(self, t: jax.Array, tk: jax.Array) -&gt; jax.Array:\n    \"\"\"Evaluate RBF kernel: k(t, tk) = exp(-||t - tk||^2 / (2*sigma^2))\n\n    Args:\n        t: Query times, shape (T,) or (T, 1)\n        tk: Control point times, shape (num_support_pts,) or (num_support_pts, 1)\n\n    Returns:\n        K: kernel matrix, shape (T, num_support_pts)\n    \"\"\"\n    # Ensure proper shapes for broadcasting\n    if t.ndim == 1:\n        t = t[:, None]  # (T, 1)\n    if tk.ndim == 1:\n        tk = tk[None, :]  # (1, num_support_pts)\n\n    # Squared Euclidean distance in 1D time space\n    # t[:, None] - tk creates (T, num_support_pts) difference matrix\n    d = (t - tk) ** 2\n\n    # RBF formula\n    k = jnp.exp(-d / (2 * self.sigma**2 + 1e-8))\n\n    return k\n</code></pre>"},{"location":"api/kmppi/#jax_mppi.kmppi.RBFKernel.__init__","title":"<code>__init__(sigma=1.0)</code>","text":"<p>Initialize RBF kernel.</p> <p>Parameters:</p> Name Type Description Default <code>sigma</code> <code>float</code> <p>Bandwidth parameter (controls kernel width)</p> <code>1.0</code> Source code in <code>src/jax_mppi/kmppi.py</code> <pre><code>def __init__(self, sigma: float = 1.0):\n    \"\"\"Initialize RBF kernel.\n\n    Args:\n        sigma: Bandwidth parameter (controls kernel width)\n    \"\"\"\n    self.sigma = sigma\n</code></pre>"},{"location":"api/kmppi/#jax_mppi.kmppi.TimeKernel","title":"<code>TimeKernel</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for time-domain kernels used in trajectory interpolation.</p> Source code in <code>src/jax_mppi/kmppi.py</code> <pre><code>class TimeKernel(Protocol):\n    \"\"\"Protocol for time-domain kernels used in trajectory interpolation.\"\"\"\n\n    def __call__(self, t: jax.Array, tk: jax.Array) -&gt; jax.Array:\n        \"\"\"Evaluate kernel between time points.\n\n        Args:\n            t: Query time points, shape (T,) or (T, 1)\n            tk: Control point times, shape (num_support_pts,) or (num_support_pts, 1)\n\n        Returns:\n            K: Kernel matrix, shape (T, num_support_pts)\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/kmppi/#jax_mppi.kmppi.TimeKernel.__call__","title":"<code>__call__(t, tk)</code>","text":"<p>Evaluate kernel between time points.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Array</code> <p>Query time points, shape (T,) or (T, 1)</p> required <code>tk</code> <code>Array</code> <p>Control point times, shape (num_support_pts,) or (num_support_pts, 1)</p> required <p>Returns:</p> Name Type Description <code>K</code> <code>Array</code> <p>Kernel matrix, shape (T, num_support_pts)</p> Source code in <code>src/jax_mppi/kmppi.py</code> <pre><code>def __call__(self, t: jax.Array, tk: jax.Array) -&gt; jax.Array:\n    \"\"\"Evaluate kernel between time points.\n\n    Args:\n        t: Query time points, shape (T,) or (T, 1)\n        tk: Control point times, shape (num_support_pts,) or (num_support_pts, 1)\n\n    Returns:\n        K: Kernel matrix, shape (T, num_support_pts)\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/kmppi/#jax_mppi.kmppi.command","title":"<code>command(config, kmppi_state, current_obs, dynamics, running_cost, kernel_fn, terminal_cost=None, shift=True)</code>","text":"<p>Compute optimal action using Kernel MPPI.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>KMPPIConfig</code> <p>KMPPI configuration</p> required <code>kmppi_state</code> <code>KMPPIState</code> <p>Current KMPPI state</p> required <code>current_obs</code> <code>Array</code> <p>(nx,) current observation/state</p> required <code>dynamics</code> <code>DynamicsFn</code> <p>Dynamics function</p> required <code>running_cost</code> <code>RunningCostFn</code> <p>Running cost function</p> required <code>kernel_fn</code> <code>TimeKernel</code> <p>Kernel function for interpolation</p> required <code>terminal_cost</code> <code>Optional[TerminalCostFn]</code> <p>Optional terminal cost function</p> <code>None</code> <code>shift</code> <code>bool</code> <p>Whether to shift nominal trajectory after computing action</p> <code>True</code> <p>Returns:</p> Name Type Description <code>action</code> <code>Array</code> <p>(u_per_command * nu,) or (nu,) optimal action</p> <code>new_state</code> <code>KMPPIState</code> <p>Updated KMPPI state</p> Source code in <code>src/jax_mppi/kmppi.py</code> <pre><code>def command(\n    config: KMPPIConfig,\n    kmppi_state: KMPPIState,\n    current_obs: jax.Array,\n    dynamics: DynamicsFn,\n    running_cost: RunningCostFn,\n    kernel_fn: TimeKernel,\n    terminal_cost: Optional[TerminalCostFn] = None,\n    shift: bool = True,\n) -&gt; Tuple[jax.Array, KMPPIState]:\n    \"\"\"Compute optimal action using Kernel MPPI.\n\n    Args:\n        config: KMPPI configuration\n        kmppi_state: Current KMPPI state\n        current_obs: (nx,) current observation/state\n        dynamics: Dynamics function\n        running_cost: Running cost function\n        kernel_fn: Kernel function for interpolation\n        terminal_cost: Optional terminal cost function\n        shift: Whether to shift nominal trajectory after computing action\n\n    Returns:\n        action: (u_per_command * nu,) or (nu,) optimal action\n        new_state: Updated KMPPI state\n    \"\"\"\n    # Sample noise in control point space\n    noise_theta, new_key = _sample_noise(\n        kmppi_state.key,\n        config.num_samples,\n        config.num_support_pts,\n        kmppi_state.noise_mu,\n        kmppi_state.noise_sigma,\n        config.sample_null_action,\n    )\n\n    # Perturb control points\n    perturbed_theta = (\n        kmppi_state.theta[None, :, :] + noise_theta\n    )  # (K, num_support_pts, nu)\n    perturbed_theta = _bound_action(\n        perturbed_theta, kmppi_state.u_min, kmppi_state.u_max\n    )\n\n    # Effective noise after bounding\n    effective_noise_theta = perturbed_theta - kmppi_state.theta[None, :, :]\n\n    # Interpolate perturbed control points to full trajectories\n    def interpolate_single(theta_single):\n        U_interp, _ = _kernel_interpolate(\n            kmppi_state.Hs, kmppi_state.Tk, theta_single, kernel_fn\n        )\n        return U_interp\n\n    perturbed_actions = jax.vmap(interpolate_single)(\n        perturbed_theta\n    )  # (K, T, nu)\n    perturbed_actions = _bound_action(\n        perturbed_actions, kmppi_state.u_min, kmppi_state.u_max\n    )\n\n    # Compute rollout costs\n    rollout_costs = _compute_rollout_costs(\n        config,\n        current_obs,\n        perturbed_actions,\n        dynamics,\n        running_cost,\n        terminal_cost,\n    )\n\n    # Compute noise cost (in control point space)\n    noise_costs = _compute_noise_cost(\n        effective_noise_theta,\n        kmppi_state.noise_sigma_inv,\n        config.noise_abs_cost,\n    )\n\n    # Total cost\n    total_costs = rollout_costs + noise_costs\n\n    # Compute importance weights\n    weights = _compute_weights(total_costs, config.lambda_)\n\n    # Update control points (optimization in control point space)\n    delta_theta = jnp.sum(\n        weights[:, None, None] * effective_noise_theta, axis=0\n    )\n    new_theta = kmppi_state.theta + delta_theta\n\n    # Interpolate updated control points to get full trajectory\n    new_U, _ = _kernel_interpolate(\n        kmppi_state.Hs, kmppi_state.Tk, new_theta, kernel_fn\n    )\n\n    # Update state\n    new_state = replace(\n        kmppi_state,\n        U=new_U,\n        theta=new_theta,\n        key=new_key,\n    )\n\n    # Shift nominal trajectory if requested\n    if shift:\n        shifted_theta = _shift_control_points(\n            new_state.theta,\n            new_state.Tk,\n            new_state.u_init,\n            config.u_per_command,\n            kernel_fn,\n        )\n        # Also shift U (via interpolation)\n        shifted_U, _ = _kernel_interpolate(\n            new_state.Hs, new_state.Tk, shifted_theta, kernel_fn\n        )\n        new_state = replace(new_state, U=shifted_U, theta=shifted_theta)\n\n    # Extract action to return\n    if config.u_per_command == 1:\n        action = new_state.U[0] * config.u_scale\n    else:\n        action = (\n            new_state.U[: config.u_per_command].reshape(-1) * config.u_scale\n        )\n\n    return action, new_state\n</code></pre>"},{"location":"api/kmppi/#jax_mppi.kmppi.create","title":"<code>create(nx, nu, noise_sigma, num_samples=100, horizon=15, lambda_=1.0, noise_mu=None, u_min=None, u_max=None, u_init=None, U_init=None, num_support_pts=None, kernel=None, u_scale=1.0, u_per_command=1, step_dependent_dynamics=False, rollout_samples=1, rollout_var_cost=0.0, rollout_var_discount=0.95, sample_null_action=False, noise_abs_cost=False, key=None)</code>","text":"<p>Create KMPPI configuration, state, and kernel.</p> <p>Parameters:</p> Name Type Description Default <code>nx</code> <code>int</code> <p>State dimension</p> required <code>nu</code> <code>int</code> <p>Action dimension</p> required <code>noise_sigma</code> <code>Array</code> <p>(nu, nu) noise covariance matrix</p> required <code>num_samples</code> <code>int</code> <p>Number of MPPI samples (K)</p> <code>100</code> <code>horizon</code> <code>int</code> <p>Planning horizon (T)</p> <code>15</code> <code>lambda_</code> <code>float</code> <p>Temperature parameter for importance weighting</p> <code>1.0</code> <code>noise_mu</code> <code>Optional[Array]</code> <p>(nu,) noise mean (default: zeros)</p> <code>None</code> <code>u_min</code> <code>Optional[Array]</code> <p>(nu,) lower bounds on actions</p> <code>None</code> <code>u_max</code> <code>Optional[Array]</code> <p>(nu,) upper bounds on actions</p> <code>None</code> <code>u_init</code> <code>Optional[Array]</code> <p>(nu,) default action for shift (default: zeros)</p> <code>None</code> <code>U_init</code> <code>Optional[Array]</code> <p>(T, nu) initial trajectory (default: zeros)</p> <code>None</code> <code>num_support_pts</code> <code>Optional[int]</code> <p>Number of control points (default: horizon // 2)</p> <code>None</code> <code>kernel</code> <code>Optional[TimeKernel]</code> <p>TimeKernel instance (default: RBFKernel(sigma=1.0))</p> <code>None</code> <code>u_scale</code> <code>float</code> <p>Scale factor for control</p> <code>1.0</code> <code>u_per_command</code> <code>int</code> <p>Number of control steps per command</p> <code>1</code> <code>step_dependent_dynamics</code> <code>bool</code> <p>Whether dynamics depend on timestep</p> <code>False</code> <code>rollout_samples</code> <code>int</code> <p>Number of rollout samples for stochastic dynamics</p> <code>1</code> <code>rollout_var_cost</code> <code>float</code> <p>Variance cost weight</p> <code>0.0</code> <code>rollout_var_discount</code> <code>float</code> <p>Discount factor for variance cost</p> <code>0.95</code> <code>sample_null_action</code> <code>bool</code> <p>Whether to include null action in samples</p> <code>False</code> <code>noise_abs_cost</code> <code>bool</code> <p>Use absolute value cost for noise</p> <code>False</code> <code>key</code> <code>Optional[Array]</code> <p>PRNG key (default: create new)</p> <code>None</code> <p>Returns:</p> Name Type Description <code>config</code> <code>KMPPIConfig</code> <p>KMPPI configuration</p> <code>state</code> <code>KMPPIState</code> <p>KMPPI initial state</p> <code>kernel_fn</code> <code>TimeKernel</code> <p>Kernel function instance</p> Source code in <code>src/jax_mppi/kmppi.py</code> <pre><code>def create(\n    nx: int,\n    nu: int,\n    noise_sigma: jax.Array,\n    num_samples: int = 100,\n    horizon: int = 15,\n    lambda_: float = 1.0,\n    noise_mu: Optional[jax.Array] = None,\n    u_min: Optional[jax.Array] = None,\n    u_max: Optional[jax.Array] = None,\n    u_init: Optional[jax.Array] = None,\n    U_init: Optional[jax.Array] = None,\n    num_support_pts: Optional[int] = None,\n    kernel: Optional[TimeKernel] = None,\n    u_scale: float = 1.0,\n    u_per_command: int = 1,\n    step_dependent_dynamics: bool = False,\n    rollout_samples: int = 1,\n    rollout_var_cost: float = 0.0,\n    rollout_var_discount: float = 0.95,\n    sample_null_action: bool = False,\n    noise_abs_cost: bool = False,\n    key: Optional[jax.Array] = None,\n) -&gt; Tuple[KMPPIConfig, KMPPIState, TimeKernel]:\n    \"\"\"Create KMPPI configuration, state, and kernel.\n\n    Args:\n        nx: State dimension\n        nu: Action dimension\n        noise_sigma: (nu, nu) noise covariance matrix\n        num_samples: Number of MPPI samples (K)\n        horizon: Planning horizon (T)\n        lambda_: Temperature parameter for importance weighting\n        noise_mu: (nu,) noise mean (default: zeros)\n        u_min: (nu,) lower bounds on actions\n        u_max: (nu,) upper bounds on actions\n        u_init: (nu,) default action for shift (default: zeros)\n        U_init: (T, nu) initial trajectory (default: zeros)\n        num_support_pts: Number of control points (default: horizon // 2)\n        kernel: TimeKernel instance (default: RBFKernel(sigma=1.0))\n        u_scale: Scale factor for control\n        u_per_command: Number of control steps per command\n        step_dependent_dynamics: Whether dynamics depend on timestep\n        rollout_samples: Number of rollout samples for stochastic dynamics\n        rollout_var_cost: Variance cost weight\n        rollout_var_discount: Discount factor for variance cost\n        sample_null_action: Whether to include null action in samples\n        noise_abs_cost: Use absolute value cost for noise\n        key: PRNG key (default: create new)\n\n    Returns:\n        config: KMPPI configuration\n        state: KMPPI initial state\n        kernel_fn: Kernel function instance\n    \"\"\"\n    # Initialize defaults\n    if noise_mu is None:\n        noise_mu = jnp.zeros(nu)\n    if u_init is None:\n        u_init = jnp.zeros(nu)\n    if key is None:\n        key = jax.random.PRNGKey(0)\n    if num_support_pts is None:\n        num_support_pts = max(horizon // 2, 2)  # At least 2 support points\n    if kernel is None:\n        kernel = RBFKernel(sigma=1.0)\n\n    # Scale bounds\n    u_min_scaled = _scaled_bounds(u_min, u_scale)\n    u_max_scaled = _scaled_bounds(u_max, u_scale)\n\n    # Initialize control points (theta) and time grids\n    if U_init is None:\n        theta = jnp.zeros((num_support_pts, nu))\n    else:\n        # Sample initial control points from U_init\n        indices = jnp.linspace(0, horizon - 1, num_support_pts, dtype=jnp.int32)\n        theta = U_init[indices]\n\n    # Time grids\n    Tk = jnp.linspace(0, horizon - 1, num_support_pts)  # Control point times\n    Hs = jnp.linspace(0, horizon - 1, horizon)  # Full trajectory times\n\n    # Interpolate theta to get full trajectory U\n    u_control, _ = _kernel_interpolate(Hs, Tk, theta, kernel)\n\n    # Compute noise covariance inverse\n    noise_sigma_inv = jnp.linalg.inv(noise_sigma)\n\n    # Create config\n    config = KMPPIConfig(\n        num_samples=num_samples,\n        horizon=horizon,\n        nx=nx,\n        nu=nu,\n        lambda_=lambda_,\n        u_scale=u_scale,\n        u_per_command=u_per_command,\n        step_dependent_dynamics=step_dependent_dynamics,\n        rollout_samples=rollout_samples,\n        rollout_var_cost=rollout_var_cost,\n        rollout_var_discount=rollout_var_discount,\n        sample_null_action=sample_null_action,\n        noise_abs_cost=noise_abs_cost,\n        num_support_pts=num_support_pts,\n    )\n\n    # Create state\n    state = KMPPIState(\n        U=u_control,\n        u_init=u_init,\n        noise_mu=noise_mu,\n        noise_sigma=noise_sigma,\n        noise_sigma_inv=noise_sigma_inv,\n        u_min=u_min_scaled,\n        u_max=u_max_scaled,\n        key=key,\n        theta=theta,\n        Tk=Tk,\n        Hs=Hs,\n    )\n\n    return config, state, kernel\n</code></pre>"},{"location":"api/kmppi/#jax_mppi.kmppi.get_rollouts","title":"<code>get_rollouts(config, kmppi_state, current_obs, dynamics, num_rollouts=1)</code>","text":"<p>Generate rollout trajectories using current control sequence.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>KMPPIConfig</code> <p>KMPPI configuration</p> required <code>kmppi_state</code> <code>KMPPIState</code> <p>Current KMPPI state</p> required <code>current_obs</code> <code>Array</code> <p>(nx,) or (batch, nx) current state</p> required <code>dynamics</code> <code>DynamicsFn</code> <p>Dynamics function</p> required <code>num_rollouts</code> <code>int</code> <p>Number of rollout samples</p> <code>1</code> <p>Returns:</p> Name Type Description <code>rollouts</code> <code>Array</code> <p>(num_rollouts, horizon+1, nx) state trajectories</p> Source code in <code>src/jax_mppi/kmppi.py</code> <pre><code>def get_rollouts(\n    config: KMPPIConfig,\n    kmppi_state: KMPPIState,\n    current_obs: jax.Array,\n    dynamics: DynamicsFn,\n    num_rollouts: int = 1,\n) -&gt; jax.Array:\n    \"\"\"Generate rollout trajectories using current control sequence.\n\n    Args:\n        config: KMPPI configuration\n        kmppi_state: Current KMPPI state\n        current_obs: (nx,) or (batch, nx) current state\n        dynamics: Dynamics function\n        num_rollouts: Number of rollout samples\n\n    Returns:\n        rollouts: (num_rollouts, horizon+1, nx) state trajectories\n    \"\"\"\n\n    def single_rollout(carry, _):\n        state = carry\n\n        def step_fn(s, inputs):\n            t, action = inputs\n            next_s = _call_dynamics(\n                dynamics, s, action, t, config.step_dependent_dynamics\n            )\n            next_s_trimmed = _state_for_cost(next_s, config.nx)\n            return next_s, next_s_trimmed\n\n        ts = jnp.arange(config.horizon)\n        _, trajectory = jax.lax.scan(step_fn, state, (ts, kmppi_state.U))\n\n        # Prepend initial state\n        initial_state = _state_for_cost(state, config.nx)\n        full_trajectory = jnp.concatenate(\n            [initial_state[None, :], trajectory], axis=0\n        )\n\n        return state, full_trajectory\n\n    # Handle batched or single state\n    if current_obs.ndim == 1:\n        obs_batch = current_obs[None, :]\n    else:\n        obs_batch = current_obs\n\n    # Generate multiple rollouts\n    _, rollouts = jax.lax.scan(\n        single_rollout, obs_batch[0], jnp.arange(num_rollouts)\n    )\n\n    return rollouts\n</code></pre>"},{"location":"api/kmppi/#jax_mppi.kmppi.reset","title":"<code>reset(config, kmppi_state, kernel_fn, key)</code>","text":"<p>Reset KMPPI state with new random key.</p> Source code in <code>src/jax_mppi/kmppi.py</code> <pre><code>def reset(\n    config: KMPPIConfig,\n    kmppi_state: KMPPIState,\n    kernel_fn: TimeKernel,\n    key: jax.Array,\n) -&gt; KMPPIState:\n    \"\"\"Reset KMPPI state with new random key.\"\"\"\n    # Reset control points to zeros\n    theta_reset = jnp.zeros_like(kmppi_state.theta)\n\n    # Interpolate to get U\n    U_reset, _ = _kernel_interpolate(\n        kmppi_state.Hs, kmppi_state.Tk, theta_reset, kernel_fn\n    )\n\n    return replace(\n        kmppi_state,\n        U=U_reset,\n        theta=theta_reset,\n        key=key,\n    )\n</code></pre>"},{"location":"api/mppi/","title":"MPPI","text":""},{"location":"api/mppi/#jax_mppi.mppi.command","title":"<code>command(config, mppi_state, current_obs, dynamics, running_cost, terminal_cost=None, shift=True)</code>","text":"<p>Compute optimal action and return updated state.</p> Source code in <code>src/jax_mppi/mppi.py</code> <pre><code>def command(\n    config: MPPIConfig,\n    mppi_state: MPPIState,\n    current_obs: jax.Array,\n    dynamics: DynamicsFn,\n    running_cost: RunningCostFn,\n    terminal_cost: Optional[TerminalCostFn] = None,\n    shift: bool = True,\n) -&gt; Tuple[jax.Array, MPPIState]:\n    \"\"\"Compute optimal action and return updated state.\"\"\"\n    noise, key = _sample_noise(\n        mppi_state.key,\n        config.num_samples,\n        config.horizon,\n        mppi_state.noise_mu,\n        mppi_state.noise_sigma,\n        config.sample_null_action,\n    )\n\n    perturbed_actions = mppi_state.U[None, :, :] + noise\n    scaled_actions = perturbed_actions * config.u_scale\n    scaled_actions = _bound_action(\n        scaled_actions, mppi_state.u_min, mppi_state.u_max\n    )\n\n    rollout_costs = _compute_rollout_costs(\n        config,\n        current_obs,\n        scaled_actions,\n        dynamics,\n        running_cost,\n        terminal_cost,\n    )\n    noise_costs = _compute_noise_cost(\n        noise, mppi_state.noise_sigma_inv, config.noise_abs_cost\n    )\n    total_costs = rollout_costs + noise_costs\n\n    weights = _compute_weights(total_costs, config.lambda_)\n    delta_U = jnp.tensordot(weights, noise, axes=1)\n    U_new = mppi_state.U + delta_U\n\n    u_min_scaled, u_max_scaled = _scaled_bounds(\n        mppi_state.u_min, mppi_state.u_max, config.u_scale\n    )\n    U_new = _bound_action(U_new, u_min_scaled, u_max_scaled)\n\n    action_seq = U_new[: config.u_per_command]\n    scaled_action_seq = _bound_action(\n        action_seq * config.u_scale, mppi_state.u_min, mppi_state.u_max\n    )\n    action = (\n        scaled_action_seq[0] if config.u_per_command == 1 else scaled_action_seq\n    )\n\n    new_state = replace(mppi_state, U=U_new, key=key)\n    if shift:\n        new_state = _shift_nominal(new_state, config.u_per_command)\n\n    return action, new_state\n</code></pre>"},{"location":"api/mppi/#jax_mppi.mppi.create","title":"<code>create(nx, nu, noise_sigma, num_samples=100, horizon=15, lambda_=1.0, noise_mu=None, u_min=None, u_max=None, u_init=None, U_init=None, u_scale=1.0, u_per_command=1, step_dependent_dynamics=False, rollout_samples=1, rollout_var_cost=0.0, rollout_var_discount=0.95, sample_null_action=False, noise_abs_cost=False, key=None)</code>","text":"<p>Factory: create config + initial state.</p> Source code in <code>src/jax_mppi/mppi.py</code> <pre><code>def create(\n    nx: int,\n    nu: int,\n    noise_sigma: jax.Array,\n    num_samples: int = 100,\n    horizon: int = 15,\n    lambda_: float = 1.0,\n    noise_mu: Optional[jax.Array] = None,\n    u_min: Optional[jax.Array] = None,\n    u_max: Optional[jax.Array] = None,\n    u_init: Optional[jax.Array] = None,\n    U_init: Optional[jax.Array] = None,\n    u_scale: float = 1.0,\n    u_per_command: int = 1,\n    step_dependent_dynamics: bool = False,\n    rollout_samples: int = 1,\n    rollout_var_cost: float = 0.0,\n    rollout_var_discount: float = 0.95,\n    sample_null_action: bool = False,\n    noise_abs_cost: bool = False,\n    key: Optional[jax.Array] = None,\n) -&gt; Tuple[MPPIConfig, MPPIState]:\n    \"\"\"Factory: create config + initial state.\"\"\"\n    if key is None:\n        key = jax.random.PRNGKey(0)\n\n    config = MPPIConfig(\n        num_samples=num_samples,\n        horizon=horizon,\n        nx=nx,\n        nu=nu,\n        lambda_=lambda_,\n        u_scale=u_scale,\n        u_per_command=u_per_command,\n        step_dependent_dynamics=step_dependent_dynamics,\n        rollout_samples=rollout_samples,\n        rollout_var_cost=rollout_var_cost,\n        rollout_var_discount=rollout_var_discount,\n        sample_null_action=sample_null_action,\n        noise_abs_cost=noise_abs_cost,\n    )\n\n    # Initialize state variables\n    if noise_mu is None:\n        noise_mu = jnp.zeros(nu)\n\n    # Ensure noise_sigma is 2D\n    if noise_sigma.ndim == 1:\n        noise_sigma = jnp.diag(noise_sigma)\n\n    noise_sigma_inv = jnp.linalg.inv(noise_sigma)\n\n    if u_init is None:\n        u_init = jnp.zeros(nu)\n\n    if U_init is None:\n        U_init = jnp.tile(u_init, (horizon, 1))\n\n    mppi_state = MPPIState(\n        U=U_init,\n        u_init=u_init,\n        noise_mu=noise_mu,\n        noise_sigma=noise_sigma,\n        noise_sigma_inv=noise_sigma_inv,\n        u_min=None if u_min is None else jnp.array(u_min),\n        u_max=None if u_max is None else jnp.array(u_max),\n        key=key,\n    )\n\n    return config, mppi_state\n</code></pre>"},{"location":"api/mppi/#jax_mppi.mppi.get_rollouts","title":"<code>get_rollouts(config, mppi_state, current_obs, dynamics, num_rollouts=1)</code>","text":"<p>Forward-simulate trajectories for visualization.</p> Source code in <code>src/jax_mppi/mppi.py</code> <pre><code>def get_rollouts(\n    config: MPPIConfig,\n    mppi_state: MPPIState,\n    current_obs: jax.Array,\n    dynamics: DynamicsFn,\n    num_rollouts: int = 1,\n) -&gt; jax.Array:\n    \"\"\"Forward-simulate trajectories for visualization.\"\"\"\n    noise, key = _sample_noise(\n        mppi_state.key,\n        num_rollouts,\n        config.horizon,\n        mppi_state.noise_mu,\n        mppi_state.noise_sigma,\n        sample_null_action=False,\n    )\n    perturbed_actions = mppi_state.U[None, :, :] + noise\n    scaled_actions = perturbed_actions * config.u_scale\n    scaled_actions = _bound_action(\n        scaled_actions, mppi_state.u_min, mppi_state.u_max\n    )\n\n    def rollout_single(actions, obs):\n        def step_fn(state, inputs):\n            t, action = inputs\n            next_state = _call_dynamics(\n                dynamics, state, action, t, config.step_dependent_dynamics\n            )\n            return next_state, _state_for_cost(next_state, config.nx)\n\n        ts = jnp.arange(config.horizon)\n        init_state = obs\n        _, states = jax.lax.scan(step_fn, init_state, (ts, actions))\n        init_out = _state_for_cost(init_state, config.nx)\n        return jnp.concatenate([init_out[None, :], states], axis=0)\n\n    if current_obs.ndim == 1:\n        rollouts = jax.vmap(lambda a: rollout_single(a, current_obs))(\n            scaled_actions\n        )\n    else:\n        rollouts = jax.vmap(\n            lambda obs: jax.vmap(lambda a: rollout_single(a, obs))(\n                scaled_actions\n            )\n        )(current_obs)\n\n    return rollouts\n</code></pre>"},{"location":"api/mppi/#jax_mppi.mppi.reset","title":"<code>reset(config, mppi_state, key)</code>","text":"<p>Reset nominal trajectory.</p> Source code in <code>src/jax_mppi/mppi.py</code> <pre><code>def reset(\n    config: MPPIConfig, mppi_state: MPPIState, key: jax.Array\n) -&gt; MPPIState:\n    \"\"\"Reset nominal trajectory.\"\"\"\n    U_new = jnp.tile(mppi_state.u_init, (config.horizon, 1))\n    return replace(mppi_state, U=U_new, key=key)\n</code></pre>"},{"location":"api/smppi/","title":"SMPPI","text":"<p>Smooth MPPI (SMPPI) implementation in JAX.</p> <p>SMPPI operates in a lifted control space where the nominal trajectory U represents velocity/acceleration commands rather than direct actions. The actual action sequence is computed through numerical integration, with smoothness penalties on action differences.</p> <p>Reference: Based on pytorch_mppi SMPPI implementation</p>"},{"location":"api/smppi/#jax_mppi.smppi.SMPPIConfig","title":"<code>SMPPIConfig</code>  <code>dataclass</code>","text":"<p>Configuration for Smooth MPPI.</p> <p>Extends base MPPI with smoothness-specific parameters.</p> Source code in <code>src/jax_mppi/smppi.py</code> <pre><code>@dataclass(frozen=True)\nclass SMPPIConfig:\n    \"\"\"Configuration for Smooth MPPI.\n\n    Extends base MPPI with smoothness-specific parameters.\n    \"\"\"\n\n    # Base MPPI parameters\n    num_samples: int  # K\n    horizon: int  # T\n    nx: int\n    nu: int\n    lambda_: float\n    u_scale: float\n    u_per_command: int\n    step_dependent_dynamics: bool\n    rollout_samples: int  # M\n    rollout_var_cost: float\n    rollout_var_discount: float\n    sample_null_action: bool\n    noise_abs_cost: bool\n\n    # SMPPI-specific parameters\n    w_action_seq_cost: float  # Weight on smoothness penalty\n    delta_t: float  # Integration timestep\n</code></pre>"},{"location":"api/smppi/#jax_mppi.smppi.SMPPIState","title":"<code>SMPPIState</code>  <code>dataclass</code>","text":"<p>State for Smooth MPPI.</p> <p>Includes both velocity controls (U) and integrated actions (action_sequence).</p> Source code in <code>src/jax_mppi/smppi.py</code> <pre><code>@register_pytree_node_class\n@dataclass\nclass SMPPIState:\n    \"\"\"State for Smooth MPPI.\n\n    Includes both velocity controls (U) and integrated actions (action_sequence).\n    \"\"\"\n\n    # Base MPPI state\n    U: jax.Array  # (T, nu) velocity/acceleration commands\n    u_init: jax.Array  # (nu,) default velocity for shift\n    noise_mu: jax.Array  # (nu,)\n    noise_sigma: jax.Array  # (nu, nu)\n    noise_sigma_inv: jax.Array\n    u_min: Optional[jax.Array]  # Control velocity bounds\n    u_max: Optional[jax.Array]\n    key: jax.Array  # PRNG key\n\n    # SMPPI-specific state\n    action_sequence: jax.Array  # (T, nu) integrated actions\n    action_min: Optional[jax.Array]  # Action bounds\n    action_max: Optional[jax.Array]\n\n    def tree_flatten(self):\n        return (\n            (\n                self.U,\n                self.u_init,\n                self.noise_mu,\n                self.noise_sigma,\n                self.noise_sigma_inv,\n                self.u_min,\n                self.u_max,\n                self.key,\n                self.action_sequence,\n                self.action_min,\n                self.action_max,\n            ),\n            None,\n        )\n\n    @classmethod\n    def tree_unflatten(cls, aux_data, children):\n        return cls(*children)\n</code></pre>"},{"location":"api/smppi/#jax_mppi.smppi.command","title":"<code>command(config, smppi_state, current_obs, dynamics, running_cost, terminal_cost=None, shift=True)</code>","text":"<p>Compute optimal action using Smooth MPPI.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>SMPPIConfig</code> <p>SMPPI configuration</p> required <code>smppi_state</code> <code>SMPPIState</code> <p>Current SMPPI state</p> required <code>current_obs</code> <code>Array</code> <p>(nx,) current observation/state</p> required <code>dynamics</code> <code>DynamicsFn</code> <p>Dynamics function</p> required <code>running_cost</code> <code>RunningCostFn</code> <p>Running cost function</p> required <code>terminal_cost</code> <code>Optional[TerminalCostFn]</code> <p>Optional terminal cost function</p> <code>None</code> <code>shift</code> <code>bool</code> <p>Whether to shift nominal trajectory after computing action</p> <code>True</code> <p>Returns:</p> Name Type Description <code>action</code> <code>Array</code> <p>(u_per_command * nu,) or (nu,) optimal action</p> <code>new_state</code> <code>SMPPIState</code> <p>Updated SMPPI state</p> Source code in <code>src/jax_mppi/smppi.py</code> <pre><code>def command(\n    config: SMPPIConfig,\n    smppi_state: SMPPIState,\n    current_obs: jax.Array,\n    dynamics: DynamicsFn,\n    running_cost: RunningCostFn,\n    terminal_cost: Optional[TerminalCostFn] = None,\n    shift: bool = True,\n) -&gt; Tuple[jax.Array, SMPPIState]:\n    \"\"\"Compute optimal action using Smooth MPPI.\n\n    Args:\n        config: SMPPI configuration\n        smppi_state: Current SMPPI state\n        current_obs: (nx,) current observation/state\n        dynamics: Dynamics function\n        running_cost: Running cost function\n        terminal_cost: Optional terminal cost function\n        shift: Whether to shift nominal trajectory after computing action\n\n    Returns:\n        action: (u_per_command * nu,) or (nu,) optimal action\n        new_state: Updated SMPPI state\n    \"\"\"\n    # Sample noise in velocity space\n    noise, new_key = _sample_noise(\n        smppi_state.key,\n        config.num_samples,\n        config.horizon,\n        smppi_state.noise_mu,\n        smppi_state.noise_sigma,\n        config.sample_null_action,\n    )\n\n    # Compute perturbed actions and effective noise\n    perturbed_actions, effective_noise = _compute_perturbed_actions_and_noise(\n        config, smppi_state, noise\n    )\n\n    # Compute rollout costs\n    rollout_costs = _compute_rollout_costs(\n        config,\n        current_obs,\n        perturbed_actions,\n        dynamics,\n        running_cost,\n        terminal_cost,\n    )\n\n    # Compute noise cost (in velocity space)\n    noise_costs = _compute_noise_cost(\n        effective_noise,\n        smppi_state.noise_sigma_inv,\n        config.noise_abs_cost,\n    )\n\n    # Compute smoothness cost\n    smoothness_costs = _compute_smoothness_cost(perturbed_actions, config)\n\n    # Total cost combines all three components\n    total_costs = rollout_costs + noise_costs + smoothness_costs\n\n    # Compute importance weights\n    weights = _compute_weights(total_costs, config.lambda_)\n\n    # Weighted update to control velocity\n    delta_U = jnp.sum(weights[:, None, None] * effective_noise, axis=0)\n    new_U = smppi_state.U + delta_U\n\n    # Integrate to update action sequence\n    new_action_sequence = smppi_state.action_sequence + new_U * config.delta_t\n\n    # Update state\n    new_state = replace(\n        smppi_state,\n        U=new_U,\n        action_sequence=new_action_sequence,\n        key=new_key,\n    )\n\n    # Shift nominal trajectory if requested\n    if shift:\n        new_state = _shift_nominal(new_state, shift_steps=config.u_per_command)\n\n    # Extract action to return\n    if config.u_per_command == 1:\n        action = new_action_sequence[0] * config.u_scale\n    else:\n        action = (\n            new_action_sequence[: config.u_per_command].reshape(-1)\n            * config.u_scale\n        )\n\n    return action, new_state\n</code></pre>"},{"location":"api/smppi/#jax_mppi.smppi.create","title":"<code>create(nx, nu, noise_sigma, num_samples=100, horizon=15, lambda_=1.0, noise_mu=None, u_min=None, u_max=None, u_init=None, U_init=None, action_min=None, action_max=None, u_scale=1.0, u_per_command=1, step_dependent_dynamics=False, rollout_samples=1, rollout_var_cost=0.0, rollout_var_discount=0.95, sample_null_action=False, noise_abs_cost=False, w_action_seq_cost=1.0, delta_t=1.0, key=None)</code>","text":"<p>Create SMPPI configuration and initial state.</p> <p>Parameters:</p> Name Type Description Default <code>nx</code> <code>int</code> <p>State dimension</p> required <code>nu</code> <code>int</code> <p>Action dimension</p> required <code>noise_sigma</code> <code>Array</code> <p>(nu, nu) noise covariance matrix</p> required <code>num_samples</code> <code>int</code> <p>Number of MPPI samples (K)</p> <code>100</code> <code>horizon</code> <code>int</code> <p>Planning horizon (T)</p> <code>15</code> <code>lambda_</code> <code>float</code> <p>Temperature parameter for importance weighting</p> <code>1.0</code> <code>noise_mu</code> <code>Optional[Array]</code> <p>(nu,) noise mean (default: zeros)</p> <code>None</code> <code>u_min</code> <code>Optional[Array]</code> <p>(nu,) lower bounds on control velocity</p> <code>None</code> <code>u_max</code> <code>Optional[Array]</code> <p>(nu,) upper bounds on control velocity</p> <code>None</code> <code>u_init</code> <code>Optional[Array]</code> <p>(nu,) default control velocity for shift (default: zeros)</p> <code>None</code> <code>U_init</code> <code>Optional[Array]</code> <p>(T, nu) initial control velocity trajectory (default: zeros)</p> <code>None</code> <code>action_min</code> <code>Optional[Array]</code> <p>(nu,) lower bounds on actions</p> <code>None</code> <code>action_max</code> <code>Optional[Array]</code> <p>(nu,) upper bounds on actions</p> <code>None</code> <code>u_scale</code> <code>float</code> <p>Scale factor for control</p> <code>1.0</code> <code>u_per_command</code> <code>int</code> <p>Number of control steps per command</p> <code>1</code> <code>step_dependent_dynamics</code> <code>bool</code> <p>Whether dynamics depend on timestep</p> <code>False</code> <code>rollout_samples</code> <code>int</code> <p>Number of rollout samples for stochastic dynamics</p> <code>1</code> <code>rollout_var_cost</code> <code>float</code> <p>Variance cost weight</p> <code>0.0</code> <code>rollout_var_discount</code> <code>float</code> <p>Discount factor for variance cost</p> <code>0.95</code> <code>sample_null_action</code> <code>bool</code> <p>Whether to include null action in samples</p> <code>False</code> <code>noise_abs_cost</code> <code>bool</code> <p>Use absolute value cost for noise</p> <code>False</code> <code>w_action_seq_cost</code> <code>float</code> <p>Weight on smoothness penalty</p> <code>1.0</code> <code>delta_t</code> <code>float</code> <p>Integration timestep</p> <code>1.0</code> <code>key</code> <code>Optional[Array]</code> <p>PRNG key (default: create new)</p> <code>None</code> <p>Returns:</p> Name Type Description <code>config</code> <code>SMPPIConfig</code> <p>SMPPI configuration</p> <code>state</code> <code>SMPPIState</code> <p>SMPPI initial state</p> Source code in <code>src/jax_mppi/smppi.py</code> <pre><code>def create(\n    nx: int,\n    nu: int,\n    noise_sigma: jax.Array,\n    num_samples: int = 100,\n    horizon: int = 15,\n    lambda_: float = 1.0,\n    noise_mu: Optional[jax.Array] = None,\n    u_min: Optional[jax.Array] = None,\n    u_max: Optional[jax.Array] = None,\n    u_init: Optional[jax.Array] = None,\n    U_init: Optional[jax.Array] = None,\n    action_min: Optional[jax.Array] = None,\n    action_max: Optional[jax.Array] = None,\n    u_scale: float = 1.0,\n    u_per_command: int = 1,\n    step_dependent_dynamics: bool = False,\n    rollout_samples: int = 1,\n    rollout_var_cost: float = 0.0,\n    rollout_var_discount: float = 0.95,\n    sample_null_action: bool = False,\n    noise_abs_cost: bool = False,\n    w_action_seq_cost: float = 1.0,\n    delta_t: float = 1.0,\n    key: Optional[jax.Array] = None,\n) -&gt; Tuple[SMPPIConfig, SMPPIState]:\n    \"\"\"Create SMPPI configuration and initial state.\n\n    Args:\n        nx: State dimension\n        nu: Action dimension\n        noise_sigma: (nu, nu) noise covariance matrix\n        num_samples: Number of MPPI samples (K)\n        horizon: Planning horizon (T)\n        lambda_: Temperature parameter for importance weighting\n        noise_mu: (nu,) noise mean (default: zeros)\n        u_min: (nu,) lower bounds on control velocity\n        u_max: (nu,) upper bounds on control velocity\n        u_init: (nu,) default control velocity for shift (default: zeros)\n        U_init: (T, nu) initial control velocity trajectory (default: zeros)\n        action_min: (nu,) lower bounds on actions\n        action_max: (nu,) upper bounds on actions\n        u_scale: Scale factor for control\n        u_per_command: Number of control steps per command\n        step_dependent_dynamics: Whether dynamics depend on timestep\n        rollout_samples: Number of rollout samples for stochastic dynamics\n        rollout_var_cost: Variance cost weight\n        rollout_var_discount: Discount factor for variance cost\n        sample_null_action: Whether to include null action in samples\n        noise_abs_cost: Use absolute value cost for noise\n        w_action_seq_cost: Weight on smoothness penalty\n        delta_t: Integration timestep\n        key: PRNG key (default: create new)\n\n    Returns:\n        config: SMPPI configuration\n        state: SMPPI initial state\n    \"\"\"\n    # Initialize defaults\n    if noise_mu is None:\n        noise_mu = jnp.zeros(nu)\n    if u_init is None:\n        u_init = jnp.zeros(nu)\n    if key is None:\n        key = jax.random.PRNGKey(0)\n\n    # Scale bounds\n    u_min_scaled = _scaled_bounds(u_min, u_scale)\n    u_max_scaled = _scaled_bounds(u_max, u_scale)\n    action_min_scaled = _scaled_bounds(action_min, u_scale)\n    action_max_scaled = _scaled_bounds(action_max, u_scale)\n\n    # Symmetric bounds inference\n    if action_min_scaled is not None and action_max_scaled is None:\n        action_max_scaled = -action_min_scaled\n    if action_max_scaled is not None and action_min_scaled is None:\n        action_min_scaled = -action_max_scaled\n\n    # Initialize control velocity trajectory (U starts at zeros for SMPPI)\n    if U_init is None:\n        u_control = jnp.zeros((horizon, nu))\n        action_sequence = jnp.zeros((horizon, nu))\n    else:\n        u_control = jnp.zeros_like(U_init)  # Start with zero velocity\n        action_sequence = (\n            U_init.copy()\n        )  # U_init is interpreted as initial actions\n\n    # Compute noise covariance inverse\n    noise_sigma_inv = jnp.linalg.inv(noise_sigma)\n\n    # Create config\n    config = SMPPIConfig(\n        num_samples=num_samples,\n        horizon=horizon,\n        nx=nx,\n        nu=nu,\n        lambda_=lambda_,\n        u_scale=u_scale,\n        u_per_command=u_per_command,\n        step_dependent_dynamics=step_dependent_dynamics,\n        rollout_samples=rollout_samples,\n        rollout_var_cost=rollout_var_cost,\n        rollout_var_discount=rollout_var_discount,\n        sample_null_action=sample_null_action,\n        noise_abs_cost=noise_abs_cost,\n        w_action_seq_cost=w_action_seq_cost,\n        delta_t=delta_t,\n    )\n\n    # Create state\n    state = SMPPIState(\n        U=u_control,\n        u_init=u_init,\n        noise_mu=noise_mu,\n        noise_sigma=noise_sigma,\n        noise_sigma_inv=noise_sigma_inv,\n        u_min=u_min_scaled,\n        u_max=u_max_scaled,\n        key=key,\n        action_sequence=action_sequence,\n        action_min=action_min_scaled,\n        action_max=action_max_scaled,\n    )\n\n    return config, state\n</code></pre>"},{"location":"api/smppi/#jax_mppi.smppi.get_rollouts","title":"<code>get_rollouts(config, smppi_state, current_obs, dynamics, num_rollouts=1)</code>","text":"<p>Generate rollout trajectories using current action sequence.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>SMPPIConfig</code> <p>SMPPI configuration</p> required <code>smppi_state</code> <code>SMPPIState</code> <p>Current SMPPI state</p> required <code>current_obs</code> <code>Array</code> <p>(nx,) or (batch, nx) current state</p> required <code>dynamics</code> <code>DynamicsFn</code> <p>Dynamics function</p> required <code>num_rollouts</code> <code>int</code> <p>Number of rollout samples</p> <code>1</code> <p>Returns:</p> Name Type Description <code>rollouts</code> <code>Array</code> <p>(num_rollouts, horizon+1, nx) state trajectories</p> Source code in <code>src/jax_mppi/smppi.py</code> <pre><code>def get_rollouts(\n    config: SMPPIConfig,\n    smppi_state: SMPPIState,\n    current_obs: jax.Array,\n    dynamics: DynamicsFn,\n    num_rollouts: int = 1,\n) -&gt; jax.Array:\n    \"\"\"Generate rollout trajectories using current action sequence.\n\n    Args:\n        config: SMPPI configuration\n        smppi_state: Current SMPPI state\n        current_obs: (nx,) or (batch, nx) current state\n        dynamics: Dynamics function\n        num_rollouts: Number of rollout samples\n\n    Returns:\n        rollouts: (num_rollouts, horizon+1, nx) state trajectories\n    \"\"\"\n\n    def single_rollout(carry, _):\n        state = carry\n\n        def step_fn(s, inputs):\n            t, action = inputs\n            next_s = _call_dynamics(\n                dynamics, s, action, t, config.step_dependent_dynamics\n            )\n            next_s_trimmed = _state_for_cost(next_s, config.nx)\n            return next_s, next_s_trimmed\n\n        ts = jnp.arange(config.horizon)\n        _, trajectory = jax.lax.scan(\n            step_fn, state, (ts, smppi_state.action_sequence)\n        )\n\n        # Prepend initial state\n        initial_state = _state_for_cost(state, config.nx)\n        full_trajectory = jnp.concatenate(\n            [initial_state[None, :], trajectory], axis=0\n        )\n\n        return state, full_trajectory\n\n    # Handle batched or single state\n    if current_obs.ndim == 1:\n        obs_batch = current_obs[None, :]\n    else:\n        obs_batch = current_obs\n\n    # Generate multiple rollouts\n    _, rollouts = jax.lax.scan(\n        single_rollout, obs_batch[0], jnp.arange(num_rollouts)\n    )\n\n    return rollouts\n</code></pre>"},{"location":"api/smppi/#jax_mppi.smppi.reset","title":"<code>reset(config, smppi_state, key)</code>","text":"<p>Reset SMPPI state with new random key.</p> Source code in <code>src/jax_mppi/smppi.py</code> <pre><code>def reset(\n    config: SMPPIConfig, smppi_state: SMPPIState, key: jax.Array\n) -&gt; SMPPIState:\n    \"\"\"Reset SMPPI state with new random key.\"\"\"\n    # Reset both U and action_sequence to zeros\n    return replace(\n        smppi_state,\n        U=jnp.zeros_like(smppi_state.U),\n        action_sequence=jnp.zeros_like(smppi_state.action_sequence),\n        key=key,\n    )\n</code></pre>"},{"location":"api/types/","title":"Types","text":""},{"location":"examples/pendulum/","title":"Pendulum Swing-Up","text":"<p>This example demonstrates how to use <code>jax_mppi</code> to control an inverted pendulum. The goal is to swing the pendulum up from a hanging position and stabilize it at the top.</p>"},{"location":"examples/pendulum/#code","title":"Code","text":"<p>The full example code is available in <code>examples/pendulum.py</code>.</p>"},{"location":"examples/pendulum/#dynamics","title":"Dynamics","text":"<p>The pendulum dynamics are defined as a pure function:</p> <pre><code>def pendulum_dynamics(state: jax.Array, action: jax.Array) -&gt; jax.Array:\n    \"\"\"Pendulum dynamics.\n\n    State: [theta, theta_dot]\n        theta: angle from upright (0 = upright, pi = hanging down)\n        theta_dot: angular velocity\n    Action: [torque]\n        torque: applied torque (control input)\n    \"\"\"\n    g = 10.0  # gravity\n    m = 1.0  # mass\n    l = 1.0  # length\n    dt = 0.05  # timestep\n\n    theta, theta_dot = state[0], state[1]\n    torque = action[0]\n\n    # Clip torque to reasonable bounds\n    torque = jnp.clip(torque, -2.0, 2.0)\n\n    # Pendulum dynamics: theta_ddot = (torque - m*g*l*sin(theta)) / (m*l^2)\n    theta_ddot = (torque - m * g * l * jnp.sin(theta)) / (m * l * l)\n\n    # Euler integration\n    theta_dot_next = theta_dot + theta_ddot * dt\n    theta_next = theta + theta_dot_next * dt\n\n    # Normalize angle to [-pi, pi]\n    theta_next = ((theta_next + jnp.pi) % (2 * jnp.pi)) - jnp.pi\n\n    return jnp.array([theta_next, theta_dot_next])\n</code></pre>"},{"location":"examples/pendulum/#cost-function","title":"Cost Function","text":"<p>The running cost penalizes deviation from the upright position and high control effort:</p> <pre><code>def pendulum_cost(state: jax.Array, action: jax.Array) -&gt; jax.Array:\n    theta, theta_dot = state[0], state[1]\n    torque = action[0]\n\n    # Cost for being away from upright (theta=0)\n    angle_cost = theta**2\n\n    # Cost for high angular velocity\n    velocity_cost = 0.1 * theta_dot**2\n\n    # Cost for using torque\n    control_cost = 0.01 * torque**2\n\n    return angle_cost + velocity_cost + control_cost\n</code></pre>"},{"location":"examples/pendulum/#running-the-example","title":"Running the Example","text":"<p>You can run the example using:</p> <pre><code>python examples/pendulum.py --visualize\n</code></pre>"},{"location":"plan/example_performance_investigation/","title":"Performance Investigation: Quadrotor Examples","text":"<p>This document details the findings regarding performance differences between the quadrotor control examples: <code>quadrotor_hover.py</code>, <code>quadrotor_circle.py</code>, and <code>quadrotor_figure8_comparison.py</code>.</p>"},{"location":"plan/example_performance_investigation/#summary","title":"Summary","text":"<ul> <li><code>quadrotor_hover.py</code>: Fast (~0.2s/step simulated).</li> <li><code>quadrotor_circle.py</code>: Previously slow (~45x slower). Optimized to use <code>jax.lax.scan</code> for maximum performance.</li> <li><code>quadrotor_figure8_comparison.py</code>: Previously slowest. Optimized to use <code>jax.lax.scan</code> for maximum performance.</li> </ul>"},{"location":"plan/example_performance_investigation/#root-cause-analysis","title":"Root Cause Analysis","text":"<p>The performance disparity was primarily due to the usage of JAX's Just-In-Time (JIT) compilation and how cost functions are handled in the control loop.</p>"},{"location":"plan/example_performance_investigation/#1-quadrotor_hoverpy-fast","title":"1. <code>quadrotor_hover.py</code> (Fast)","text":"<p>In this example, the MPPI command function is explicitly JIT-compiled by the user, and the cost function is effectively constant (closed over static parameters).</p>"},{"location":"plan/example_performance_investigation/#2-quadrotor_circlepy-slow-optimized","title":"2. <code>quadrotor_circle.py</code> (Slow -&gt; Optimized)","text":"<p>Originally, this example re-created the cost function at every time step to update the reference target. This prevented JIT compilation of the main MPPI loop, forcing it to run in eager execution mode (or incurring massive re-compilation costs).</p> <p>Optimization Implemented: The implementation has been refactored to: 1.  Use <code>step_dependent_dynamics=True</code> to allow passing the time step <code>t</code> to the cost function. 2.  Use <code>jax.lax.scan</code> to wrap the entire simulation loop into a single JIT-compiled kernel. This eliminates Python loop dispatch overhead completely. 3.  Pass the entire reference trajectory to the scan function and use <code>jax.lax.dynamic_slice</code> inside the loop to extract the current horizon's reference. This allows the cost function to close over dynamic data efficiently without recompilation.</p> <p>Parameter Tuning: To improve tracking performance, the following parameters were tuned: - <code>num_samples</code>: Increased from 1000 to 2000. - <code>horizon</code>: Increased from 30 to 50. - <code>lambda</code>: Decreased from 1.0 to 0.1 (sharper selection). - Cost weights: Significantly increased position and velocity weights.</p>"},{"location":"plan/example_performance_investigation/#3-quadrotor_figure8_comparisonpy-slowest-optimized","title":"3. <code>quadrotor_figure8_comparison.py</code> (Slowest -&gt; Optimized)","text":"<p>This example shared the same issue as <code>quadrotor_circle.py</code> but for three different controllers (<code>mppi</code>, <code>smppi</code>, <code>kmppi</code>).</p> <p>Optimization Implemented: Similar to <code>quadrotor_circle.py</code>, the controllers have been updated to use <code>jax.lax.scan</code> for the simulation loop. This required adapting the update logic for all three variants to be compatible with <code>scan</code> and dynamic reference slicing.</p> <p>Parameter Tuning: Parameters were similarly tuned to handle the aggressive figure-8 trajectory (samples=2000, horizon=50, lambda=0.1).</p>"},{"location":"plan/example_performance_investigation/#recommendation-for-future-reference","title":"Recommendation (For Future Reference)","text":"<p>When implementing tracking controllers with JAX MPPI: 1.  Use <code>jax.lax.scan</code>: For simulation loops, wrapping the entire loop in <code>scan</code> provides the best performance by minimizing Python overhead. 2.  Parametrize the Cost Function: Avoid capturing changing concrete values (like current target) in closures if they prevent JIT. 3.  Use Data Dependencies: Pass changing targets as arguments (Tracers) to the JIT-compiled function. 4.  Step-Dependent Dynamics: Use <code>step_dependent_dynamics=True</code> to utilize the relative time index <code>t</code> for looking up references in a passed trajectory slice.</p>"},{"location":"plan/performance_analysis/","title":"Performance Analysis of JAX-MPPI Autotuning","text":"<p>This document outlines the performance bottlenecks and issues identified in the autotuning module of <code>jax_mppi</code>, specifically focusing on the <code>evosax</code> integration.</p>"},{"location":"plan/performance_analysis/#1-architectural-bottleneck-stateful-vs-functional","title":"1. Architectural Bottleneck: Stateful vs Functional","text":"<p>The primary reason why <code>autotune_evosax.py</code> does not achieve expected performance gains over <code>cma</code> (CPU-based) is a fundamental mismatch between the <code>Autotune</code> framework architecture and JAX's functional programming model.</p> <ul> <li>Current Architecture: The <code>Autotune</code> class and <code>TunableParameter</code> interface rely on a shared, mutable <code>ConfigStateHolder</code>. The <code>evaluate_fn</code> is a black-box function that relies on this side-effect-laden state update mechanism.</li> <li>Impact: This prevents <code>vmap</code>-ing the evaluation function over a population of parameters. JAX requires pure functions to parallelize execution. Because <code>TunableParameter.apply_parameter_value</code> modifies the global holder in-place, it cannot be safely used within a <code>jax.vmap</code> or <code>jax.lax.scan</code> context without significant refactoring.</li> </ul>"},{"location":"plan/performance_analysis/#2-sequential-evaluation-in-evosax-optimization","title":"2. Sequential Evaluation in Evosax Optimization","text":"<p>In <code>src/jax_mppi/autotune_evosax.py</code>, the <code>optimize_step</code> method performs the following loop:</p> <pre><code># Evaluate all solutions sequentially\nresults = []\nfitness_values = []\n\nfor x in solutions:\n    result = self.evaluate_fn(np.array(x))  # type: ignore\n    results.append(result)\n    # ...\n</code></pre> <ul> <li>Issue: The population generated by <code>evosax</code> (on GPU) is iterated over in Python. Each candidate solution is converted to a NumPy array, transferred to CPU, and evaluated individually.</li> <li>Consequence: This completely negates the massive parallelization advantage of JAX. Instead of running <code>N</code> simulations in parallel on the GPU, they are run sequentially (or with limited batching if <code>evaluate_fn</code> internally batches, but typically <code>evaluate_fn</code> runs one configuration).</li> <li>Comparison: While <code>cma</code> is CPU-based and expects sequential/parallel CPU evaluation, <code>evosax</code> is designed to run the entire ask-evaluate-tell loop on the GPU. The current implementation uses <code>evosax</code> only for the \"ask\" and \"tell\" steps, leaving the most expensive part (evaluation) to a slow Python loop.</li> </ul>"},{"location":"plan/performance_analysis/#3-data-transfer-overhead","title":"3. Data Transfer Overhead","text":"<p>The interface forces repeated data movement between device and host:</p> <ol> <li><code>solutions</code> (from <code>es.ask</code>) are JAX arrays on GPU.</li> <li><code>np.array(x)</code> moves individual solution vectors to CPU.</li> <li><code>evaluate_fn</code> likely uses JAX internally, so it might move data back to GPU for simulation.</li> <li>Results are moved back to CPU.</li> <li><code>fitness_array = jnp.array(fitness_values)</code> moves costs back to GPU for <code>es.tell</code>.</li> </ol>"},{"location":"plan/performance_analysis/#4-lack-of-end-to-end-jit-compilation","title":"4. Lack of End-to-End JIT Compilation","text":"<p><code>evosax</code> allows for the entire optimization process (multiple generations) to be JIT-compiled using <code>jax.lax.scan</code>.</p> <ul> <li>Current State: <code>optimize_step</code> is a Python method that cannot be JIT-compiled because it calls the Python-based <code>evaluate_fn</code> loop.</li> <li>Unused Code: <code>_create_jax_evaluate_fn</code> exists in <code>autotune_evosax.py</code> but is not utilized effectively to enable JAX-pure evaluation.</li> </ul>"},{"location":"plan/performance_analysis/#5-issues-in-quality-diversity-qd-tuning","title":"5. Issues in Quality Diversity (QD) Tuning","text":"<p>The same sequential evaluation pattern is present in <code>src/jax_mppi/autotune_qd.py</code>:</p> <pre><code>for solution in solutions:\n    result = self.evaluate_fn(solution)\n    results.append(result)\n</code></pre> <p>This limits the scalability of the QD algorithms (CMA-ME), which typically benefit from large population sizes.</p>"},{"location":"plan/performance_analysis/#6-minor-issues","title":"6. Minor Issues","text":"<ul> <li>Hardcoded PRNG Key: <code>EvoSaxOptimizer.setup_optimization</code> resets the random key to <code>jax.random.PRNGKey(0)</code>. This forces deterministic behavior that resets on every setup call, which might not be desired if the user wants to continue optimization or run multiple independent trials.</li> <li>Type Hinting: <code>Autotune.optimize_all</code> is typed to return <code>EvaluationResult</code>, but can return <code>None</code> if <code>iterations=0</code>.</li> <li>Unused Variable: <code>self.jax_evaluate_fn</code> in <code>EvoSaxOptimizer</code> is assigned but never used.</li> </ul>"},{"location":"plan/performance_analysis/#recommendations-for-improvement","title":"Recommendations for Improvement","text":"<ol> <li>Refactor <code>TunableParameter</code>: Create a functional interface where parameters can be applied to a config/state to produce a new config/state without side effects.</li> <li>Vectorized Evaluation: Update <code>Autotune</code> to support a <code>batched_evaluate_fn</code> that accepts a batch of parameters (JAX array) and returns a batch of costs.</li> <li>JIT-compile Loop: Once evaluation is vectorized and pure, use <code>jax.lax.scan</code> to run the optimization loop entirely on the GPU.</li> </ol>"},{"location":"plan/quadrotor_trajectory_following/","title":"Quadrotor Trajectory Following with MPPI","text":"<p>Status: In Progress Branch: <code>feat/quadrotor-traj-foll-example</code> Created: 2026-02-01</p>"},{"location":"plan/quadrotor_trajectory_following/#objective","title":"Objective","text":"<p>Implement a comprehensive set of examples demonstrating quadrotor trajectory following using MPPI control. The goal is to showcase the JAX-MPPI library's capabilities on a realistic robotic system with nonlinear dynamics and provide a reference implementation for users.</p>"},{"location":"plan/quadrotor_trajectory_following/#background","title":"Background","text":"<p>The current JAX-MPPI library includes:</p> <ul> <li>Three MPPI variants: standard MPPI, SMPPI (smooth), and KMPPI (kernel-based)</li> <li>Examples: inverted pendulum, 2D navigation with obstacles</li> <li>Well-structured functional API with JIT compilation support</li> <li>Autotuning infrastructure for hyperparameter optimization</li> </ul> <p>A quadrotor trajectory following example will:</p> <ul> <li>Demonstrate MPPI on a high-dimensional nonlinear system (13D state space)</li> <li>Showcase reference tracking capabilities</li> <li>Provide a realistic robotics benchmark</li> <li>Enable comparison between MPPI variants for trajectory smoothness</li> </ul>"},{"location":"plan/quadrotor_trajectory_following/#theoretical-background","title":"Theoretical Background","text":"<p>The quadrotor is modeled as a rigid body with 6 degrees of freedom. The state space is 13-dimensional, representing position, velocity, orientation (quaternion), and angular velocity.</p> <p>Frame Conventions: - NED (North-East-Down): Global/world frame where Z-axis points down (gravity is positive Z) - FRD (Forward-Right-Down): Body frame where X-axis points forward, Y-axis points right, Z-axis points down</p>"},{"location":"plan/quadrotor_trajectory_following/#state-and-control","title":"State and Control","text":"<p>The state vector \\(\\mathbf{x} \\in \\mathbb{R}^{13}\\) is defined as:</p> <p>[    \\mathbf{x} = [\\mathbf{p}^T, \\mathbf{v}^T, \\mathbf{q}^T, \\boldsymbol{\\omega}^T]^T    ]</p> <p>where:</p> <ul> <li>\\(\\mathbf{p} = [p_x, p_y, p_z]^T\\) is the position in the NED world frame.</li> <li>\\(\\mathbf{v} = [v_x, v_y, v_z]^T\\) is the linear velocity in the NED world frame.</li> <li>\\(\\mathbf{q} = [q_w, q_x, q_y, q_z]^T\\) is the unit quaternion representing orientation (body FRD to world NED).</li> <li>\\(\\boldsymbol{\\omega} = [\\omega_x, \\omega_y, \\omega_z]^T\\) is the angular velocity in the FRD body frame.</li> </ul> <p>The control input \\(\\mathbf{u} \\in \\mathbb{R}^{4}\\) consists of the total thrust and body angular rates:</p> <p>[    \\mathbf{u} = [T, \\omega_{x,cmd}, \\omega_{y,cmd}, \\omega_{z,cmd}]^T    ]</p>"},{"location":"plan/quadrotor_trajectory_following/#dynamics","title":"Dynamics","text":"<p>The system dynamics are governed by the following equations:</p>"},{"location":"plan/quadrotor_trajectory_following/#translational-kinematics","title":"Translational Kinematics","text":"\\[ \\dot{\\mathbf{p}} = \\mathbf{v} \\]"},{"location":"plan/quadrotor_trajectory_following/#translational-dynamics","title":"Translational Dynamics","text":"\\[ \\dot{\\mathbf{v}} = \\mathbf{g} + \\frac{1}{m} R(\\mathbf{q}) \\begin{bmatrix} 0 \\\\ 0 \\\\ -T \\end{bmatrix} \\] <p>where \\(\\mathbf{g} = [0, 0, g]^T\\) is the gravity vector in NED frame (positive down), \\(m\\) is the mass, \\(T\\) is the thrust magnitude (positive), and \\(R(\\mathbf{q})\\) is the rotation matrix from FRD body frame to NED world frame. The thrust vector in body frame is \\([0, 0, -T]^T\\) (upward thrust is negative Z in FRD).</p>"},{"location":"plan/quadrotor_trajectory_following/#rotational-kinematics","title":"Rotational Kinematics","text":"<p>The time derivative of the quaternion is given by:</p> \\[ \\dot{\\mathbf{q}} = \\frac{1}{2} \\mathbf{q} \\otimes \\begin{bmatrix} 0 \\\\ \\boldsymbol{\\omega} \\end{bmatrix} \\] <p>where \\(\\otimes\\) denotes quaternion multiplication. In matrix form involving the skew-symmetric matrix</p> \\[ \\dot{\\mathbf{q}} = \\frac{1}{2} \\Omega(\\boldsymbol{\\omega}) \\mathbf{q} \\] <p>Note: The implementation must ensure \\(\\|\\mathbf{q}\\| = 1\\), typically by normalization after integration.</p>"},{"location":"plan/quadrotor_trajectory_following/#rotational-dynamics-first-order-actuator-model","title":"Rotational Dynamics (First-order actuator model)","text":"\\[ \\dot{\\boldsymbol{\\omega}} = \\frac{1}{\\tau_\\omega} (\\boldsymbol{\\omega}_{cmd} - \\boldsymbol{\\omega}) \\] <p>where \\(\\tau_\\omega\\) is the time constant for the angular velocity tracking.</p>"},{"location":"plan/quadrotor_trajectory_following/#cost-function","title":"Cost Function","text":"<p>The MPPI controller optimizes a cost function \\(J\\) over a horizon \\(H\\). The instantaneous cost \\(C(\\mathbf{x}_t, \\mathbf{u}_t)\\) is defined as</p> \\[ C(\\mathbf{x}_t, \\mathbf{u}_t) = \\|\\mathbf{p}_t - \\mathbf{p}_{ref,t}\\|_{Q_{pos}}^2 + \\|\\mathbf{v}_t - \\mathbf{v}_{ref,t}\\|_{Q_{vel}}^2 + \\|\\mathbf{u}_t\\|_{R}^2 \\] <p>where \\(\\|\\mathbf{z}\\|_W^2 = \\mathbf{z}^T W \\mathbf{z}\\).</p>"},{"location":"plan/quadrotor_trajectory_following/#requirements","title":"Requirements","text":""},{"location":"plan/quadrotor_trajectory_following/#functional-requirements","title":"Functional Requirements","text":""},{"location":"plan/quadrotor_trajectory_following/#quadrotor-dynamics-model","title":"Quadrotor Dynamics Model","text":"<ul> <li>6-DOF rigid body dynamics</li> <li>State representation: position (3D), velocity (3D), orientation (quaternion), angular velocity (3D) = 13D</li> <li>Control inputs: body thrust + body rates (roll, pitch, yaw rates)</li> <li>Physical parameters: mass, inertia matrix, arm length</li> <li>Full nonlinear dynamics with quaternion-based attitude representation</li> </ul>"},{"location":"plan/quadrotor_trajectory_following/#trajectory-generation","title":"Trajectory Generation","text":"<ul> <li>Multiple reference trajectory types:</li> <li>Circular/helical trajectories</li> <li>Lemniscate (figure-8) trajectories</li> <li>Minimum snap polynomial trajectories</li> <li>Waypoint-based trajectories</li> <li>Time-parameterized trajectories with position, velocity, acceleration references</li> </ul>"},{"location":"plan/quadrotor_trajectory_following/#cost-functions","title":"Cost Functions","text":"<ul> <li>Position tracking error (weighted L2 norm)</li> <li>Velocity tracking error</li> <li>Attitude tracking error (quaternion distance: 1 - |q^T q_ref|)</li> <li>Control effort penalty (R matrix on actions)</li> <li>Trajectory smoothness penalty (action rate limiting)</li> <li>Terminal cost for goal convergence</li> </ul>"},{"location":"plan/quadrotor_trajectory_following/#examples-to-implement","title":"Examples to Implement","text":"<ul> <li>Example 1: Basic hover control (stabilization around setpoint)</li> <li>Example 2: Circular trajectory following</li> <li>Example 3: Figure-8 trajectory with MPPI/SMPPI/KMPPI comparison</li> <li>Example 4: Minimum snap trajectory following</li> <li>Example 5: Obstacle avoidance during trajectory following (stretch goal)</li> </ul>"},{"location":"plan/quadrotor_trajectory_following/#visualization","title":"Visualization","text":"<ul> <li>3D trajectory plots (reference vs actual)</li> <li>Tracking error over time</li> <li>Control inputs over time</li> <li>Energy consumption</li> <li>Optional: animated 3D quadrotor visualization</li> </ul>"},{"location":"plan/quadrotor_trajectory_following/#non-functional-requirements","title":"Non-Functional Requirements","text":"<ul> <li>Performance: JIT-compiled control loops running at &gt;100 Hz on CPU</li> <li>Code Quality: Follow existing examples pattern (pendulum.py structure)</li> <li>Documentation: Clear docstrings, inline comments for dynamics equations</li> <li>Testing: Unit tests for dynamics, cost functions, and integration tests</li> </ul>"},{"location":"plan/quadrotor_trajectory_following/#technical-design","title":"Technical Design","text":""},{"location":"plan/quadrotor_trajectory_following/#state-representation","title":"State Representation","text":""},{"location":"plan/quadrotor_trajectory_following/#13d-state-with-quaternion-representation","title":"13D State with Quaternion Representation","text":"<pre><code>state = [\n    px, py, pz,        # position (3)\n    vx, vy, vz,        # velocity (3)\n    qw, qx, qy, qz,    # quaternion (4) - unit norm constraint\n    wx, wy, wz         # angular velocity in body frame (3)\n]\n</code></pre>"},{"location":"plan/quadrotor_trajectory_following/#rationale","title":"Rationale","text":"<ul> <li>Quaternions avoid gimbal lock singularities present in Euler angle representations</li> <li>More numerically stable for aggressive maneuvers</li> <li>Standard representation in modern quadrotor control literature</li> <li>Unit norm constraint: ||q|| = 1 (enforced after integration)</li> </ul>"},{"location":"plan/quadrotor_trajectory_following/#control-input-representation","title":"Control Input Representation","text":""},{"location":"plan/quadrotor_trajectory_following/#4d-control-body-thrust-body-rates","title":"4D Control: Body Thrust + Body Rates","text":"<pre><code>action = [\n    T,              # total thrust magnitude (N) - [0, max_thrust]\n                    # Acts in -Z direction of FRD body frame (upward)\n    wx_cmd,         # roll rate command (rad/s) - body X-axis (FRD forward)\n    wy_cmd,         # pitch rate command (rad/s) - body Y-axis (FRD right)\n    wz_cmd          # yaw rate command (rad/s) - body Z-axis (FRD down)\n]\n</code></pre>"},{"location":"plan/quadrotor_trajectory_following/#rationale_1","title":"Rationale","text":"<ul> <li>Direct control of thrust and angular velocities</li> <li>Easier to enforce control bounds than motor-level commands</li> <li>More intuitive for trajectory tracking</li> <li>Standard in many quadrotor control frameworks</li> <li>FRD body frame convention: thrust acts in -Z direction (upward)</li> </ul>"},{"location":"plan/quadrotor_trajectory_following/#dynamics-model","title":"Dynamics Model","text":"<p>Implement a modular dynamics function following the library's pattern:</p> <pre><code>def quadrotor_dynamics(\n    state: jax.Array,\n    action: jax.Array,\n    dt: float = 0.01,\n    mass: float = 1.0,\n    inertia: jax.Array = jnp.eye(3) * 0.1,\n    gravity: float = 9.81,\n    tau_omega: float = 0.05  # angular velocity time constant\n) -&gt; jax.Array:\n    \"\"\"\n    6-DOF quadrotor dynamics with quaternion representation.\n    Frame conventions: NED (world), FRD (body)\n\n    State: [px, py, pz, vx, vy, vz, qw, qx, qy, qz, wx, wy, wz] (13D)\n    - Position/velocity in NED world frame\n    - Quaternion: body FRD to world NED\n    - Angular velocity in FRD body frame\n\n    Action: [T, wx_cmd, wy_cmd, wz_cmd] (4D)\n    - T: thrust magnitude (positive, acts in -Z body direction)\n    - w_cmd: angular rate commands in FRD body frame\n\n    Returns: next_state after dt using RK4 integration\n    \"\"\"\n    # Extract state components\n    pos = state[0:3]\n    vel = state[3:6]\n    quat = state[6:10]  # [qw, qx, qy, qz]\n    omega = state[10:13]  # angular velocity in FRD body frame\n\n    # Extract control\n    thrust = action[0]  # positive magnitude\n    omega_cmd = action[1:4]\n\n    # Rotation matrix from FRD body to NED world frame\n    R = quaternion_to_rotation_matrix(quat)\n\n    # Translational dynamics (NED world frame)\n    # Gravity: positive Z in NED (downward)\n    f_gravity = jnp.array([0, 0, mass * gravity])\n    # Thrust in body frame: [0, 0, -T] (upward in FRD)\n    # Transform to world frame\n    f_thrust = R @ jnp.array([0, 0, -thrust])\n    accel = (f_gravity + f_thrust) / mass\n\n    # Rotational dynamics (FRD body frame, first-order model)\n    # For more realism, can use: omega_dot = inv(I) @ (torque - omega x (I @ omega))\n    omega_dot = (omega_cmd - omega) / tau_omega\n\n    # Quaternion kinematics: q_dot = 0.5 * Omega(omega) @ q\n    # where Omega(omega) is the skew-symmetric matrix\n    q_dot = 0.5 * jnp.array([\n        -omega[0]*quat[1] - omega[1]*quat[2] - omega[2]*quat[3],  # qw_dot\n         omega[0]*quat[0] + omega[2]*quat[2] - omega[1]*quat[3],  # qx_dot\n         omega[1]*quat[0] - omega[2]*quat[1] + omega[0]*quat[3],  # qy_dot\n         omega[2]*quat[0] + omega[1]*quat[1] - omega[0]*quat[2]   # qz_dot\n    ])\n\n    # State derivative\n    state_dot = jnp.concatenate([vel, accel, q_dot, omega_dot])\n\n    # Integration (can use RK4 for better accuracy)\n    next_state = state + dt * state_dot\n\n    # Normalize quaternion to maintain unit norm\n    next_quat = next_state[6:10]\n    next_quat = next_quat / jnp.linalg.norm(next_quat)\n    next_state = next_state.at[6:10].set(next_quat)\n\n    return next_state\n</code></pre>"},{"location":"plan/quadrotor_trajectory_following/#key-implementation-notes","title":"Key Implementation Notes","text":"<ul> <li>Frame Conventions: NED world frame, FRD body frame</li> <li>Gravity: Acts in +Z direction in NED (down is positive)</li> <li>Thrust: Magnitude T (positive) acts in -Z direction in FRD (upward)</li> <li>Quaternion normalization after integration is critical</li> <li>RK4 integration recommended for better accuracy</li> <li>First-order model for angular velocity (can be extended to full Euler dynamics)</li> </ul>"},{"location":"plan/quadrotor_trajectory_following/#cost-function-design","title":"Cost Function Design","text":""},{"location":"plan/quadrotor_trajectory_following/#running-cost","title":"Running Cost","text":"<pre><code>def trajectory_running_cost(\n    state: jax.Array,\n    action: jax.Array,\n    reference: jax.Array,\n    t: int,\n    Q_pos: jax.Array,\n    Q_vel: jax.Array,\n    Q_att: jax.Array,\n    R: jax.Array\n) -&gt; float:\n    \"\"\"\n    Trajectory tracking cost with control penalty.\n\n    reference: [px_ref, py_ref, pz_ref, vx_ref, vy_ref, vz_ref, ...]\n    \"\"\"\n    # Extract reference for current time step\n    ref_t = reference[t]  # or interpolate\n\n    # Position tracking error\n    pos_error = state[0:3] - ref_t[0:3]\n    cost_pos = pos_error.T @ Q_pos @ pos_error\n\n    # Velocity tracking error\n    vel_error = state[3:6] - ref_t[3:6]\n    cost_vel = vel_error.T @ Q_vel @ vel_error\n\n    # Attitude tracking (optional)\n    # att_error = ...\n    # cost_att = att_error.T @ Q_att @ att_error\n\n    # Control effort\n    cost_control = action.T @ R @ action\n\n    return cost_pos + cost_vel + cost_control\n</code></pre>"},{"location":"plan/quadrotor_trajectory_following/#terminal-cost","title":"Terminal Cost","text":"<pre><code>def trajectory_terminal_cost(\n    state: jax.Array,\n    last_action: jax.Array,\n    goal: jax.Array,\n    Q_terminal: jax.Array\n) -&gt; float:\n    \"\"\"Terminal cost for reaching goal state.\"\"\"\n    error = state - goal\n    return error.T @ Q_terminal @ error\n</code></pre>"},{"location":"plan/quadrotor_trajectory_following/#trajectory-generators","title":"Trajectory Generators","text":"<p>Implement modular trajectory generators:</p> <pre><code>def generate_circle_trajectory(\n    radius: float,\n    height: float,\n    period: float,\n    num_steps: int,\n    dt: float\n) -&gt; jax.Array:\n    \"\"\"\n    Generate circular trajectory in NED frame.\n\n    Args:\n        radius: Circle radius in xy plane (m)\n        height: Altitude in NED frame (positive down, e.g., -5.0 for 5m above ground)\n        period: Period of one revolution (s)\n        num_steps: Number of trajectory points\n        dt: Time step (s)\n    \"\"\"\n    t = jnp.arange(num_steps) * dt\n    omega = 2 * jnp.pi / period\n\n    x = radius * jnp.cos(omega * t)\n    y = radius * jnp.sin(omega * t)\n    z = jnp.ones_like(t) * height  # NED: positive down\n\n    vx = -radius * omega * jnp.sin(omega * t)\n    vy = radius * omega * jnp.cos(omega * t)\n    vz = jnp.zeros_like(t)\n\n    # Stack into trajectory array\n    trajectory = jnp.stack([x, y, z, vx, vy, vz], axis=1)\n    return trajectory\n\n\ndef generate_lemniscate_trajectory(\n    scale: float,\n    height: float,\n    period: float,\n    num_steps: int,\n    dt: float\n) -&gt; jax.Array:\n    \"\"\"\n    Generate figure-8 (lemniscate) trajectory in NED frame.\n\n    Args:\n        scale: Size of the figure-8 (m)\n        height: Altitude in NED frame (positive down, e.g., -5.0 for 5m above ground)\n        period: Period of one complete figure-8 (s)\n        num_steps: Number of trajectory points\n        dt: Time step (s)\n    \"\"\"\n    t = jnp.arange(num_steps) * dt\n    omega = 2 * jnp.pi / period\n\n    # Lemniscate of Gerono\n    x = scale * jnp.sin(omega * t)\n    y = scale * jnp.sin(omega * t) * jnp.cos(omega * t)\n    z = jnp.ones_like(t) * height  # NED: positive down\n\n    # Velocities (derivatives)\n    vx = scale * omega * jnp.cos(omega * t)\n    vy = scale * omega * (jnp.cos(omega * t)**2 - jnp.sin(omega * t)**2)\n    vz = jnp.zeros_like(t)\n\n    trajectory = jnp.stack([x, y, z, vx, vy, vz], axis=1)\n    return trajectory\n</code></pre>"},{"location":"plan/quadrotor_trajectory_following/#implementation-plan","title":"Implementation Plan","text":""},{"location":"plan/quadrotor_trajectory_following/#phase-1-core-components","title":"Phase 1: Core Components \u2713","text":"<ul> <li> Explore existing codebase</li> <li> Create feature branch <code>feat/quadrotor-traj-foll-example</code></li> <li> Draft implementation plan</li> <li> Implement quadrotor dynamics module (<code>src/jax_mppi/dynamics/quadrotor.py</code>)</li> <li> Quaternion utilities (to rotation matrix, normalization, etc.)</li> <li> Quaternion kinematics</li> <li> 6-DOF dynamics with RK4 integration</li> <li> Unit tests for dynamics (quaternion norm preservation, energy conservation)</li> <li> Implement trajectory cost functions (<code>src/jax_mppi/costs/quadrotor.py</code>)</li> <li> Position/velocity tracking cost</li> <li> Quaternion-based attitude tracking cost</li> <li> Terminal cost</li> <li> Unit tests for costs</li> </ul>"},{"location":"plan/quadrotor_trajectory_following/#phase-2-trajectory-generators","title":"Phase 2: Trajectory Generators \u2713","text":"<ul> <li> Create trajectory generation utilities (<code>examples/quadrotor/trajectories.py</code>)</li> <li> Circular trajectory</li> <li> Figure-8 (lemniscate) trajectory</li> <li> Hover setpoint</li> <li> Helix trajectory (bonus)</li> <li> Waypoint interpolation with cubic Hermite splines</li> <li> Trajectory metrics computation</li> <li> Unit tests for trajectory generators (28 tests, all passing)</li> </ul>"},{"location":"plan/quadrotor_trajectory_following/#phase-3-basic-examples","title":"Phase 3: Basic Examples \u2713","text":"<ul> <li> Example 1: Hover control (<code>examples/quadrotor_hover.py</code>)</li> <li> Stabilization around fixed setpoint</li> <li> Visualization of state vs time</li> <li> Performance metrics (settling time, overshoot)</li> <li> Example 2: Circle following (<code>examples/quadrotor_circle.py</code>)</li> <li> Circular trajectory tracking</li> <li> Tracking error visualization</li> <li> Control input visualization</li> <li> 3D trajectory plotting</li> <li> Integration tests (11 tests covering both examples)</li> </ul>"},{"location":"plan/quadrotor_trajectory_following/#phase-4-advanced-examples","title":"Phase 4: Advanced Examples \u2713","text":"<ul> <li> Example 3: Figure-8 comparison (<code>examples/quadrotor_figure8_comparison.py</code>)</li> <li> MPPI vs SMPPI vs KMPPI comparison</li> <li> Smoothness metrics (control rate, jerk)</li> <li> Energy consumption comparison</li> <li> Side-by-side trajectory plots (6 subplots)</li> <li> Comprehensive performance comparison table</li> <li> Example 4: Custom trajectory (<code>examples/quadrotor_custom_trajectory.py</code>)</li> <li> Waypoint-based trajectories with cubic Hermite interpolation</li> <li> User-defined reference trajectories</li> <li> Waypoint passage verification</li> <li> Command-line waypoint parsing</li> <li> Integration tests (13 tests for advanced examples)</li> </ul>"},{"location":"plan/quadrotor_trajectory_following/#phase-5-documentation-and-polish","title":"Phase 5: Documentation and Polish","text":"<ul> <li> Add comprehensive docstrings</li> <li> Create README for quadrotor examples (<code>examples/quadrotor/README.md</code>)</li> <li> Add theory documentation (<code>docs/examples/quadrotor.md</code>)</li> <li> Integration tests</li> <li> Performance benchmarks</li> <li> Update main README with quadrotor examples</li> </ul>"},{"location":"plan/quadrotor_trajectory_following/#phase-6-stretch-goals-optional","title":"Phase 6: Stretch Goals (Optional)","text":"<ul> <li> Obstacle avoidance during trajectory following</li> <li> Full Euler dynamics for rotational motion (torque-based control)</li> <li> Motor-level control (PWM to thrust mapping)</li> <li> Wind disturbance modeling</li> <li> Autotuning example for quadrotor MPPI hyperparameters</li> <li> Real-time visualization with animation</li> <li> ROS integration example</li> </ul>"},{"location":"plan/quadrotor_trajectory_following/#file-structure","title":"File Structure","text":"<pre><code>jax_mppi/\n\u251c\u2500\u2500 src/jax_mppi/\n\u2502   \u251c\u2500\u2500 dynamics/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 linear.py\n\u2502   \u2502   \u2514\u2500\u2500 quadrotor.py         # NEW: Quadrotor dynamics\n\u2502   \u251c\u2500\u2500 costs/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 basic.py\n\u2502   \u2502   \u2514\u2500\u2500 quadrotor.py         # NEW: Quadrotor-specific costs\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 examples/\n\u2502   \u251c\u2500\u2500 pendulum.py\n\u2502   \u251c\u2500\u2500 smooth_comparison.py\n\u2502   \u251c\u2500\u2500 quadrotor/               # NEW: Quadrotor examples directory\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 trajectories.py      # NEW: Trajectory generators\n\u2502   \u2502   \u251c\u2500\u2500 plotting.py          # NEW: Visualization utilities\n\u2502   \u2502   \u2514\u2500\u2500 README.md            # NEW: Quadrotor examples guide\n\u2502   \u251c\u2500\u2500 quadrotor_hover.py       # NEW: Example 1\n\u2502   \u251c\u2500\u2500 quadrotor_circle.py      # NEW: Example 2\n\u2502   \u251c\u2500\u2500 quadrotor_figure8_comparison.py  # NEW: Example 3\n\u2502   \u2514\u2500\u2500 quadrotor_custom_trajectory.py   # NEW: Example 4\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 test_quadrotor_dynamics.py  # NEW\n\u2502   \u251c\u2500\u2500 test_quadrotor_costs.py     # NEW\n\u2502   \u2514\u2500\u2500 test_quadrotor_examples.py  # NEW\n\u2514\u2500\u2500 docs/\n    \u251c\u2500\u2500 plan/\n    \u2502   \u2514\u2500\u2500 quadrotor_trajectory_following.md  # This file\n    \u2514\u2500\u2500 examples/\n        \u2514\u2500\u2500 quadrotor.md         # NEW: Theory and usage guide\n</code></pre>"},{"location":"plan/quadrotor_trajectory_following/#success-criteria","title":"Success Criteria","text":"<ol> <li>Functionality: All examples run without errors and demonstrate trajectory following</li> <li>Performance: Control loops run at &gt;100 Hz on CPU (JIT-compiled)</li> <li>Accuracy: Tracking error &lt;5% of trajectory scale for well-tuned parameters</li> <li>Code Quality: Follows existing code style, comprehensive tests (&gt;80% coverage)</li> <li>Documentation: Clear README, docstrings, and theory documentation</li> <li>Usability: New users can run examples out-of-the-box with minimal setup</li> </ol>"},{"location":"plan/quadrotor_trajectory_following/#testing-strategy","title":"Testing Strategy","text":""},{"location":"plan/quadrotor_trajectory_following/#unit-tests","title":"Unit Tests","text":"<ul> <li>Dynamics model: verify state evolution, energy conservation</li> <li>Cost functions: verify gradient correctness, cost bounds</li> <li>Trajectory generators: verify continuity, derivative correctness</li> </ul>"},{"location":"plan/quadrotor_trajectory_following/#integration-tests","title":"Integration Tests","text":"<ul> <li>End-to-end MPPI control loop with quadrotor</li> <li>JIT compilation compatibility</li> <li>Batch rollout generation for visualization</li> </ul>"},{"location":"plan/quadrotor_trajectory_following/#performance-tests","title":"Performance Tests","text":"<ul> <li>Benchmark control loop frequency</li> <li>Memory usage profiling</li> <li>Comparison with baseline implementations</li> </ul>"},{"location":"plan/quadrotor_trajectory_following/#risk-mitigation","title":"Risk Mitigation","text":"Risk Impact Mitigation Dynamics too complex for real-time control High Profile performance early, optimize JIT compilation Quaternion norm drift during integration Medium Normalize after each integration step Poor tracking performance Medium Implement autotuning example, provide tuning guidelines Integration complexity Low Follow existing example patterns closely"},{"location":"plan/quadrotor_trajectory_following/#dependencies","title":"Dependencies","text":"<ul> <li>JAX (already required)</li> <li>matplotlib (for visualization, already used in examples)</li> <li>scipy (optional, for minimum snap trajectories)</li> <li>All dependencies should be compatible with existing <code>pyproject.toml</code></li> </ul>"},{"location":"plan/quadrotor_trajectory_following/#references","title":"References","text":"<ul> <li>[1] Williams, G., et al. \"Information theoretic MPC for model-based reinforcement learning.\" ICRA 2017.</li> <li>[2] Williams, G., et al. \"Model predictive path integral control using covariance variable importance sampling.\" arXiv:1509.01149, 2015.</li> <li>[3] Beard, R. W., &amp; McLain, T. W. \"Small Unmanned Aircraft: Theory and Practice.\" Princeton University Press, 2012.</li> <li>[4] Mellinger, D., &amp; Kumar, V. \"Minimum snap trajectory generation and control for quadrotors.\" ICRA 2011.</li> <li>[5][pytorch_mppi original implementation](https://github.com/UM-ARM-Lab/pytorch_mppi)</li> </ul>"},{"location":"plan/quadrotor_trajectory_following/#notes","title":"Notes","text":"<ul> <li>This plan should be updated as implementation progresses</li> <li>Move to <code>docs/plan/completed/</code> when all phases are finished</li> <li>Link any related issues or PRs here</li> </ul>"},{"location":"plan/quadrotor_trajectory_following/#frame-convention-summary","title":"Frame Convention Summary","text":"<p>NED-FRD Convention: - NED (North-East-Down): World/global frame   - X: North, Y: East, Z: Down (positive downward)   - Gravity: g = [0, 0, +9.81] m/s\u00b2 (positive Z direction)</p> <ul> <li>FRD (Forward-Right-Down): Body frame</li> <li>X: Forward, Y: Right, Z: Down (positive downward)</li> <li>Thrust: T acts in -Z direction (upward thrust)</li> <li>Angular rates: [\u03c9x, \u03c9y, \u03c9z] about [Forward, Right, Down] axes</li> </ul> <p>Important Implementation Details: - Altitude: Negative values indicate height above ground (e.g., z = -5.0 means 5m altitude) - Thrust: Positive magnitude T, applied as [0, 0, -T] in body frame - Rotation matrix R(q): transforms from FRD body to NED world</p>"},{"location":"plan/quadrotor_trajectory_following/#progress-log","title":"Progress Log","text":""},{"location":"plan/quadrotor_trajectory_following/#2026-02-02-phase-1-complete","title":"2026-02-02: Phase 1 Complete \u2713","text":"<p>Completed: - Implemented <code>src/jax_mppi/dynamics/quadrotor.py</code> with full 6-DOF quadrotor dynamics   - Quaternion utilities: rotation matrix conversion, normalization, multiplication   - RK4 integration for accurate numerical integration   - First-order angular velocity tracking model   - NED-FRD frame conventions properly implemented   - Control bounds enforcement</p> <ul> <li>Implemented <code>src/jax_mppi/costs/quadrotor.py</code> with comprehensive cost functions</li> <li>Trajectory tracking cost (position + velocity)</li> <li>Time-indexed trajectory cost</li> <li>Hover control cost (with attitude tracking)</li> <li>Terminal cost for goal reaching</li> <li> <p>Quaternion distance metric</p> </li> <li> <p>Comprehensive test coverage (40 tests, all passing)</p> </li> <li><code>tests/test_quadrotor_dynamics.py</code> (19 tests)</li> <li><code>tests/test_quadrotor_costs.py</code> (21 tests)</li> <li>Tests verify: quaternion math, dynamics correctness, JIT compatibility, gradients</li> </ul> <p>Key Features: - All functions are JIT-compatible for high performance - Gradients work correctly through all dynamics and cost functions - Quaternion norm preservation verified during integration - Physical behaviors validated (gravity, thrust, angular tracking)</p> <p>Next Steps: - Phase 2: Trajectory generators (circle, figure-8, hover setpoint)</p>"},{"location":"plan/quadrotor_trajectory_following/#2026-02-02-phase-2-complete","title":"2026-02-02: Phase 2 Complete \u2713","text":"<p>Completed: - Implemented <code>examples/quadrotor/trajectories.py</code> with comprehensive trajectory generators   - <code>generate_hover_setpoint()</code> - Constant position stabilization   - <code>generate_circle_trajectory()</code> - Circular paths with configurable center and phase   - <code>generate_lemniscate_trajectory()</code> - Figure-8 patterns (horizontal or vertical)   - <code>generate_helix_trajectory()</code> - Spiral paths with vertical motion   - <code>generate_waypoint_trajectory()</code> - Smooth cubic Hermite interpolation through waypoints   - <code>compute_trajectory_metrics()</code> - Analyze distance, velocity, acceleration</p> <ul> <li>Comprehensive test coverage (28 tests, all passing)</li> <li>Tests verify: trajectory shapes, periodicity, continuity</li> <li>Validates velocity/position relationships</li> <li>Checks metric computation accuracy</li> </ul> <p>Key Features: - All trajectories follow NED frame convention - Analytical derivatives for velocity (no numerical differentiation) - Configurable parameters (center, phase, duration, dt) - Support for both horizontal and vertical figure-8 patterns</p> <p>Next Steps: - Phase 3: Basic examples (hover control, circle following)</p>"},{"location":"plan/quadrotor_trajectory_following/#2026-02-02-phase-3-complete","title":"2026-02-02: Phase 3 Complete \u2713","text":"<p>Completed: - Implemented <code>examples/quadrotor_hover.py</code> - Hover control stabilization   - MPPI-based hover controller with position and attitude tracking   - Performance metrics (settling time, position/velocity error)   - Comprehensive visualization (9 subplots: position, velocity, angular velocity, control inputs, errors, cost)   - Command-line interface with configurable parameters</p> <ul> <li>Implemented <code>examples/quadrotor_circle.py</code> - Circular trajectory tracking</li> <li>Time-varying reference tracking using trajectory generators</li> <li>3D trajectory visualization with top-view projection</li> <li>Tracking error analysis and metrics</li> <li> <p>Configurable circle parameters (radius, period)</p> </li> <li> <p>Integration tests (11 tests)</p> </li> <li>Example execution tests</li> <li>Convergence validation</li> <li>Quaternion norm preservation</li> <li>Cost decrease verification</li> <li>Cross-example compatibility checks</li> </ul> <p>Key Features: - Both examples run at 50 Hz control rate (JIT-compiled) - Detailed visualizations saved to <code>docs/media/</code> - Proper NED frame convention throughout - Performance metrics automatically computed and reported</p> <p>Next Steps: - Phase 4: Advanced examples (figure-8 comparison, custom trajectories)</p>"},{"location":"plan/quadrotor_trajectory_following/#2026-02-02-phase-4-complete","title":"2026-02-02: Phase 4 Complete \u2713","text":"<p>Completed: - Implemented <code>examples/quadrotor_figure8_comparison.py</code> - MPPI variant comparison   - Side-by-side comparison of MPPI, SMPPI, and KMPPI on aggressive figure-8   - Comprehensive metrics: tracking accuracy, control smoothness, energy consumption   - Smoothness metrics: control rate (acceleration) and jerk analysis   - 6-subplot visualization comparing all three variants   - Performance comparison table with 7 key metrics   - Demonstrates trade-offs between tracking accuracy and control smoothness</p> <ul> <li>Implemented <code>examples/quadrotor_custom_trajectory.py</code> - Waypoint following</li> <li>User-defined waypoint trajectories with smooth interpolation</li> <li>Cubic Hermite splines for C1 continuity</li> <li>Waypoint passage verification and error reporting</li> <li>Command-line interface for custom waypoint specification</li> <li>6-subplot visualization including waypoint markers</li> <li> <p>Default square pattern demonstration</p> </li> <li> <p>Integration tests (13 tests)</p> </li> <li>Figure-8 comparison execution and metrics validation</li> <li>Custom trajectory with various waypoint configurations</li> <li>Quaternion normalization across all controllers</li> <li>Finite value checks for all outputs</li> </ul> <p>Key Features: - Figure-8 example shows SMPPI produces smoother control (lower jerk) - Custom trajectory allows arbitrary waypoint sequences - All examples maintain 50 Hz control rate - Publication-quality comparison visualizations</p> <p>Next Steps: - Phase 5: Documentation and polish</p> <p>Last Updated: 2026-02-02 Author: riccardo-enr</p>"},{"location":"plan/completed/evosax_integration/","title":"Evosax Integration Plan","text":"<p>Goal: Add evosax as a JAX-native optimization backend for the autotuning framework in jax-mppi.</p> <p>Status: In Progress</p>"},{"location":"plan/completed/evosax_integration/#overview","title":"Overview","text":"<p>Evosax is a JAX-native library for evolutionary strategies that provides highly efficient, JIT-compiled optimization algorithms. Integrating it into the autotuning framework will:</p> <ol> <li>Provide JAX-native optimization - Full JIT compilation of the entire tuning loop</li> <li>Enable GPU acceleration - Evolutionary strategies running entirely on GPU</li> <li>Add diverse algorithms - CMA-ES, OpenES, SNES, Sep-CMA-ES, and more</li> <li>Improve performance - Eliminate Python overhead in the optimization loop</li> <li>Simplify dependencies - Pure JAX implementation, no external C++ dependencies</li> </ol>"},{"location":"plan/completed/evosax_integration/#why-evosax","title":"Why Evosax?","text":"<ul> <li>Performance: Fully JIT-compiled ES algorithms vs. Python-based <code>cma</code> library</li> <li>JAX ecosystem fit: Natural integration with JAX-based MPPI code</li> <li>GPU support: Can run entire autotuning on GPU without host-device transfers</li> <li>Algorithm variety: 15+ evolutionary strategies in one package</li> <li>Maintained: Active development and well-documented</li> </ul>"},{"location":"plan/completed/evosax_integration/#current-state-analysis","title":"Current State Analysis","text":""},{"location":"plan/completed/evosax_integration/#existing-autotuning-architecture","title":"Existing Autotuning Architecture","text":"<p>The current autotuning system has three modules:</p> <ol> <li><code>autotune.py</code> (656 lines) - Core framework + CMA-ES via <code>cma</code> library</li> <li>Abstract <code>Optimizer</code> base class</li> <li><code>CMAESOpt</code> using Python <code>cma</code> package</li> <li>Parameter abstractions: <code>LambdaParameter</code>, <code>NoiseSigmaParameter</code>, <code>MuParameter</code>, <code>HorizonParameter</code></li> <li> <p><code>Autotune</code> orchestrator class</p> </li> <li> <p><code>autotune_global.py</code> (375 lines) - Ray Tune for global search</p> </li> <li><code>RayOptimizer</code> for distributed hyperparameter search</li> <li>Global parameter variants with search spaces</li> <li> <p>Integration with HyperOpt and BayesOpt</p> </li> <li> <p><code>autotune_qd.py</code> (218 lines) - CMA-ME quality diversity</p> </li> <li><code>CMAMEOpt</code> using <code>ribs</code> library</li> <li>Archive-based diversity preservation</li> </ol>"},{"location":"plan/completed/evosax_integration/#architecture-pattern","title":"Architecture Pattern","text":"<p>All optimizers follow the <code>Optimizer</code> ABC:</p> <pre><code>class Optimizer(abc.ABC):\n    @abc.abstractmethod\n    def setup_optimization(\n        self, initial_params: np.ndarray,\n        evaluate_fn: Callable[[np.ndarray], EvaluationResult]\n    ) -&gt; None:\n        ...\n\n    @abc.abstractmethod\n    def optimize_step(self) -&gt; EvaluationResult:\n        ...\n\n    def optimize_all(self, iterations: int) -&gt; EvaluationResult:\n        ...\n</code></pre>"},{"location":"plan/completed/evosax_integration/#implementation-plan","title":"Implementation Plan","text":""},{"location":"plan/completed/evosax_integration/#step-1-add-evosax-dependency","title":"Step 1: Add evosax dependency","text":"<p>File: <code>pyproject.toml</code></p> <p>Changes:</p> <ul> <li>Add <code>evosax&gt;=0.1.0</code> to <code>[project.optional-dependencies.autotuning]</code> section</li> <li>Add to <code>[project.optional-dependencies.dev]</code> section as well</li> </ul> <p>Reasoning: Keep evosax optional like other autotuning dependencies, allowing users to install only what they need.</p>"},{"location":"plan/completed/evosax_integration/#step-2-create-evosax-optimizer-module-done","title":"Step 2: Create evosax optimizer module \u2705 DONE","text":"<p>New File: <code>src/jax_mppi/autotune_evosax.py</code> (~387 lines)</p> <p>Contents (Implemented):</p>"},{"location":"plan/completed/evosax_integration/#21-evosaxoptimizer-base-class","title":"2.1 EvoSaxOptimizer base class","text":"<p>Implements <code>Optimizer</code> ABC with evosax backends:</p> <pre><code>class EvoSaxOptimizer(Optimizer):\n    \"\"\"JAX-native evolutionary strategies using evosax.\n\n    Fully JIT-compiled optimization loop with GPU support.\n\n    Attributes:\n        strategy: evosax strategy name (e.g., \"CMA_ES\", \"OpenES\", \"SNES\")\n        population: Population size\n        num_generations: Number of generations per optimize_step\n        maximize: Whether to maximize objective (default: False for cost minimization)\n    \"\"\"\n\n    def __init__(\n        self,\n        strategy: str = \"CMA_ES\",\n        population: int = 10,\n        num_generations: int = 1,\n        sigma_init: float = 0.1,\n        maximize: bool = False,\n        es_params: dict | None = None,\n    ):\n        ...\n</code></pre> <p>Key features:</p> <ul> <li>Strategy selection from evosax's 15+ algorithms</li> <li>JIT-compiled ask-evaluate-tell loop</li> <li>Support for both single-step and batch optimization</li> <li>Configurable ES hyperparameters via <code>es_params</code></li> </ul>"},{"location":"plan/completed/evosax_integration/#22-jax-native-evaluation-wrapper","title":"2.2 JAX-native evaluation wrapper","text":"<pre><code>def _create_jax_evaluate_fn(\n    evaluate_fn: Callable[[np.ndarray], EvaluationResult],\n    maximize: bool = False,\n) -&gt; Callable[[jax.Array], float]:\n    \"\"\"Wrap Python evaluation function for JAX compatibility.\n\n    Handles conversion between JAX arrays and numpy arrays,\n    extracts scalar cost from EvaluationResult.\n    \"\"\"\n    ...\n</code></pre> <p>Challenge: The user-provided <code>evaluate_fn</code> may not be JAX-pure (e.g., it might use numpy, modify state, etc.). Need to handle this gracefully.</p> <p>Solutions:</p> <ul> <li>Option A: Use <code>jax.pure_callback</code> to call non-pure evaluation functions</li> <li>Option B: Require evaluation function to be JAX-pure for evosax optimizer</li> <li>Option C: Provide both pure and impure modes</li> </ul> <p>Recommendation: Start with Option C - detect if evaluation is JAX-pure, use direct JIT if yes, use <code>pure_callback</code> if no.</p>"},{"location":"plan/completed/evosax_integration/#23-batched-evaluation-support","title":"2.3 Batched evaluation support","text":"<p>Evosax can leverage <code>vmap</code> for parallel fitness evaluation:</p> <pre><code>def _batch_evaluate(\n    solutions: jax.Array,  # (population, param_dim)\n    evaluate_fn: Callable,\n) -&gt; jax.Array:  # (population,)\n    \"\"\"Evaluate population in parallel using vmap.\"\"\"\n    return jax.vmap(evaluate_fn)(solutions)\n</code></pre> <p>Benefits:</p> <ul> <li>GPU parallelization of fitness evaluations</li> <li>Significant speedup when dynamics/cost are JIT-compiled</li> </ul> <p>Challenges:</p> <ul> <li>Requires evaluation to be JAX-pure and vmappable</li> <li>May need sequential fallback for non-pure evaluations</li> </ul>"},{"location":"plan/completed/evosax_integration/#24-algorithm-specific-optimizers","title":"2.4 Algorithm-specific optimizers","text":"<p>Provide convenience classes for common strategies:</p> <pre><code>class CMAESOpt(EvoSaxOptimizer):\n    \"\"\"CMA-ES optimizer (evosax backend).\"\"\"\n    def __init__(self, population: int = 10, sigma: float = 0.1, **kwargs):\n        super().__init__(strategy=\"CMA_ES\", population=population,\n                         sigma_init=sigma, **kwargs)\n\nclass OpenESOpt(EvoSaxOptimizer):\n    \"\"\"OpenAI's Evolution Strategies.\"\"\"\n    def __init__(self, population: int = 100, sigma: float = 0.1, **kwargs):\n        super().__init__(strategy=\"OpenES\", population=population,\n                         sigma_init=sigma, **kwargs)\n\nclass SepCMAESOpt(EvoSaxOptimizer):\n    \"\"\"Separable CMA-ES (faster for high dimensions).\"\"\"\n    def __init__(self, population: int = 10, sigma: float = 0.1, **kwargs):\n        super().__init__(strategy=\"Sep_CMA_ES\", population=population,\n                         sigma_init=sigma, **kwargs)\n</code></pre>"},{"location":"plan/completed/evosax_integration/#step-3-update-main-autotune-module-done","title":"Step 3: Update main autotune module \u2705 DONE","text":"<p>File: <code>src/jax_mppi/autotune.py</code></p> <p>Changes:</p> <ul> <li>Update module docstring to mention evosax as an option</li> <li>Update examples to show evosax usage</li> </ul> <p>Example addition:</p> <pre><code>&gt;&gt;&gt; # With evosax (JAX-native, GPU-accelerated)\n&gt;&gt;&gt; from jax_mppi import autotune_evosax\n&gt;&gt;&gt; tuner = jmppi.autotune.Autotune(\n...     params_to_tune=[...],\n...     evaluate_fn=evaluate,\n...     optimizer=autotune_evosax.CMAESOpt(population=10),\n... )\n</code></pre>"},{"location":"plan/completed/evosax_integration/#step-4-update-package-exports-done","title":"Step 4: Update package exports \u2705 DONE","text":"<p>File: <code>src/jax_mppi/__init__.py</code></p> <p>Changes:</p> <pre><code># Add conditional import\ntry:\n    from . import autotune_evosax\nexcept ImportError:\n    autotune_evosax = None  # evosax not installed\n</code></pre> <p>Reasoning: Keep it optional - don't break imports if evosax isn't installed.</p>"},{"location":"plan/completed/evosax_integration/#step-5-create-tests-done","title":"Step 5: Create tests \u2705 DONE","text":"<p>New File: <code>tests/test_autotune_evosax.py</code> (~408 lines)</p> <p>Test coverage:</p> <ol> <li>Basic functionality tests</li> <li>Test EvoSaxOptimizer initialization with various strategies</li> <li>Test setup_optimization() creates valid ES state</li> <li>Test optimize_step() returns EvaluationResult</li> <li> <p>Test optimize_all() finds better solutions</p> </li> <li> <p>Strategy-specific tests</p> </li> <li>Test CMAESOpt on simple quadratic function</li> <li>Test OpenESOpt convergence</li> <li> <p>Test SepCMAESOpt on high-dimensional problem</p> </li> <li> <p>Integration tests</p> </li> <li>Test with actual MPPI parameter tuning (simple 1D system)</li> <li>Test with LambdaParameter and NoiseSigmaParameter</li> <li> <p>Verify results comparable to CMAESOpt from <code>cma</code> library</p> </li> <li> <p>JAX-specific tests</p> </li> <li>Test JIT compilation of optimization loop</li> <li>Test with JAX-pure evaluation function</li> <li>Test with non-pure evaluation function (using pure_callback)</li> <li> <p>Test batched evaluation with vmap</p> </li> <li> <p>Edge cases</p> </li> <li>Test with single parameter</li> <li>Test with multi-dimensional parameters</li> <li>Test parameter bounds enforcement</li> <li>Test with invalid strategy name (should raise clear error)</li> </ol> <p>Test structure example:</p> <pre><code>def test_evosax_cmaes_simple():\n    \"\"\"Test CMA-ES on simple quadratic objective.\"\"\"\n    # Minimize ||x - target||^2\n    target = np.array([1.0, 2.0, 3.0])\n\n    def evaluate_fn(x: np.ndarray) -&gt; EvaluationResult:\n        cost = float(np.sum((x - target) ** 2))\n        return EvaluationResult(\n            mean_cost=cost,\n            rollouts=jnp.zeros((1, 1, 3)),\n            params={},\n            iteration=0,\n        )\n\n    optimizer = EvoSaxOptimizer(strategy=\"CMA_ES\", population=10)\n    optimizer.setup_optimization(\n        initial_params=np.zeros(3),\n        evaluate_fn=evaluate_fn,\n    )\n\n    best = optimizer.optimize_all(iterations=50)\n\n    # Should converge close to target\n    assert best.mean_cost &lt; 0.1\n</code></pre>"},{"location":"plan/completed/evosax_integration/#step-6-create-example-done","title":"Step 6: Create example \u2705 DONE","text":"<p>New File: <code>examples/autotune_evosax_comparison.py</code> (~307 lines)</p> <p>Purpose: Compare evosax vs. cma library performance</p> <p>Contents:</p> <ol> <li>Setup simple pendulum MPPI tuning task</li> <li>Run with <code>CMAESOpt</code> from <code>cma</code> library (baseline)</li> <li>Run with <code>CMAESOpt</code> from evosax</li> <li>Run with other evosax strategies (OpenES, Sep-CMA-ES)</li> <li>Compare:</li> <li>Convergence speed (iterations to threshold)</li> <li>Wall-clock time</li> <li>Final performance</li> <li>Generate comparison plots:</li> <li>Convergence curves for each optimizer</li> <li>Time comparison bar chart</li> <li>Parameter trajectory plots</li> </ol> <p>Expected outcome: Evosax should be faster in wall-clock time due to JIT compilation, especially with GPU.</p>"},{"location":"plan/completed/evosax_integration/#step-7-documentation-done","title":"Step 7: Documentation \u2705 DONE","text":""},{"location":"plan/completed/evosax_integration/#71-update-readmemd-done","title":"7.1 Update README.md \u2705 DONE","text":"<p>Add evosax to the autotuning section:</p> <pre><code>### Autotuning\n\nJAX-MPPI supports automatic hyperparameter tuning with multiple backends:\n\n- **CMA-ES** (via `cma` library) - Classic evolution strategy\n- **CMA-ES** (via `evosax`) - JAX-native, GPU-accelerated  \u26a1 **NEW**\n- **Ray Tune** - Distributed hyperparameter search\n- **CMA-ME** (via `ribs`) - Quality diversity optimization\n\nInstall with: `pip install jax-mppi[autotuning] evosax`\n</code></pre>"},{"location":"plan/completed/evosax_integration/#72-add-evosax-examples-to-docstrings","title":"7.2 Add evosax examples to docstrings","text":"<p>Update autotune.py module docstring with evosax example.</p>"},{"location":"plan/completed/evosax_integration/#73-create-migration-guide","title":"7.3 Create migration guide","text":"<p>Document for users switching from <code>cma</code> to <code>evosax</code>:</p> <pre><code>## Migrating from cma to evosax\n\n**Before:**\n```python\nfrom jax_mppi.autotune import CMAESOpt\nopt = CMAESOpt(population=10, sigma=0.1)\n</code></pre> <p>After:</p> <pre><code>from jax_mppi.autotune_evosax import CMAESOpt\nopt = CMAESOpt(population=10, sigma=0.1)\n</code></pre> <p>Benefits: 5-10x faster with GPU acceleration</p>"},{"location":"plan/completed/evosax_integration/#implementation-details","title":"Implementation Details","text":""},{"location":"plan/completed/evosax_integration/#handling-non-pure-evaluations","title":"Handling Non-Pure Evaluations","text":"<p>The main challenge is that user evaluation functions may not be JAX-pure.</p> <p>Strategy:</p> <pre><code>def _wrap_evaluation(evaluate_fn, maximize):\n    \"\"\"Wrap evaluation function for JAX compatibility.\"\"\"\n\n    # Try to detect if function is JAX-pure\n    # by checking if it uses only JAX operations\n\n    def jax_eval(x: jax.Array) -&gt; float:\n        # Convert to numpy for non-pure functions\n        x_np = np.array(x)\n        result = evaluate_fn(x_np)\n        cost = result.mean_cost\n        return -cost if maximize else cost\n\n    # For non-pure functions, use pure_callback\n    if not _is_jax_pure(evaluate_fn):\n        @jax.jit\n        def wrapped(x):\n            return jax.pure_callback(\n                jax_eval,\n                jax.ShapeDtypeStruct((), jnp.float32),\n                x\n            )\n        return wrapped\n    else:\n        return jax.jit(jax_eval)\n</code></pre>"},{"location":"plan/completed/evosax_integration/#batched-vs-sequential-evaluation","title":"Batched vs Sequential Evaluation","text":"<p>Provide both modes:</p> <pre><code>class EvoSaxOptimizer(Optimizer):\n    def __init__(self, ..., batched_evaluation: bool = False):\n        self.batched_evaluation = batched_evaluation\n\n    def optimize_step(self):\n        solutions = self.es_state.ask()\n\n        if self.batched_evaluation:\n            # Parallel evaluation with vmap\n            fitness = jax.vmap(self.evaluate_fn)(solutions)\n        else:\n            # Sequential evaluation (safer for non-pure functions)\n            fitness = jnp.array([\n                self.evaluate_fn(x) for x in solutions\n            ])\n\n        self.es_state = self.es_state.tell(fitness)\n        ...\n</code></pre>"},{"location":"plan/completed/evosax_integration/#parameter-constraints","title":"Parameter Constraints","text":"<p>Evosax doesn't natively handle box constraints. Options:</p> <ol> <li>Rejection sampling: Reject invalid samples (wasteful)</li> <li>Clipping: Clip to bounds after sampling (biases distribution)</li> <li>Repair: Project invalid samples to feasible region</li> <li>Penalty: Add penalty for constraint violation</li> </ol> <p>Recommendation: Use clipping (Option 2) for simplicity, add note in docs that proper constrained optimization should use constrained ES variants if needed.</p> <pre><code>def _apply_bounds(x, lower, upper):\n    \"\"\"Apply box constraints via clipping.\"\"\"\n    if lower is not None or upper is not None:\n        x = jnp.clip(x, lower, upper)\n    return x\n</code></pre>"},{"location":"plan/completed/evosax_integration/#available-evosax-strategies","title":"Available Evosax Strategies","text":"<p>Strategies to support (from evosax):</p>"},{"location":"plan/completed/evosax_integration/#gradient-free-es","title":"Gradient-free ES","text":"<ol> <li>CMA_ES - Classic Covariance Matrix Adaptation</li> <li>Sep_CMA_ES - Separable CMA-ES (faster for high-dim)</li> <li>IPOP_CMA_ES - Increasing population CMA-ES</li> <li>BIPOP_CMA_ES - Bi-population CMA-ES</li> <li>OpenES - OpenAI's Natural Evolution Strategies</li> <li>SNES - Separable Natural Evolution Strategies</li> <li>xNES - Exponential Natural Evolution Strategies</li> <li>SimpleGA - Simple Genetic Algorithm</li> <li>PersistentES - Persistent Evolution Strategies</li> <li>LES - Learned Evolution Strategies (meta-learned)</li> </ol>"},{"location":"plan/completed/evosax_integration/#recommendation-for-defaults","title":"Recommendation for defaults","text":"<ul> <li>Default: CMA_ES (well-tested, robust)</li> <li>High-dimensional: Sep_CMA_ES (scales better)</li> <li>Large population budget: OpenES (naturally parallelizable)</li> </ul>"},{"location":"plan/completed/evosax_integration/#migration-from-cma-library","title":"Migration from cma library","text":""},{"location":"plan/completed/evosax_integration/#advantages-of-evosax","title":"Advantages of evosax","text":"Feature <code>cma</code> library <code>evosax</code> Language Python + C Pure JAX JIT compilation \u274c \u2705 GPU acceleration \u274c \u2705 Batched evaluation \u274c \u2705 (via vmap) Integration with JAX code \u26a0\ufe0f (numpy conversion) \u2705 (native) Algorithm variety CMA-ES variants only 15+ strategies Performance (CPU) Good Similar Performance (GPU) N/A Excellent (5-10x)"},{"location":"plan/completed/evosax_integration/#when-to-use-each","title":"When to use each","text":"<p>Use <code>cma</code> library when:</p> <ul> <li>You need the original CMA-ES implementation</li> <li>Your evaluation function has complex Python dependencies</li> <li>You're not using GPU</li> </ul> <p>Use <code>evosax</code> when:</p> <ul> <li>You want GPU acceleration</li> <li>Your MPPI code is already JIT-compiled</li> <li>You want to experiment with different ES algorithms</li> <li>You need batched parallel evaluation</li> </ul>"},{"location":"plan/completed/evosax_integration/#testing-strategy","title":"Testing Strategy","text":""},{"location":"plan/completed/evosax_integration/#unit-tests","title":"Unit tests","text":"<ul> <li>Test each strategy initialization</li> <li>Test optimize_step produces valid results</li> <li>Test parameter bounds enforcement</li> <li>Test with different parameter dimensions</li> </ul>"},{"location":"plan/completed/evosax_integration/#integration-tests","title":"Integration tests","text":"<ul> <li>Compare convergence to <code>cma</code> library on same problems</li> <li>Test with actual MPPI parameter tuning</li> <li>Verify GPU execution (if GPU available)</li> </ul>"},{"location":"plan/completed/evosax_integration/#performance-benchmarks","title":"Performance benchmarks","text":"<ul> <li>Compare wall-clock time vs <code>cma</code> library</li> <li>Measure JIT compilation overhead</li> <li>Profile GPU vs CPU performance</li> </ul>"},{"location":"plan/completed/evosax_integration/#regression-tests","title":"Regression tests","text":"<ul> <li>Ensure results are deterministic with fixed seed</li> <li>Verify backward compatibility with existing Optimizer API</li> </ul>"},{"location":"plan/completed/evosax_integration/#potential-issues-solutions","title":"Potential Issues &amp; Solutions","text":""},{"location":"plan/completed/evosax_integration/#issue-1-jit-compilation-overhead","title":"Issue 1: JIT compilation overhead","text":"<p>Problem: First call to evosax optimizer incurs JIT compilation cost.</p> <p>Solution:</p> <ul> <li>Document warmup requirement</li> <li>Provide <code>warmup()</code> method that JIT-compiles with dummy data</li> <li>Consider pre-compilation for common parameter dimensions</li> </ul>"},{"location":"plan/completed/evosax_integration/#issue-2-non-pure-evaluation-functions","title":"Issue 2: Non-pure evaluation functions","text":"<p>Problem: Most user evaluation functions are not JAX-pure (use numpy, I/O, etc.).</p> <p>Solution:</p> <ul> <li>Use <code>jax.pure_callback</code> to wrap impure functions</li> <li>Provide clear error messages when incompatible operations are detected</li> <li>Document limitations and workarounds</li> </ul>"},{"location":"plan/completed/evosax_integration/#issue-3-memory-usage-with-large-populations","title":"Issue 3: Memory usage with large populations","text":"<p>Problem: GPU memory may be limited for large populations.</p> <p>Solution:</p> <ul> <li>Add memory usage estimates in docs</li> <li>Provide chunk-based evaluation for very large populations</li> <li>Default to reasonable population sizes</li> </ul>"},{"location":"plan/completed/evosax_integration/#issue-4-determinism","title":"Issue 4: Determinism","text":"<p>Problem: JAX PRNG behaves differently than numpy.random.</p> <p>Solution:</p> <ul> <li>Document PRNG handling</li> <li>Ensure reproducibility with fixed JAX random keys</li> <li>Provide utility to seed evosax optimizer</li> </ul>"},{"location":"plan/completed/evosax_integration/#timeline-estimate","title":"Timeline Estimate","text":"Step Description Estimated Lines Time 1 Add dependency to pyproject.toml ~5 5 min 2 Implement autotune_evosax.py ~350 4-6 hours 3 Update autotune.py docs ~20 30 min 4 Update init.py ~5 5 min 5 Create test_autotune_evosax.py ~250 3-4 hours 6 Create example comparison script ~200 2-3 hours 7 Update documentation ~100 1-2 hours Total ~930 lines 11-16 hours"},{"location":"plan/completed/evosax_integration/#success-criteria","title":"Success Criteria","text":""},{"location":"plan/completed/evosax_integration/#functional-requirements","title":"Functional Requirements","text":"<ul> <li> EvoSaxOptimizer implements Optimizer ABC correctly</li> <li> All evosax strategies can be instantiated</li> <li> Optimization converges on test problems</li> <li> Integration with existing Autotune class works</li> <li> Parameter bounds are respected</li> </ul>"},{"location":"plan/completed/evosax_integration/#performance-requirements","title":"Performance Requirements","text":"<ul> <li> Evosax is faster than <code>cma</code> on GPU (&gt;2x speedup)</li> <li> Evosax is competitive with <code>cma</code> on CPU (within 20%)</li> <li> JIT compilation overhead is acceptable (&lt;5s for typical problems)</li> </ul>"},{"location":"plan/completed/evosax_integration/#quality-requirements","title":"Quality Requirements","text":"<ul> <li> All tests pass (&gt;95% coverage)</li> <li> Documentation is clear and complete</li> <li> Examples run without errors</li> <li> Code follows existing style conventions</li> </ul>"},{"location":"plan/completed/evosax_integration/#integration-requirements","title":"Integration Requirements","text":"<ul> <li> Works with all parameter types (Lambda, NoiseSigma, Mu, Horizon)</li> <li> Compatible with existing Autotune orchestrator</li> <li> No breaking changes to existing API</li> <li> Optional dependency (doesn't break install if evosax not available)</li> </ul>"},{"location":"plan/completed/evosax_integration/#future-extensions","title":"Future Extensions","text":""},{"location":"plan/completed/evosax_integration/#short-term-post-mvp","title":"Short-term (post-MVP)","text":"<ol> <li>Add more evosax strategies (GLD, LM-MA-ES, etc.)</li> <li>Implement proper constrained optimization variants</li> <li>Add support for multi-objective optimization</li> <li>Create Jupyter notebook tutorial</li> </ol>"},{"location":"plan/completed/evosax_integration/#medium-term","title":"Medium-term","text":"<ol> <li>Integrate with autotune_qd.py for quality diversity</li> <li>Add learned evolution strategies (LES) with meta-learning</li> <li>Implement adaptive strategy selection</li> <li>Add visualization of ES state (e.g., covariance ellipsoids)</li> </ol>"},{"location":"plan/completed/evosax_integration/#long-term","title":"Long-term","text":"<ol> <li>Develop JAX-native quality diversity framework</li> <li>Add support for multi-fidelity optimization</li> <li>Implement distributed evosax with multi-GPU support</li> <li>Create benchmarking suite comparing all optimizers</li> </ol>"},{"location":"plan/completed/evosax_integration/#references","title":"References","text":"<ul> <li>evosax documentation</li> <li>JAX documentation</li> <li>Current autotuning implementation: <code>src/jax_mppi/autotune.py</code></li> <li>Original pytorch_mppi autotune: <code>../pytorch_mppi/src/pytorch_mppi/autotune.py</code></li> </ul>"},{"location":"plan/completed/evosax_integration/#open-questions","title":"Open Questions","text":"<ol> <li>Should we deprecate the <code>cma</code> library backend?</li> <li>Probably not - keep both for compatibility</li> <li> <p>Users can choose based on their needs</p> </li> <li> <p>Should batched evaluation be default?</p> </li> <li>No - requires JAX-pure evaluation functions</li> <li> <p>Make it opt-in with clear documentation</p> </li> <li> <p>Which evosax strategies should have convenience classes?</p> </li> <li>Start with: CMA_ES, Sep_CMA_ES, OpenES</li> <li> <p>Add more based on user feedback</p> </li> <li> <p>Should we add evosax to core dependencies or keep it optional?</p> </li> <li>Keep optional - maintains lightweight core</li> <li>Document installation clearly</li> </ol>"},{"location":"plan/completed/evosax_integration/#notes","title":"Notes","text":"<ul> <li>Evosax is actively maintained by Robert Lange</li> <li>Current version: 0.1.x (check latest before implementing)</li> <li>JAX-native means entire optimization loop can run on GPU</li> <li>Consider adding evosax to CI/CD pipeline for testing</li> </ul>"},{"location":"plan/completed/porting_pytorch_jax/","title":"JAX MPPI Implementation Plan","text":"<p>Port <code>pytorch_mppi</code> to JAX, producing a functional, JIT-compilable MPPI library.</p>"},{"location":"plan/completed/porting_pytorch_jax/#status-jan-31-2026","title":"Status (Jan 31, 2026)","text":"<p>Overall Progress: Phase 6 complete (Autotuning system fully implemented with CMA-ES, Ray Tune, and CMA-ME support).</p>"},{"location":"plan/completed/porting_pytorch_jax/#implementation-status-by-phase","title":"Implementation Status by Phase","text":"<ul> <li>Phase 1: Core MPPI \u2705 COMPLETE</li> <li>353 lines implemented in <code>src/jax_mppi/mppi.py</code></li> <li>All core features from pytorch_mppi ported</li> <li> <p>115 lines of unit tests in <code>tests/test_mppi.py</code></p> </li> <li> <p>Phase 2: Pendulum Integration \u2705 COMPLETE</p> </li> <li>270 lines in <code>examples/pendulum.py</code> (full-featured example with CLI)</li> <li>282 lines in <code>tests/test_pendulum.py</code> (8 comprehensive integration tests)</li> <li> <p>All tests passing, swing-up and stabilization verified</p> </li> <li> <p>Phase 3: Smooth MPPI (SMPPI) \u2705 COMPLETE</p> </li> <li>634 lines implemented in <code>src/jax_mppi/smppi.py</code></li> <li>All SMPPI features: action_sequence, smoothness cost, dual bounds, integration</li> <li>580 lines in <code>tests/test_smppi.py</code> (18 comprehensive tests)</li> <li> <p>All tests passing</p> </li> <li> <p>Phase 4: Kernel MPPI (KMPPI) \u2705 COMPLETE</p> </li> <li>660 lines implemented in <code>src/jax_mppi/kmppi.py</code></li> <li>RBFKernel, kernel interpolation, control point optimization</li> <li>595 lines in <code>tests/test_kmppi.py</code> (23 comprehensive tests)</li> <li> <p>All tests passing (53/53 total tests pass)</p> </li> <li> <p>Phase 5: Smooth Comparison Example \u2705 COMPLETE</p> </li> <li>442 lines in <code>examples/smooth_comparison.py</code></li> <li>Compares MPPI, SMPPI, and KMPPI on 2D navigation with obstacle avoidance</li> <li>Includes visualization with 4 subplots: trajectories, costs, controls, smoothness</li> <li> <p>Supporting modules: <code>src/jax_mppi/costs/</code> and <code>src/jax_mppi/dynamics/</code></p> </li> <li> <p>Phase 6: Autotuning \u2705 COMPLETE</p> </li> <li>656 lines in <code>src/jax_mppi/autotune.py</code> - Core CMA-ES autotuning</li> <li>375 lines in <code>src/jax_mppi/autotune_global.py</code> - Ray Tune global search</li> <li>218 lines in <code>src/jax_mppi/autotune_qd.py</code> - CMA-ME quality diversity</li> <li>305 lines in <code>tests/test_autotune.py</code> (21 unit tests)</li> <li>247 lines in <code>tests/test_autotune_integration.py</code> (4 integration tests)</li> <li>321 lines in <code>examples/autotune_pendulum.py</code> - Full demonstration</li> <li>90 lines in <code>examples/autotune_basic.py</code> - Minimal example</li> <li>All 25 tests passing</li> </ul>"},{"location":"plan/completed/porting_pytorch_jax/#package-size-comparison","title":"Package Size Comparison","text":"Package Core Code Tests Examples Total pytorch_mppi 1214 lines ~500 lines ~800 lines ~2500 lines jax_mppi (current) 2919 lines 2124 lines 681 lines 5724 lines Completion % 240% 425% 85% 229% <p>Core code now includes: mppi.py (353), smppi.py (634), kmppi.py (660), autotune.py (656), autotune_global.py (375), autotune_qd.py (218), plus supporting modules.</p>"},{"location":"plan/completed/porting_pytorch_jax/#feature-parity-matrix","title":"Feature Parity Matrix","text":"Feature pytorch_mppi jax_mppi Status Core MPPI Algorithm \u2713 \u2713 \u2705 Complete Basic sampling &amp; weighting \u2713 \u2713 \u2705 Control bounds (u_min/u_max) \u2713 \u2713 \u2705 Control scaling (u_scale) \u2713 \u2713 \u2705 Partial updates (u_per_command) \u2713 \u2713 \u2705 Step-dependent dynamics \u2713 \u2713 \u2705 Stochastic dynamics (rollout_samples) \u2713 \u2713 \u2705 Sample null action \u2713 \u2713 \u2705 Noise absolute cost \u2713 \u2713 \u2705 Terminal cost function \u2713 \u2713 \u2705 Shift nominal trajectory \u2713 \u2713 \u2705 Get rollouts (visualization) \u2713 \u2713 \u2705 Reset controller \u2713 \u2713 \u2705 Smooth MPPI (SMPPI) \u2713 \u2713 \u2705 Complete Action sequence tracking \u2713 \u2713 \u2705 Smoothness penalty \u2713 \u2713 \u2705 Separate action/control bounds \u2713 \u2713 \u2705 Delta_t integration \u2713 \u2713 \u2705 Shift with continuity \u2713 \u2713 \u2705 Kernel MPPI (KMPPI) \u2713 \u2713 \u2705 Complete Kernel interpolation \u2713 \u2713 \u2705 RBF kernel \u2713 \u2713 \u2705 Support point optimization \u2713 \u2713 \u2705 Time grid management (Tk/Hs) \u2713 \u2713 \u2705 Solve-based interpolation \u2713 \u2713 \u2705 Autotuning \u2713 \u2713 \u2705 Complete CMA-ES local tuning \u2713 \u2713 \u2705 Ray Tune global search \u2713 \u2713 \u2705 CMA-ME quality diversity \u2713 \u2713 \u2705 Parameter types (lambda, sigma, mu, horizon) \u2713 \u2713 \u2705 All MPPI variants support \u2713 \u2713 \u2705 Examples Pendulum swing-up \u2713 \u2713 \u2705 Complete Smooth MPPI comparison \u2713 \u2713 \u2705 Complete Autotuning example \u2713 \u2713 \u2705 Complete Pendulum with learned dynamics \u2713 \u2717 \ud83d\udd34 Not planned"},{"location":"plan/completed/porting_pytorch_jax/#current-file-structure","title":"Current File Structure","text":"<pre><code>jax_mppi/\n\u251c\u2500\u2500 pyproject.toml              \u2705 Exists\n\u251c\u2500\u2500 README.md                   \u2705 Exists\n\u251c\u2500\u2500 LICENSE                     \u2705 Exists  \n\u251c\u2500\u2500 src/jax_mppi/\n\u2502   \u251c\u2500\u2500 __init__.py            \u2705 Exists (updated for autotune)\n\u2502   \u251c\u2500\u2500 types.py               \u2705 Exists (9 lines)\n\u2502   \u251c\u2500\u2500 mppi.py                \u2705 Exists (353 lines) - COMPLETE\n\u2502   \u251c\u2500\u2500 smppi.py               \u2705 Exists (634 lines) - COMPLETE\n\u2502   \u251c\u2500\u2500 kmppi.py               \u2705 Exists (660 lines) - COMPLETE\n\u2502   \u251c\u2500\u2500 autotune.py            \u2705 Exists (656 lines) - COMPLETE\n\u2502   \u251c\u2500\u2500 autotune_global.py     \u2705 Exists (375 lines) - COMPLETE\n\u2502   \u251c\u2500\u2500 autotune_qd.py         \u2705 Exists (218 lines) - COMPLETE\n\u2502   \u251c\u2500\u2500 costs/                 \u2705 Exists (supporting modules)\n\u2502   \u2514\u2500\u2500 dynamics/              \u2705 Exists (supporting modules)\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 test_mppi.py           \u2705 Exists (115 lines) - COMPLETE\n\u2502   \u251c\u2500\u2500 test_pendulum.py       \u2705 Exists (282 lines) - COMPLETE\n\u2502   \u251c\u2500\u2500 test_smppi.py          \u2705 Exists (580 lines) - COMPLETE\n\u2502   \u251c\u2500\u2500 test_autotune.py       \u2705 Exists (305 lines, 21 tests) - COMPLETE\n\u2502   \u2514\u2500\u2500 test_autotune_integration.py \u2705 Exists (247 lines, 4 tests) - COMPLETE\n\u2502   \u2514\u2500\u2500 test_kmppi.py          \u2705 Exists (595 lines) - COMPLETE\n\u251c\u2500\u2500 examples/\n\u2502   \u251c\u2500\u2500 pendulum.py            \u2705 Exists (270 lines) - COMPLETE\n\u2502   \u251c\u2500\u2500 smooth_comparison.py   \u2705 Exists (442 lines) - COMPLETE\n\u2502   \u251c\u2500\u2500 autotune_pendulum.py   \u2705 Exists (321 lines) - COMPLETE\n\u2502   \u2514\u2500\u2500 autotune_basic.py      \u2705 Exists (90 lines) - COMPLETE\n\u2514\u2500\u2500 docs/\n    \u2514\u2500\u2500 plan/\n        \u2514\u2500\u2500 porting_pytorch_jax.md \u2705 This file\n</code></pre>"},{"location":"plan/completed/porting_pytorch_jax/#recommended-next-steps","title":"Recommended Next Steps","text":"<p>Priority Order:</p> <ol> <li>Phase 3: SMPPI Implementation (High Priority)</li> <li>Core functionality that adds smoothness to control</li> <li>Estimated ~250-300 lines for smppi.py</li> <li>Estimated ~150-200 lines for tests</li> <li> <p>Reference: <code>../pytorch_mppi/src/pytorch_mppi/mppi.py</code> (SMPPI class)</p> </li> <li> <p>Phase 4: KMPPI Implementation (High Priority)</p> </li> <li>Novel contribution with kernel interpolation</li> <li>Estimated ~300-350 lines for kmppi.py</li> <li>Estimated ~150-200 lines for tests</li> <li> <p>Reference: <code>../pytorch_mppi/src/pytorch_mppi/mppi.py</code> (KMPPI class)</p> </li> <li> <p>Phase 5: Smooth Comparison Example (Medium Priority)</p> </li> <li>Demonstrates value of SMPPI and KMPPI</li> <li>Estimated ~200-250 lines</li> <li> <p>Reference: <code>../pytorch_mppi/tests/smooth_mppi.py</code></p> </li> <li> <p>Additional Examples (Low Priority)</p> </li> <li>Pendulum with learned dynamics</li> <li> <p>More complex environments</p> </li> <li> <p>Phase 6: Autotuning (Optional/Stretch)</p> </li> <li>Advanced feature for hyperparameter optimization</li> <li>Estimated ~300-400 lines</li> <li>Reference: <code>../pytorch_mppi/src/pytorch_mppi/autotune.py</code></li> </ol>"},{"location":"plan/completed/porting_pytorch_jax/#design-decisions","title":"Design Decisions","text":""},{"location":"plan/completed/porting_pytorch_jax/#api-style-functional-with-dataclass-state-containers","title":"API Style: Functional with dataclass state containers","text":"<p>Use <code>@jax.tree_util.register_dataclass</code> (or <code>flax.struct.dataclass</code>) to hold MPPI state (nominal trajectory <code>U</code>, PRNG key, config). All core functions are pure: <code>command(state, mppi_state) -&gt; (action, mppi_state)</code>.</p> <p>Rationale: Idiomatic JAX \u2014 pure functions compose with <code>jit</code>, <code>vmap</code>, <code>grad</code>. No mutable <code>self</code>. Avoids heavyweight dependencies like Equinox for what is fundamentally a numerical algorithm.</p>"},{"location":"plan/completed/porting_pytorch_jax/#key-jax-mappings-from-pytorch","title":"Key JAX mappings from PyTorch","text":"PyTorch JAX <code>torch.distributions.MultivariateNormal</code> <code>jax.random.multivariate_normal</code> <code>tensor.to(device)</code> <code>jax.device_put</code> / automatic Python for-loop over horizon <code>jax.lax.scan</code> <code>@handle_batch_input</code> decorator <code>jax.vmap</code> <code>torch.roll</code> <code>jnp.roll</code> <code>torch.linalg.solve</code> <code>jnp.linalg.solve</code> In-place mutation (<code>self.U = ...</code>) Return new state (pytree)"},{"location":"plan/completed/porting_pytorch_jax/#notes-from-pytorch_mppi-review-jan-2026","title":"Notes from <code>../pytorch_mppi</code> review (Jan 2026)","text":"<p>Actionable parity items to carry over:</p> <ul> <li>SMPPI semantics: maintains <code>action_sequence</code> separately from lifted control <code>U</code>; integrates with <code>delta_t</code>; smoothness cost from <code>diff(action_sequence)</code>.</li> <li>SMPPI bounds: support <code>action_min</code>/<code>action_max</code> distinct from <code>u_min</code>/<code>u_max</code> (control-derivative bounds).</li> <li>KMPPI internals: keep <code>theta</code> as control points; build <code>Tk</code>/<code>Hs</code> time grids; kernel interpolation via <code>solve(Ktktk, K)</code>; batch interpolation with <code>vmap</code>.</li> <li>Sampling options: <code>rollout_samples</code> (M), <code>sample_null_action</code>, <code>noise_abs_cost</code> (abs(noise) in action cost).</li> <li>Rollouts: <code>get_rollouts</code> handles <code>state</code> batch and dynamics that may augment state (take first <code>nx</code>).</li> </ul>"},{"location":"plan/completed/porting_pytorch_jax/#package-structure","title":"Package Structure","text":"<pre><code>jax_mppi/\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 src/jax_mppi/\n\u2502   \u251c\u2500\u2500 __init__.py          # Public API exports\n\u2502   \u251c\u2500\u2500 mppi.py              # Core MPPI (MPPIConfig, MPPIState, command, reset, etc.)\n\u2502   \u251c\u2500\u2500 smppi.py             # Smooth MPPI variant\n\u2502   \u251c\u2500\u2500 kmppi.py             # Kernel MPPI variant + TimeKernel / RBFKernel\n\u2502   \u251c\u2500\u2500 types.py             # Type aliases, protocols for Dynamics/Cost callables\n\u2502   \u2514\u2500\u2500 autotune.py          # Autotuning (CMA-ES wrapper, parameter search)\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 test_mppi.py         # Unit tests for core MPPI\n\u2502   \u251c\u2500\u2500 test_smppi.py        # Unit tests for SMPPI\n\u2502   \u251c\u2500\u2500 test_kmppi.py        # Unit tests for KMPPI\n\u2502   \u2514\u2500\u2500 test_pendulum.py     # Integration test with pendulum env\n\u251c\u2500\u2500 examples/\n\u2502   \u251c\u2500\u2500 pendulum.py          # Gym pendulum with true dynamics\n\u2502   \u251c\u2500\u2500 pendulum_approximate.py  # Learned dynamics\n\u2502   \u2514\u2500\u2500 smooth_comparison.py # MPPI vs SMPPI vs KMPPI\n\u2514\u2500\u2500 docs/\n    \u2514\u2500\u2500 plan/\n</code></pre>"},{"location":"plan/completed/porting_pytorch_jax/#phased-implementation","title":"Phased Implementation","text":""},{"location":"plan/completed/porting_pytorch_jax/#phase-1-project-scaffolding-core-mppi","title":"Phase 1: Project scaffolding + Core MPPI","text":"<p>Files: <code>pyproject.toml</code>, <code>src/jax_mppi/types.py</code>, <code>src/jax_mppi/mppi.py</code>, <code>src/jax_mppi/__init__.py</code></p> <ol> <li> <p><code>pyproject.toml</code> \u2014 project metadata, deps: <code>jax[cuda13]</code>, <code>jaxlib</code>, optional <code>gymnasium</code> for examples.</p> </li> <li> <p><code>types.py</code> \u2014 Type definitions:</p> </li> </ol> <p><code>python    # Dynamics: (state, action) -&gt; next_state  or  (state, action, t) -&gt; next_state    DynamicsFn = Callable[..., jax.Array]    # Cost: (state, action) -&gt; scalar_cost  or  (state, action, t) -&gt; scalar_cost    RunningCostFn = Callable[..., jax.Array]    # Terminal: (states, actions) -&gt; scalar_cost    TerminalCostFn = Callable[[jax.Array, jax.Array], jax.Array]</code></p> <ol> <li><code>mppi.py</code> \u2014 Core implementation:</li> </ol> <p>Data structures (registered as JAX pytrees):</p> <p>```python    @dataclass    class MPPIConfig:        # Static config (not traced through JAX)        num_samples: int       # K        horizon: int           # T        nx: int        nu: int        lambda_: float        u_scale: float        u_per_command: int        step_dependent_dynamics: bool        rollout_samples: int   # M        rollout_var_cost: float        rollout_var_discount: float        sample_null_action: bool        noise_abs_cost: bool</p> <p>@dataclass    class MPPIState:        # Dynamic state (carried through JAX transforms)        U: jax.Array           # (T, nu) nominal trajectory        u_init: jax.Array      # (nu,) default action for shift        noise_mu: jax.Array    # (nu,)        noise_sigma: jax.Array # (nu, nu)        noise_sigma_inv: jax.Array        u_min: jax.Array | None        u_max: jax.Array | None        key: jax.Array         # PRNG key    ```</p> <p>Functions:</p> <p>```python    def create(        nx, nu, noise_sigma, num_samples=100, horizon=15, lambda_=1.0,        noise_mu=None, u_min=None, u_max=None, u_init=None, U_init=None,        u_scale=1, u_per_command=1, step_dependent_dynamics=False,        rollout_samples=1, rollout_var_cost=0., rollout_var_discount=0.95,        sample_null_action=False, noise_abs_cost=False, key=None,    ) -&gt; tuple[MPPIConfig, MPPIState]:        \"\"\"Factory: create config + initial state.\"\"\"</p> <p>def command(        config: MPPIConfig,        mppi_state: MPPIState,        current_obs: jax.Array,        dynamics: DynamicsFn,        running_cost: RunningCostFn,        terminal_cost: TerminalCostFn | None = None,        shift: bool = True,    ) -&gt; tuple[jax.Array, MPPIState]:        \"\"\"Compute optimal action and return updated state.\"\"\"</p> <p>def reset(config: MPPIConfig, mppi_state: MPPIState, key: jax.Array) -&gt; MPPIState:        \"\"\"Reset nominal trajectory.\"\"\"</p> <p>def get_rollouts(        config: MPPIConfig, mppi_state: MPPIState,        current_obs: jax.Array, dynamics: DynamicsFn,        num_rollouts: int = 1,    ) -&gt; jax.Array:        \"\"\"Forward-simulate trajectories for visualization.\"\"\"    ```</p> <p>Internal functions (all JIT-compatible):    - <code>_shift_nominal(mppi_state) -&gt; MPPIState</code> \u2014 <code>jnp.roll</code> + set last to <code>u_init</code>    - <code>_sample_noise(key, K, T, noise_mu, noise_sigma) -&gt; (noise, new_key)</code> \u2014 sample from multivariate normal    - <code>_compute_rollout_costs(config, current_obs, perturbed_actions, dynamics, running_cost, terminal_cost)</code> \u2014 uses <code>jax.lax.scan</code> over horizon, <code>jax.vmap</code> over K samples    - <code>_compute_weights(costs, lambda_)</code> \u2014 softmax importance weighting    - <code>_bound_action(action, u_min, u_max)</code> \u2014 <code>jnp.clip</code></p> <p>Key JAX patterns:    - Rollout loop: <code>jax.lax.scan</code> with carry = <code>(state,)</code>, xs = <code>actions[t]</code>    - Batch over K samples: <code>jax.vmap(_single_rollout, in_axes=(0, None, ...))</code>    - Batch over M rollout samples (stochastic dynamics): nested vmap or scan    - All internal functions decorated with <code>@jax.jit</code> or called inside a top-level jitted <code>command</code></p> <ol> <li>Unit test: <code>tests/test_mppi.py</code></li> <li>Test <code>create()</code> produces valid config/state</li> <li>Test <code>command()</code> returns correct shape</li> <li>Test cost reduction over iterations on simple 1D problem</li> <li>Test bounds are respected</li> </ol>"},{"location":"plan/completed/porting_pytorch_jax/#phase-2-pendulum-example-integration-test","title":"Phase 2: Pendulum example (integration test)","text":"<p>Files: <code>examples/pendulum.py</code>, <code>tests/test_pendulum.py</code></p> <ol> <li>Implement pendulum dynamics as a pure JAX function (no gym dependency for core test)</li> <li>Run MPPI loop, verify convergence (swing-up or stabilization)</li> <li>Optional: gym rendering wrapper for visualization</li> </ol>"},{"location":"plan/completed/porting_pytorch_jax/#phase-3-smooth-mppi-smppi","title":"Phase 3: Smooth MPPI (SMPPI)","text":"<p>Files: <code>src/jax_mppi/smppi.py</code>, <code>tests/test_smppi.py</code></p> <ol> <li>Data structures:</li> </ol> <p><code>python    @dataclass    class SMPPIState(MPPIState):        action_sequence: jax.Array  # (T, nu) actual actions        w_action_seq_cost: float        delta_t: float        action_min: jax.Array | None        action_max: jax.Array | None</code></p> <ol> <li>Functions: Same API as <code>mppi.py</code> but with:</li> <li><code>_shift_nominal</code> shifts both <code>U</code> (velocity) and <code>action_sequence</code></li> <li><code>_compute_perturbed_actions</code> integrates velocity to get actions</li> <li><code>_compute_total_cost</code> adds smoothness penalty: <code>||diff(actions)||^2</code></li> <li><code>reset()</code> zeros both <code>U</code> and <code>action_sequence</code></li> <li> <p><code>change_horizon()</code> keeps both <code>U</code> and <code>action_sequence</code> in sync (truncate/extend)</p> </li> <li> <p>Test: Verify smoother trajectories than base MPPI on 2D navigation</p> </li> </ol>"},{"location":"plan/completed/porting_pytorch_jax/#phase-4-kernel-mppi-kmppi","title":"Phase 4: Kernel MPPI (KMPPI)","text":"<p>Files: <code>src/jax_mppi/kmppi.py</code>, <code>tests/test_kmppi.py</code></p> <ol> <li>Kernel abstractions:</li> </ol> <p>```python    def rbf_kernel(t, tk, sigma=1.0):        d = jnp.sum((t[:, None] - tk) ** 2, axis=-1)        return jnp.exp(-d / (2 * sigma ** 2 + 1e-8))</p> <p>def kernel_interpolate(t, tk, coeffs, kernel_fn):        K_t_tk = kernel_fn(t, tk)        K_tk_tk = kernel_fn(tk, tk)        weights = jnp.linalg.solve(K_tk_tk, K_t_tk.T).T        return weights @ coeffs    ```</p> <ol> <li>Data structures:</li> </ol> <p><code>python    @dataclass    class KMPPIState(MPPIState):        theta: jax.Array         # (num_support_pts, nu)        num_support_pts: int</code></p> <ol> <li>Functions: Override <code>_compute_perturbed_actions</code> to sample sparse + interpolate. Update <code>theta</code> instead of <code>U</code>.</li> <li>Build <code>Tk</code> and <code>Hs</code> time grids on init and on horizon changes</li> <li>Use <code>kernel_interpolate()</code> with <code>solve(Ktktk, K)</code> (avoid explicit inverse)</li> <li> <p>Batch interpolate with <code>jax.vmap</code> for K samples</p> </li> <li> <p>Test: Verify fewer parameters produce smooth trajectories</p> </li> </ol>"},{"location":"plan/completed/porting_pytorch_jax/#phase-5-smooth-comparison-example","title":"Phase 5: Smooth comparison example","text":"<p>Files: <code>examples/smooth_comparison.py</code></p> <ul> <li>Side-by-side MPPI vs SMPPI vs KMPPI on 2D navigation</li> <li>Plot trajectories and control signals</li> </ul>"},{"location":"plan/completed/porting_pytorch_jax/#phase-6-autotuning-stretch-goal","title":"Phase 6: Autotuning (stretch goal)","text":"<p>Files: <code>src/jax_mppi/autotune.py</code></p> <ul> <li>Wrap CMA-ES (<code>cmaes</code> or <code>evosax</code> for JAX-native) for sigma/lambda/horizon tuning</li> <li>Simpler than pytorch_mppi's framework \u2014 skip Ray Tune and QD initially</li> <li>Functional API: <code>tune_step(eval_fn, params, optimizer_state) -&gt; (params, optimizer_state)</code></li> </ul>"},{"location":"plan/completed/porting_pytorch_jax/#verification-strategy","title":"Verification Strategy","text":"<ol> <li>Unit tests (per phase): <code>pytest tests/</code> \u2014 shape checks, cost reduction, bounds</li> <li>Pendulum benchmark: Compare convergence (total reward) against pytorch_mppi on same scenario</li> <li>JIT correctness: Ensure <code>jax.jit(command)</code> produces identical results to non-jitted version</li> <li>Performance: Benchmark <code>command()</code> latency vs pytorch_mppi (JAX should win after warmup due to XLA compilation)</li> <li>Smooth variants: Visual comparison of trajectory smoothness</li> </ol>"},{"location":"plan/completed/porting_pytorch_jax/#test-setup-options-src-layout","title":"Test setup options (src layout)","text":"<p>IMPORTANT: You should always use the virtual environment. To run the tests and all of the other python files.</p> <ul> <li>Option A: add a <code>tests/conftest.py</code> to insert <code>src</code> into <code>sys.path</code>.</li> <li>Option B: run tests after <code>uv pip install -e .</code> (editable install).</li> </ul>"},{"location":"plan/completed/porting_pytorch_jax/#dependencies","title":"Dependencies","text":"<p>Core: <code>jax[cuda13]</code>, <code>jaxlib</code>, <code>numpy</code> Testing: <code>pytest</code>, <code>gymnasium[classic_control]</code> Autotuning (optional): <code>cmaes</code> or <code>evosax</code> Examples (optional): <code>matplotlib</code>, <code>gymnasium</code></p>"},{"location":"plan/completed/porting_pytorch_jax/#actionable-task-checklist","title":"Actionable Task Checklist","text":""},{"location":"plan/completed/porting_pytorch_jax/#core-mppi-phase-1","title":"Core MPPI (Phase 1)","text":"<ul> <li> Mirror <code>pytorch_mppi</code> signature flags: <code>rollout_samples</code>, <code>sample_null_action</code>, <code>noise_abs_cost</code>.</li> <li> Implement <code>get_rollouts</code> handling: accept single or batched <code>state</code>; allow dynamics that augment state (take <code>:nx</code>).</li> <li> Add <code>shift_nominal_trajectory</code> via <code>jnp.roll</code> + <code>u_init</code> fill.</li> <li> Implement action cost with optional <code>abs(noise)</code> branch.</li> <li> Add <code>u_per_command</code> slicing and <code>u_scale</code> application in <code>command</code>.</li> </ul>"},{"location":"plan/completed/porting_pytorch_jax/#smppi-phase-3","title":"SMPPI (Phase 3)","text":"<ul> <li> Carry <code>action_sequence</code> in state and integrate <code>U</code> with <code>delta_t</code>.</li> <li> Implement distinct action bounds (<code>action_min</code>/<code>action_max</code>) vs control bounds (<code>u_min</code>/<code>u_max</code>).</li> <li> Add smoothness cost from <code>diff(action_sequence)</code> and weight <code>w_action_seq_cost</code>.</li> <li> Ensure <code>reset()</code> updates both <code>U</code> and <code>action_sequence</code>.</li> <li> Implement proper shift with action continuity (hold last value).</li> <li> Implement dual bounding system (_bound_control and _bound_action).</li> <li> Recompute effective noise after bounding for accurate cost.</li> </ul>"},{"location":"plan/completed/porting_pytorch_jax/#kmppi-phase-4","title":"KMPPI (Phase 4)","text":"<ul> <li> Implement <code>theta</code> control points + interpolation kernel (RBF by default).</li> <li> Build <code>Tk</code>/<code>Hs</code> grids and re-build on horizon changes.</li> <li> Use <code>solve(Ktktk, K)</code> for interpolation weights (no explicit inverse).</li> <li> Shift <code>theta</code> via interpolation when shifting nominal trajectory.</li> <li> Implement RBFKernel with configurable sigma.</li> <li> Noise sampling in control point space.</li> <li> Batched interpolation with vmap.</li> </ul>"},{"location":"plan/completed/porting_pytorch_jax/#autotune-examples-phase-6","title":"Autotune + Examples (Phase 6)","text":"<ul> <li> Mirror autotune interface from <code>pytorch_mppi/autotune*.py</code> at a minimal level (evaluation fn + optimizer loop).</li> <li> Port <code>tests/auto_tune_parameters.py</code> logic into a JAX-friendly example.</li> </ul>"}]}